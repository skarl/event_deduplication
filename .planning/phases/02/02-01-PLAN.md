---
phase: 02-matching-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/event_dedup/models/canonical_event.py
  - src/event_dedup/models/canonical_event_source.py
  - src/event_dedup/models/match_decision.py
  - src/event_dedup/models/__init__.py
  - src/event_dedup/matching/__init__.py
  - src/event_dedup/matching/config.py
  - src/event_dedup/matching/scorers/__init__.py
  - src/event_dedup/matching/scorers/date_scorer.py
  - src/event_dedup/matching/scorers/geo_scorer.py
  - src/event_dedup/matching/scorers/title_scorer.py
  - src/event_dedup/matching/scorers/desc_scorer.py
  - src/event_dedup/matching/combiner.py
  - config/matching.yaml
  - config/alembic/versions/xxxx_add_phase2_tables.py
  - tests/test_scorers.py
  - tests/test_combiner.py
  - tests/test_matching_config.py
  - pyproject.toml
autonomous: true
requirements:
  - MTCH-01
  - MTCH-03
  - MTCH-04
  - MTCH-05
  - MTCH-06
  - CANL-04

must_haves:
  truths:
    - "Each signal scorer returns 0.0-1.0 for any pair of event dicts"
    - "Combined weighted scorer produces 0.0-1.0 confidence score from individual signals"
    - "All weights and thresholds are loaded from config/matching.yaml without hardcoding"
    - "Missing data (no geo, no description) returns neutral 0.5, not penalizing 0.0"
    - "Date scoring handles multi-day overlap via Jaccard and time tolerance via configurable minutes"
    - "Title scoring uses token_sort_ratio primary with token_set_ratio blend only in ambiguous range"
    - "New database tables exist for canonical_events, canonical_event_sources, match_decisions"
  artifacts:
    - path: "src/event_dedup/matching/scorers/date_scorer.py"
      provides: "date_score() pure function"
      exports: ["date_score"]
    - path: "src/event_dedup/matching/scorers/geo_scorer.py"
      provides: "geo_score() with haversine and confidence weighting"
      exports: ["geo_score"]
    - path: "src/event_dedup/matching/scorers/title_scorer.py"
      provides: "title_score() with token_sort + token_set blend"
      exports: ["title_score"]
    - path: "src/event_dedup/matching/scorers/desc_scorer.py"
      provides: "description_score() with neutral-on-missing"
      exports: ["description_score"]
    - path: "src/event_dedup/matching/combiner.py"
      provides: "SignalScores dataclass, combined_score(), decide()"
      exports: ["SignalScores", "combined_score", "decide"]
    - path: "src/event_dedup/matching/config.py"
      provides: "MatchingConfig Pydantic model loaded from YAML"
      exports: ["MatchingConfig", "load_matching_config"]
    - path: "config/matching.yaml"
      provides: "All configurable parameters"
      contains: "scoring, thresholds, geo, date, title, cluster, canonical"
    - path: "src/event_dedup/models/canonical_event.py"
      provides: "CanonicalEvent SQLAlchemy model"
      exports: ["CanonicalEvent"]
    - path: "src/event_dedup/models/canonical_event_source.py"
      provides: "CanonicalEventSource linking table"
      exports: ["CanonicalEventSource"]
    - path: "src/event_dedup/models/match_decision.py"
      provides: "MatchDecision audit table"
      exports: ["MatchDecision"]
    - path: "tests/test_scorers.py"
      provides: "Unit tests for all four signal scorers"
      min_lines: 100
    - path: "tests/test_combiner.py"
      provides: "Unit tests for combiner and decide"
      min_lines: 40
  key_links:
    - from: "src/event_dedup/matching/combiner.py"
      to: "src/event_dedup/matching/config.py"
      via: "ScoringWeights and ThresholdConfig used by combined_score and decide"
      pattern: "ScoringWeights|ThresholdConfig"
    - from: "src/event_dedup/matching/scorers/geo_scorer.py"
      to: "src/event_dedup/matching/config.py"
      via: "GeoConfig parameters (max_distance_km, min_confidence, neutral_score)"
      pattern: "GeoConfig|max_distance_km"
    - from: "src/event_dedup/matching/scorers/date_scorer.py"
      to: "src/event_dedup/matching/config.py"
      via: "DateConfig parameters (time_tolerance_minutes, close_factor, far_factor)"
      pattern: "DateConfig|time_tolerance"
    - from: "src/event_dedup/models/__init__.py"
      to: "src/event_dedup/models/canonical_event.py"
      via: "re-export of CanonicalEvent, CanonicalEventSource, MatchDecision"
      pattern: "from event_dedup.models"
---

<objective>
Create the database models for Phase 2 (CanonicalEvent, CanonicalEventSource, MatchDecision), the matching configuration system (YAML + Pydantic), all four signal scorers (date, geo, title, description), and the weighted score combiner with three-tier decision logic.

Purpose: This is the foundational layer for the entire matching pipeline. All subsequent plans depend on these scorers, models, and configuration. By building scorers as pure functions taking dict arguments, they are independently testable and composable.

Output: 3 new SQLAlchemy models with Alembic migration, 4 signal scorers, 1 combiner, 1 YAML config file, comprehensive unit tests.
</objective>

<execution_context>
@.planning/phases/02/02-RESEARCH.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/phases/02/02-CONTEXT.md
@.planning/phases/02/02-RESEARCH.md

<interfaces>
<!-- Existing interfaces this plan builds on -->

From src/event_dedup/models/base.py:
```python
class Base(DeclarativeBase):
    metadata = sa.MetaData(naming_convention={...})
```

From src/event_dedup/models/source_event.py:
```python
class SourceEvent(Base):
    __tablename__ = "source_events"
    id: Mapped[str] = mapped_column(sa.String, primary_key=True)
    title: Mapped[str]
    short_description: Mapped[str | None]
    description: Mapped[str | None]
    highlights: Mapped[list | None] = mapped_column(sa.JSON)
    location_name: Mapped[str | None]
    location_city: Mapped[str | None]
    title_normalized: Mapped[str | None]
    short_description_normalized: Mapped[str | None]
    blocking_keys: Mapped[list | None] = mapped_column(sa.JSON)
    geo_latitude: Mapped[float | None]
    geo_longitude: Mapped[float | None]
    geo_confidence: Mapped[float | None]
    source_type: Mapped[str]
    source_code: Mapped[str]
    categories: Mapped[list | None] = mapped_column(sa.JSON)
    # ... plus dates relationship, location fields, booleans
```

From src/event_dedup/models/event_date.py:
```python
class EventDate(Base):
    __tablename__ = "event_dates"
    event_id: Mapped[str] = mapped_column(sa.ForeignKey("source_events.id"))
    date: Mapped[dt.date]
    start_time: Mapped[dt.time | None]
    end_time: Mapped[dt.time | None]
    end_date: Mapped[dt.date | None]
```

From src/event_dedup/models/__init__.py:
```python
from event_dedup.models.base import Base
from event_dedup.models.event_date import EventDate
from event_dedup.models.file_ingestion import FileIngestion
from event_dedup.models.ground_truth import GroundTruthPair
from event_dedup.models.source_event import SourceEvent
__all__ = ["Base", "EventDate", "FileIngestion", "GroundTruthPair", "SourceEvent"]
```

From src/event_dedup/config/settings.py:
```python
class Settings(BaseSettings):
    model_config = SettingsConfigDict(env_prefix="EVENT_DEDUP_")
    database_url: str = "postgresql+asyncpg://..."
    prefixes_config_path: Path = _CONFIG_DIR / "prefixes.yaml"
    city_aliases_path: Path = _CONFIG_DIR / "city_aliases.yaml"
```

From tests/conftest.py:
```python
@pytest.fixture
async def test_engine():
    engine = create_async_engine("sqlite+aiosqlite:///:memory:")
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)
    yield engine
    await engine.dispose()
```

From pyproject.toml:
```toml
dependencies = [
    "sqlalchemy[asyncio]>=2.0", "asyncpg>=0.30", "alembic>=1.14",
    "pydantic>=2.9", "pydantic-settings>=2.0", "rapidfuzz>=3.10", "pyyaml>=6.0",
]
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Database Models, Alembic Migration, Matching Config, and networkx Dependency</name>
  <files>
    src/event_dedup/models/canonical_event.py
    src/event_dedup/models/canonical_event_source.py
    src/event_dedup/models/match_decision.py
    src/event_dedup/models/__init__.py
    src/event_dedup/matching/__init__.py
    src/event_dedup/matching/config.py
    config/matching.yaml
    config/alembic/versions/xxxx_add_phase2_tables.py
    pyproject.toml
    tests/test_matching_config.py
  </files>
  <action>
1. **Add networkx dependency**: Run `uv add networkx` to add it to pyproject.toml. This is the only new dependency for Phase 2.

2. **Create `src/event_dedup/models/canonical_event.py`**:
   - Import `from event_dedup.models.base import Base`, `sqlalchemy as sa`, `Mapped`, `mapped_column`, `relationship`
   - Class `CanonicalEvent(Base)` with `__tablename__ = "canonical_events"`
   - Fields: `id` (Integer, PK, autoincrement), `title` (String, not null), `short_description` (String, nullable), `description` (Text, nullable), `highlights` (JSON, nullable), `location_name` (String, nullable), `location_city` (String, nullable), `location_district` (String, nullable), `location_street` (String, nullable), `location_zipcode` (String, nullable), `geo_latitude` (Float, nullable), `geo_longitude` (Float, nullable), `geo_confidence` (Float, nullable), `dates` (JSON, nullable -- list of date dicts), `categories` (JSON, nullable), `is_family_event` (Boolean, nullable), `is_child_focused` (Boolean, nullable), `admission_free` (Boolean, nullable), `field_provenance` (JSON, nullable -- dict mapping field_name to source_event_id), `source_count` (Integer, default=1), `match_confidence` (Float, nullable), `needs_review` (Boolean, default=False), `version` (Integer, default=1), `created_at` (DateTime, server_default CURRENT_TIMESTAMP), `updated_at` (DateTime, server_default CURRENT_TIMESTAMP)
   - Add `sources` relationship to `CanonicalEventSource` with `back_populates="canonical_event"`, `cascade="all, delete-orphan"`
   - Use `sa.text("CURRENT_TIMESTAMP")` for server defaults (same pattern as SourceEvent)

3. **Create `src/event_dedup/models/canonical_event_source.py`**:
   - Class `CanonicalEventSource(Base)` with `__tablename__ = "canonical_event_sources"`
   - Fields: `id` (Integer, PK, autoincrement), `canonical_event_id` (Integer, FK to canonical_events.id, ondelete CASCADE), `source_event_id` (String, FK to source_events.id), `added_at` (DateTime, server_default CURRENT_TIMESTAMP)
   - UniqueConstraint on (canonical_event_id, source_event_id) to prevent duplicates
   - Relationships: `canonical_event` back to CanonicalEvent, `source_event` to SourceEvent

4. **Create `src/event_dedup/models/match_decision.py`**:
   - Class `MatchDecision(Base)` with `__tablename__ = "match_decisions"`
   - Fields: `id` (Integer, PK, autoincrement), `source_event_id_a` (String, FK to source_events.id), `source_event_id_b` (String, FK to source_events.id), `combined_score` (Float), `date_score` (Float), `geo_score` (Float), `title_score` (Float), `description_score` (Float), `decision` (String -- "match"/"no_match"/"ambiguous"), `tier` (String, default="deterministic"), `decided_at` (DateTime, server_default CURRENT_TIMESTAMP)
   - Add CheckConstraint: `source_event_id_a < source_event_id_b` (canonical ordering, same pattern as GroundTruthPair)
   - Add CheckConstraint: `decision IN ('match', 'no_match', 'ambiguous')`
   - UniqueConstraint on (source_event_id_a, source_event_id_b) to prevent re-scoring

5. **Update `src/event_dedup/models/__init__.py`**: Add imports and exports for CanonicalEvent, CanonicalEventSource, MatchDecision. Keep all existing imports.

6. **Create Alembic migration**: Run `cd /Users/svenkarl/workspaces/event-deduplication && ALEMBIC_DATABASE_URL=sqlite:///alembic_tmp.db uv run alembic -c config/alembic.ini revision --autogenerate -m "add phase 2 tables"`. Then verify the generated migration file looks correct. Delete the temp db file after.

7. **Create `src/event_dedup/matching/__init__.py`**: Empty `__init__.py` file.

8. **Create `src/event_dedup/matching/config.py`**:
   - Define Pydantic BaseModel classes (NOT BaseSettings -- these are loaded from YAML, not env vars):
     - `ScoringWeights(BaseModel)`: date=0.30, geo=0.25, title=0.30, description=0.15
     - `ThresholdConfig(BaseModel)`: high=0.75, low=0.35
     - `GeoConfig(BaseModel)`: max_distance_km=10.0, min_confidence=0.85, neutral_score=0.5
     - `DateConfig(BaseModel)`: time_tolerance_minutes=30, time_close_minutes=90, close_factor=0.7, far_factor=0.3
     - `TitleConfig(BaseModel)`: primary_weight=0.7, secondary_weight=0.3, blend_lower=0.40, blend_upper=0.80
     - `ClusterConfig(BaseModel)`: max_cluster_size=15, min_internal_similarity=0.40
     - `FieldStrategies(BaseModel)`: title="longest_non_generic", short_description="longest", description="longest", highlights="union", location_name="most_complete", location_city="most_frequent", location_street="most_complete", geo="highest_confidence", categories="union", is_family_event="any_true", is_child_focused="any_true", admission_free="any_true"
     - `CanonicalConfig(BaseModel)`: field_strategies: FieldStrategies = FieldStrategies()
     - `MatchingConfig(BaseModel)`: scoring, thresholds, geo, date, title, cluster, canonical -- all with defaults
   - Function `load_matching_config(path: Path) -> MatchingConfig`: load YAML with `yaml.safe_load`, pass to `MatchingConfig(**data)`. If file doesn't exist, return `MatchingConfig()` (all defaults).

9. **Create `config/matching.yaml`**: Full configuration file with all parameters and comments explaining each section. Use exact values from research: scoring weights (date=0.30, geo=0.25, title=0.30, desc=0.15), thresholds (high=0.75, low=0.35), geo settings (max_distance_km=10.0, min_confidence=0.85), date settings (30/90 min, 0.7/0.3 factors), title settings (0.7/0.3 blend, 0.40-0.80 range), cluster settings (max_size=15, min_sim=0.40), canonical field strategies.

10. **Create `tests/test_matching_config.py`**:
    - Test loading from YAML file (write temp YAML, load, verify values)
    - Test default values when no YAML exists
    - Test partial override (YAML with only some fields, rest are defaults)
    - Test `MatchingConfig` validation (invalid types rejected)
  </action>
  <verify>
    <automated>cd /Users/svenkarl/workspaces/event-deduplication && uv run pytest tests/test_matching_config.py -x -v</automated>
  </verify>
  <done>
    - CanonicalEvent, CanonicalEventSource, MatchDecision models exist and are importable from `event_dedup.models`
    - Alembic migration file created for the three new tables
    - `config/matching.yaml` exists with all configurable parameters
    - `MatchingConfig` loads from YAML and provides typed access to all config sections
    - `test_matching_config.py` passes with tests for load, defaults, partial override
    - `networkx` added to pyproject.toml dependencies
  </done>
</task>

<task type="auto">
  <name>Task 2: Signal Scorers (date, geo, title, description) and Weighted Combiner</name>
  <files>
    src/event_dedup/matching/scorers/__init__.py
    src/event_dedup/matching/scorers/date_scorer.py
    src/event_dedup/matching/scorers/geo_scorer.py
    src/event_dedup/matching/scorers/title_scorer.py
    src/event_dedup/matching/scorers/desc_scorer.py
    src/event_dedup/matching/combiner.py
    tests/test_scorers.py
    tests/test_combiner.py
  </files>
  <action>
1. **Create `src/event_dedup/matching/scorers/__init__.py`**:
   - Re-export all four scorers: `from .date_scorer import date_score`, `from .geo_scorer import geo_score`, `from .title_scorer import title_score`, `from .desc_scorer import description_score`

2. **Create `src/event_dedup/matching/scorers/date_scorer.py`**:
   - Import `datetime as dt` and `DateConfig` from `..config`
   - Function `date_score(event_a: dict, event_b: dict, config: DateConfig | None = None) -> float`:
     - If config is None, use `DateConfig()` (defaults)
     - Extract `event_a["dates"]` and `event_b["dates"]` -- each is a list of dicts with keys: `date` (str ISO), `start_time` (str HH:MM or None), `end_time` (str or None), `end_date` (str ISO or None)
     - Call `_expand_dates(dates_list)` to get a `set[dt.date]` -- expand each date entry from `date` to `end_date` (inclusive) using `dt.timedelta(days=1)` loop
     - If either set is empty, return 0.0
     - Compute Jaccard: `len(overlap) / len(union)`
     - If Jaccard == 0.0, return 0.0
     - Compute time proximity factor via `_time_proximity_factor(dates_a_list, dates_b_list, overlap_dates, config)`:
       - For each overlapping date (sorted), find the start_time from each event's date list for that date
       - If both have start_time: compute absolute difference in minutes
       - If diff <= config.time_tolerance_minutes (30): return 1.0
       - If diff <= config.time_close_minutes (90): return config.close_factor (0.7)
       - If diff > 90: return config.far_factor (0.3)
       - If no overlapping date has time from both events: return 1.0 (no penalty for missing time data)
     - Return `jaccard * time_factor`
   - Helper `_expand_dates(date_list: list[dict]) -> set[dt.date]`: parse "date" field as `dt.date.fromisoformat()`, parse "end_date" if present, expand range inclusive
   - Helper `_get_start_time_for_date(date_list: list[dict], target_date: dt.date) -> dt.time | None`: find the dict whose "date" matches target_date and return parsed start_time

3. **Create `src/event_dedup/matching/scorers/geo_scorer.py`**:
   - Import `math` and `GeoConfig` from `..config`
   - Function `geo_score(event_a: dict, event_b: dict, config: GeoConfig | None = None) -> float`:
     - If config is None, use `GeoConfig()` (defaults)
     - Extract lat/lon/confidence from both events using `.get()` keys: `geo_latitude`, `geo_longitude`, `geo_confidence`
     - If either lacks lat or lon: return `config.neutral_score` (0.5) -- don't penalize missing geo
     - Compute `min_confidence = min(conf_a or 0.0, conf_b or 0.0)`
     - If `min_confidence < config.min_confidence` (0.85): return `config.neutral_score` (0.5) -- don't trust low-confidence geo
     - Compute haversine distance via `_haversine_km(lat_a, lon_a, lat_b, lon_b)`
     - Return `max(0.0, 1.0 - dist_km / config.max_distance_km)`
   - Helper `_haversine_km(lat1, lon1, lat2, lon2) -> float`: standard haversine formula using R=6371.0 km

4. **Create `src/event_dedup/matching/scorers/title_scorer.py`**:
   - Import `token_sort_ratio, token_set_ratio` from `rapidfuzz.fuzz`, `TitleConfig` from `..config`
   - Function `title_score(event_a: dict, event_b: dict, config: TitleConfig | None = None) -> float`:
     - If config is None, use `TitleConfig()` (defaults)
     - Extract `title_normalized` from both events via `.get()`, default to ""
     - If either is empty: return 0.0
     - Compute primary = `token_sort_ratio(title_a, title_b) / 100.0`
     - If `config.blend_lower <= primary <= config.blend_upper` (0.40-0.80):
       - Compute secondary = `token_set_ratio(title_a, title_b) / 100.0`
       - Return `primary * config.primary_weight + secondary * config.secondary_weight` (0.7/0.3 blend)
     - Otherwise return primary

5. **Create `src/event_dedup/matching/scorers/desc_scorer.py`**:
   - Import `token_sort_ratio` from `rapidfuzz.fuzz`
   - Function `description_score(event_a: dict, event_b: dict) -> float`:
     - Extract `short_description_normalized` from both events via `.get()`, default to ""
     - If both empty: return 0.5 (neutral -- no data is not evidence against)
     - If one empty: return 0.4 (slightly below neutral)
     - Return `token_sort_ratio(desc_a, desc_b) / 100.0`

6. **Create `src/event_dedup/matching/combiner.py`**:
   - Import `dataclass` from dataclasses, `ScoringWeights`, `ThresholdConfig` from `.config`
   - `@dataclass` class `SignalScores`: date (float), geo (float), title (float), description (float)
   - Function `combined_score(signals: SignalScores, weights: ScoringWeights) -> float`:
     - Compute weighted sum: `signals.date * weights.date + signals.geo * weights.geo + signals.title * weights.title + signals.description * weights.description`
     - Divide by `sum([weights.date, weights.geo, weights.title, weights.description])`
     - Return the normalized score (0.0-1.0)
   - Function `decide(score: float, thresholds: ThresholdConfig) -> str`:
     - If `score >= thresholds.high`: return "match"
     - If `score <= thresholds.low`: return "no_match"
     - Else: return "ambiguous"

7. **Create `tests/test_scorers.py`** with comprehensive pure-function tests:
   - **date_score tests:**
     - Same single date, both with start_time within 30 min -> ~1.0
     - Same single date, times 60 min apart -> Jaccard 1.0 * 0.7
     - Same single date, times 120 min apart -> Jaccard 1.0 * 0.3
     - Same single date, no time info -> Jaccard 1.0 * 1.0
     - Multi-day overlap: [Feb 12, 13] vs [Feb 12, 13, 14] -> Jaccard 2/3
     - Date range expansion: date="2026-02-12" with end_date="2026-02-14" -> {12,13,14}
     - No overlap -> 0.0
     - Empty dates -> 0.0
   - **geo_score tests:**
     - Same coordinates -> 1.0
     - 5 km apart with good confidence -> ~0.5
     - 10+ km apart -> 0.0
     - Missing coordinates -> 0.5 (neutral)
     - Low confidence (< 0.85) -> 0.5 (neutral)
     - One missing confidence -> 0.5 (neutral)
   - **title_score tests:**
     - Identical titles -> 1.0
     - Completely different -> close to 0.0
     - Word reordering (token_sort handles) -> high score
     - In ambiguous range (0.40-0.80) -> blend applied
     - One empty title -> 0.0
     - Both empty -> 0.0
   - **description_score tests:**
     - Both missing -> 0.5
     - One missing -> 0.4
     - Similar descriptions -> high score
     - Different descriptions -> low score

8. **Create `tests/test_combiner.py`**:
   - Test `combined_score` with equal weights -> average of signals
   - Test `combined_score` with unequal weights -> proper weighting
   - Test `decide` with score above high threshold -> "match"
   - Test `decide` with score below low threshold -> "no_match"
   - Test `decide` with score in between -> "ambiguous"
   - Test `decide` at exact boundaries (edge cases)
  </action>
  <verify>
    <automated>cd /Users/svenkarl/workspaces/event-deduplication && uv run pytest tests/test_scorers.py tests/test_combiner.py -x -v</automated>
  </verify>
  <done>
    - All four signal scorers are importable from `event_dedup.matching.scorers`
    - `date_score` handles Jaccard overlap + time tolerance with configurable parameters
    - `geo_score` uses haversine with confidence weighting, returns 0.5 for missing/low-confidence data
    - `title_score` uses token_sort_ratio primary with token_set_ratio blend only in 0.40-0.80 range
    - `description_score` returns 0.5 for missing data (neutral, not penalizing)
    - `combined_score` produces normalized 0.0-1.0 from weighted signals
    - `decide` returns "match"/"ambiguous"/"no_match" based on configurable thresholds
    - All tests pass
  </done>
</task>

</tasks>

<verification>
```bash
# All tests pass
cd /Users/svenkarl/workspaces/event-deduplication && uv run pytest tests/test_matching_config.py tests/test_scorers.py tests/test_combiner.py -x -v

# Models are importable
cd /Users/svenkarl/workspaces/event-deduplication && uv run python -c "from event_dedup.models import CanonicalEvent, CanonicalEventSource, MatchDecision; print('Models OK')"

# Config loads from YAML
cd /Users/svenkarl/workspaces/event-deduplication && uv run python -c "from event_dedup.matching.config import load_matching_config; from pathlib import Path; c = load_matching_config(Path('config/matching.yaml')); print(f'Weights: date={c.scoring.date}, geo={c.scoring.geo}, title={c.scoring.title}, desc={c.scoring.description}')"

# Scorers are importable and callable
cd /Users/svenkarl/workspaces/event-deduplication && uv run python -c "from event_dedup.matching.scorers import date_score, geo_score, title_score, description_score; from event_dedup.matching.combiner import SignalScores, combined_score, decide; print('All imports OK')"

# networkx installed
cd /Users/svenkarl/workspaces/event-deduplication && uv run python -c "import networkx; print(f'networkx {networkx.__version__}')"
```
</verification>

<success_criteria>
1. Three new SQLAlchemy models (CanonicalEvent, CanonicalEventSource, MatchDecision) exist and create tables via `Base.metadata.create_all`
2. Alembic migration generated for the three new tables
3. `config/matching.yaml` contains all parameters (weights, thresholds, geo, date, title, cluster, canonical field strategies)
4. `MatchingConfig` Pydantic model loads from YAML and provides typed defaults
5. Four signal scorers return 0.0-1.0 for dict-based event pairs
6. Missing data produces neutral scores (0.5 for geo, 0.5 for description, 1.0 time factor for missing times)
7. `combined_score` produces weighted average, `decide` returns three-tier decision
8. All tests pass: `test_matching_config.py`, `test_scorers.py`, `test_combiner.py`
9. `networkx` is in pyproject.toml dependencies
</success_criteria>

<output>
After completion, create `.planning/phases/02/02-01-SUMMARY.md`
</output>
