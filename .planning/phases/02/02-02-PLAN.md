---
phase: 02-matching-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/event_dedup/matching/candidate_pairs.py
  - src/event_dedup/matching/pipeline.py
  - tests/test_candidate_pairs.py
  - tests/test_pipeline.py
autonomous: true
requirements:
  - MTCH-02
  - MTCH-06

must_haves:
  truths:
    - "Candidate pairs are generated only from events sharing at least one blocking key"
    - "Only cross-source pairs are generated (same source_code events never compared)"
    - "Pairs use canonical ordering (id_a < id_b) to prevent duplicates"
    - "Blocking achieves >95% reduction vs naive all-pairs comparison"
    - "Pipeline scores all candidate pairs using all four signal scorers and the combiner"
    - "Match decisions record all signal scores, combined score, and three-tier decision"
  artifacts:
    - path: "src/event_dedup/matching/candidate_pairs.py"
      provides: "generate_candidate_pairs() from blocking keys"
      exports: ["generate_candidate_pairs"]
    - path: "src/event_dedup/matching/pipeline.py"
      provides: "score_candidate_pairs() matching pipeline orchestrator"
      exports: ["score_candidate_pairs", "MatchResult"]
    - path: "tests/test_candidate_pairs.py"
      provides: "Unit tests for blocking-based pair generation"
      min_lines: 60
    - path: "tests/test_pipeline.py"
      provides: "Integration tests for scoring pipeline"
      min_lines: 80
  key_links:
    - from: "src/event_dedup/matching/candidate_pairs.py"
      to: "blocking_keys field on event dicts"
      via: "blocking_index dict grouping events by shared blocking keys"
      pattern: "blocking_keys|blocking_index"
    - from: "src/event_dedup/matching/pipeline.py"
      to: "src/event_dedup/matching/scorers/__init__.py"
      via: "imports all four scorers and combiner"
      pattern: "date_score|geo_score|title_score|description_score|combined_score"
    - from: "src/event_dedup/matching/pipeline.py"
      to: "src/event_dedup/matching/config.py"
      via: "MatchingConfig drives all scoring parameters"
      pattern: "MatchingConfig|config\\.scoring|config\\.thresholds"
---

<objective>
Build the candidate pair generator (using Phase 1 blocking keys to find event pairs to compare) and the matching pipeline orchestrator that scores all candidate pairs using the four signal scorers from Plan 02-01.

Purpose: This converts the blocking keys from Phase 1 into actual scored pairs with match/ambiguous/no_match decisions. The pipeline is the core engine that Plan 02-03 (clustering) and Plan 02-04 (canonical synthesis) build on. Blocking reduction verification (>95%) proves the system is practical at scale.

Output: Candidate pair generator, scoring pipeline, blocking reduction stats, comprehensive tests.
</objective>

<execution_context>
@.planning/phases/02/02-RESEARCH.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/phases/02/02-CONTEXT.md
@.planning/phases/02/02-RESEARCH.md
@.planning/phases/02/02-01-SUMMARY.md

<interfaces>
<!-- From Plan 02-01 (must exist before this plan runs) -->

From src/event_dedup/matching/scorers/__init__.py:
```python
from .date_scorer import date_score
from .geo_scorer import geo_score
from .title_scorer import title_score
from .desc_scorer import description_score
```

From src/event_dedup/matching/combiner.py:
```python
@dataclass
class SignalScores:
    date: float
    geo: float
    title: float
    description: float

def combined_score(signals: SignalScores, weights: ScoringWeights) -> float: ...
def decide(score: float, thresholds: ThresholdConfig) -> str: ...
```

From src/event_dedup/matching/config.py:
```python
class ScoringWeights(BaseModel):
    date: float = 0.30; geo: float = 0.25; title: float = 0.30; description: float = 0.15
class ThresholdConfig(BaseModel):
    high: float = 0.75; low: float = 0.35
class GeoConfig(BaseModel):
    max_distance_km: float = 10.0; min_confidence: float = 0.85; neutral_score: float = 0.5
class DateConfig(BaseModel):
    time_tolerance_minutes: int = 30; time_close_minutes: int = 90; close_factor: float = 0.7; far_factor: float = 0.3
class TitleConfig(BaseModel):
    primary_weight: float = 0.7; secondary_weight: float = 0.3; blend_lower: float = 0.40; blend_upper: float = 0.80
class MatchingConfig(BaseModel):
    scoring: ScoringWeights; thresholds: ThresholdConfig; geo: GeoConfig; date: DateConfig; title: TitleConfig; cluster: ClusterConfig; canonical: CanonicalConfig
def load_matching_config(path: Path) -> MatchingConfig: ...
```

From src/event_dedup/models/match_decision.py:
```python
class MatchDecision(Base):
    __tablename__ = "match_decisions"
    source_event_id_a: Mapped[str]  # FK, canonical ordering enforced
    source_event_id_b: Mapped[str]  # FK
    combined_score: Mapped[float]
    date_score: Mapped[float]
    geo_score: Mapped[float]
    title_score: Mapped[float]
    description_score: Mapped[float]
    decision: Mapped[str]  # "match" | "no_match" | "ambiguous"
    tier: Mapped[str]  # default "deterministic"
```

From src/event_dedup/evaluation/harness.py (existing pattern to follow):
```python
def generate_predictions_from_events(events: list[dict], config: EvaluationConfig) -> set[tuple[str, str]]:
    # Build blocking index
    blocking_index: dict[str, list[dict]] = {}
    for event in events:
        for key in event.get("blocking_keys") or []:
            blocking_index.setdefault(key, []).append(event)
    # Generate cross-source pairs within blocks
    ...
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Candidate Pair Generator with Blocking Reduction Stats</name>
  <files>
    src/event_dedup/matching/candidate_pairs.py
    tests/test_candidate_pairs.py
  </files>
  <action>
1. **Create `src/event_dedup/matching/candidate_pairs.py`**:
   - Import `dataclass` from dataclasses
   - `@dataclass` class `CandidatePairStats`: `total_events` (int), `total_possible_pairs` (int -- n*(n-1)/2 cross-source pairs), `blocked_pairs` (int -- pairs after blocking), `reduction_pct` (float -- percentage reduction)
   - Function `generate_candidate_pairs(events: list[dict]) -> tuple[list[tuple[str, str]], CandidatePairStats]`:
     - Build blocking index: `dict[str, list[dict]]` mapping each blocking key to events that share it (same pattern as existing `generate_predictions_from_events` in harness.py)
     - Iterate through each blocking group
     - For each pair (i, j) in the group:
       - Skip same-source pairs: `if evt_a["source_code"] == evt_b["source_code"]: continue`
       - Canonical ordering: `id_a, id_b = (min(a_id, b_id), max(a_id, b_id))`
       - Skip already-seen pairs (use a `seen: set[tuple[str, str]]` to deduplicate across blocking groups)
     - Compute stats:
       - `total_possible_pairs`: Count unique cross-source pairs without blocking. Group events by `source_code`, then for each pair of source groups, multiply their sizes. Sum all products. This is the denominator for blocking reduction.
       - `blocked_pairs`: len(result)
       - `reduction_pct`: `(1 - blocked_pairs / total_possible_pairs) * 100` if total_possible_pairs > 0 else 0.0
     - Return `(list of (id_a, id_b), CandidatePairStats)`
   - Helper `_count_cross_source_pairs(events: list[dict]) -> int`:
     - Group events by `source_code`
     - For each combination of two different source groups, count pairs = len(group_a) * len(group_b)
     - Sum all counts (divide by 2 if counting both directions, but since we only count (a,b) where source_a < source_b or use combinations, be careful to count correctly). Simplest: use `itertools.combinations` on the list of group sizes.

2. **Create `tests/test_candidate_pairs.py`**:
   - Test with 2 events sharing a blocking key, different sources -> 1 pair
   - Test with 2 events sharing a blocking key, SAME source -> 0 pairs (cross-source only)
   - Test with 3 events in same block, 2 sources -> correct pairs (source A: 2 events, source B: 1 event -> 2 cross-source pairs)
   - Test canonical ordering: id_a < id_b always
   - Test deduplication: events sharing multiple blocking keys produce each pair only once
   - Test events with no shared blocking keys -> 0 pairs
   - Test reduction stats: set up 10 events from 3 sources with blocking keys that reduce pairs by >50%
   - Test `_count_cross_source_pairs` directly
  </action>
  <verify>
    <automated>cd /Users/svenkarl/workspaces/event-deduplication && uv run pytest tests/test_candidate_pairs.py -x -v</automated>
  </verify>
  <done>
    - `generate_candidate_pairs` produces deduplicated, cross-source, canonically ordered pairs from blocking keys
    - `CandidatePairStats` reports total possible, blocked, and reduction percentage
    - All tests pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Matching Pipeline Orchestrator (Score All Pairs + Record Decisions)</name>
  <files>
    src/event_dedup/matching/pipeline.py
    tests/test_pipeline.py
  </files>
  <action>
1. **Create `src/event_dedup/matching/pipeline.py`**:
   - Import all scorers from `.scorers`, `SignalScores` and `combined_score` and `decide` from `.combiner`, `generate_candidate_pairs` and `CandidatePairStats` from `.candidate_pairs`, `MatchingConfig` from `.config`
   - `@dataclass` class `MatchDecisionRecord`: `event_id_a` (str), `event_id_b` (str), `signals` (SignalScores), `combined_score_value` (float), `decision` (str), `tier` (str = "deterministic")
   - `@dataclass` class `MatchResult`: `decisions` (list[MatchDecisionRecord]), `pair_stats` (CandidatePairStats), `match_count` (int), `ambiguous_count` (int), `no_match_count` (int)
   - Function `score_candidate_pairs(events: list[dict], config: MatchingConfig) -> MatchResult`:
     - This is a PURE FUNCTION. No DB access. Takes event dicts, returns scored decisions.
     - Call `generate_candidate_pairs(events)` to get pairs and stats
     - Build `events_by_id = {e["id"]: e for e in events}`
     - For each `(id_a, id_b)` pair:
       - Look up event dicts: `evt_a = events_by_id[id_a]`, `evt_b = events_by_id[id_b]`
       - Score: `signals = SignalScores(date=date_score(evt_a, evt_b, config.date), geo=geo_score(evt_a, evt_b, config.geo), title=title_score(evt_a, evt_b, config.title), description=description_score(evt_a, evt_b))`
       - Combine: `score = combined_score(signals, config.scoring)`
       - Decide: `dec = decide(score, config.thresholds)`
       - Append `MatchDecisionRecord(event_id_a=id_a, event_id_b=id_b, signals=signals, combined_score_value=score, decision=dec)`
     - Count matches, ambiguous, no_match from decisions
     - Return `MatchResult(decisions=decisions, pair_stats=pair_stats, match_count=..., ambiguous_count=..., no_match_count=...)`
   - Function `get_match_pairs(result: MatchResult) -> set[tuple[str, str]]`:
     - Extract just the match-decision pairs for clustering: `{(d.event_id_a, d.event_id_b) for d in result.decisions if d.decision == "match"}`
     - This is the interface Plan 02-03 uses to build the graph

2. **Create `tests/test_pipeline.py`**:
   - Create helper function `make_event(id, title_normalized, source_code, blocking_keys, dates, geo_latitude=None, geo_longitude=None, geo_confidence=None, short_description_normalized=None)` that returns a dict matching the scorer interface
   - **Test: Two identical events from different sources**
     - Same date, same city, same normalized title, same description
     - Should produce decision "match" (all scores high -> combined score > 0.75)
   - **Test: Two completely different events from different sources sharing a blocking key**
     - Same date (required by blocking), different city, different title
     - Should produce decision "no_match" (low title + geo -> combined < 0.35)
   - **Test: Ambiguous pair**
     - Same date, same city, partially similar title (e.g. "fasnetumzug nordweil" vs "nordwiler narrenfahrplan fasnetumzug")
     - Should produce decision "ambiguous" (combined between 0.35 and 0.75)
   - **Test: Pipeline stats**
     - Create 5 events from 2 sources with overlapping blocking keys
     - Verify match_count, ambiguous_count, no_match_count sum to total pairs
     - Verify pair_stats.blocked_pairs equals len(decisions)
   - **Test: get_match_pairs returns only match decisions**
   - **Test: Cross-source enforcement**
     - Two events from same source sharing blocking key
     - Pipeline should produce 0 decisions for them
   - **Test: Config passthrough**
     - Use custom config with high threshold = 0.99 -> everything becomes "ambiguous" or "no_match"
     - Verify no "match" decisions
  </action>
  <verify>
    <automated>cd /Users/svenkarl/workspaces/event-deduplication && uv run pytest tests/test_pipeline.py -x -v</automated>
  </verify>
  <done>
    - `score_candidate_pairs` takes event dicts + MatchingConfig, returns MatchResult with all decisions
    - Each decision contains all four signal scores, combined score, and three-tier decision
    - `get_match_pairs` extracts match-only pairs for clustering
    - Cross-source and canonical ordering are enforced
    - All tests pass with match/ambiguous/no_match scenarios verified
  </done>
</task>

</tasks>

<verification>
```bash
# All plan 02-02 tests pass
cd /Users/svenkarl/workspaces/event-deduplication && uv run pytest tests/test_candidate_pairs.py tests/test_pipeline.py -x -v

# Pipeline imports work
cd /Users/svenkarl/workspaces/event-deduplication && uv run python -c "from event_dedup.matching.pipeline import score_candidate_pairs, get_match_pairs, MatchResult; print('Pipeline OK')"
cd /Users/svenkarl/workspaces/event-deduplication && uv run python -c "from event_dedup.matching.candidate_pairs import generate_candidate_pairs, CandidatePairStats; print('Candidate pairs OK')"

# All prior tests still pass
cd /Users/svenkarl/workspaces/event-deduplication && uv run pytest tests/ -x --timeout=60
```
</verification>

<success_criteria>
1. `generate_candidate_pairs` produces deduplicated, cross-source, canonically ordered pairs from blocking keys
2. `CandidatePairStats` reports blocking reduction percentage (target: >95% on real data)
3. `score_candidate_pairs` is a pure function that scores all pairs using four signal scorers + combiner
4. `MatchDecisionRecord` contains all signal scores, combined score, and decision for each pair
5. `get_match_pairs` extracts match-decision pairs for use by clustering (Plan 02-03)
6. All tests pass including match/ambiguous/no_match scenarios
7. All prior tests still pass
</success_criteria>

<output>
After completion, create `.planning/phases/02/02-02-SUMMARY.md`
</output>
