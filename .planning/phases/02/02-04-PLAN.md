---
phase: 02-matching-pipeline
plan: 04
type: execute
wave: 4
depends_on: ["02-03"]
files_modified:
  - src/event_dedup/canonical/__init__.py
  - src/event_dedup/canonical/synthesizer.py
  - src/event_dedup/canonical/enrichment.py
  - src/event_dedup/matching/pipeline.py
  - src/event_dedup/evaluation/harness.py
  - tests/test_synthesizer.py
  - tests/test_enrichment.py
  - tests/test_end_to_end.py
autonomous: true
requirements:
  - CANL-01
  - CANL-02
  - CANL-03

must_haves:
  truths:
    - "Canonical event is synthesized by selecting best field from each source event"
    - "Field provenance tracks which source event contributed each canonical field"
    - "Enrichment re-synthesizes canonical when new sources arrive without downgrading existing good data"
    - "End-to-end pipeline runs: blocking -> scoring -> clustering -> synthesis"
    - "Evaluation harness updated to use multi-signal scoring for predictions"
    - "Full pipeline produces measurable F1 score against ground truth"
  artifacts:
    - path: "src/event_dedup/canonical/synthesizer.py"
      provides: "synthesize_canonical() with field strategies and provenance"
      exports: ["synthesize_canonical"]
    - path: "src/event_dedup/canonical/enrichment.py"
      provides: "enrich_canonical() with downgrade prevention"
      exports: ["enrich_canonical"]
    - path: "tests/test_synthesizer.py"
      provides: "Unit tests for all field selection strategies"
      min_lines: 100
    - path: "tests/test_enrichment.py"
      provides: "Unit tests for enrichment with downgrade prevention"
      min_lines: 50
    - path: "tests/test_end_to_end.py"
      provides: "Integration test: events -> scoring -> clustering -> canonical + evaluation"
      min_lines: 60
  key_links:
    - from: "src/event_dedup/canonical/synthesizer.py"
      to: "src/event_dedup/matching/config.py"
      via: "CanonicalConfig with FieldStrategies drives field selection"
      pattern: "CanonicalConfig|FieldStrategies|field_strategies"
    - from: "src/event_dedup/canonical/enrichment.py"
      to: "src/event_dedup/canonical/synthesizer.py"
      via: "Re-runs synthesize_canonical with all sources, then checks for downgrades"
      pattern: "synthesize_canonical"
    - from: "src/event_dedup/matching/pipeline.py"
      to: "src/event_dedup/clustering/graph_cluster.py"
      via: "run_full_pipeline calls cluster_matches after scoring"
      pattern: "cluster_matches|ClusterResult"
    - from: "src/event_dedup/matching/pipeline.py"
      to: "src/event_dedup/canonical/synthesizer.py"
      via: "run_full_pipeline calls synthesize_canonical for each cluster"
      pattern: "synthesize_canonical"
    - from: "tests/test_end_to_end.py"
      to: "src/event_dedup/evaluation/harness.py"
      via: "Uses updated harness for F1 measurement"
      pattern: "generate_predictions_multisignal|compute_metrics"
---

<objective>
Build canonical event synthesis (best-field selection with provenance tracking), enrichment (update canonicals without downgrading), wire the full end-to-end pipeline (blocking -> scoring -> clustering -> synthesis), and update the evaluation harness to measure multi-signal matching F1 against ground truth.

Purpose: This plan completes the Phase 2 deliverable. The synthesizer creates the actual canonical events from clusters. The enrichment engine handles the incremental case (new source arrives). The end-to-end pipeline connects all pieces. The evaluation harness measures whether the whole thing actually works.

Output: Canonical synthesis + enrichment, full pipeline orchestrator, updated evaluation harness, end-to-end integration tests with F1 measurement.
</objective>

<execution_context>
@.planning/phases/02/02-RESEARCH.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/phases/02/02-CONTEXT.md
@.planning/phases/02/02-RESEARCH.md
@.planning/phases/02/02-01-SUMMARY.md
@.planning/phases/02/02-02-SUMMARY.md
@.planning/phases/02/02-03-SUMMARY.md

<interfaces>
<!-- From Plan 02-01 -->

From src/event_dedup/matching/config.py:
```python
class FieldStrategies(BaseModel):
    title: str = "longest_non_generic"
    short_description: str = "longest"
    description: str = "longest"
    highlights: str = "union"
    location_name: str = "most_complete"
    location_city: str = "most_frequent"
    location_street: str = "most_complete"
    geo: str = "highest_confidence"
    categories: str = "union"
    is_family_event: str = "any_true"
    is_child_focused: str = "any_true"
    admission_free: str = "any_true"

class CanonicalConfig(BaseModel):
    field_strategies: FieldStrategies = FieldStrategies()

class MatchingConfig(BaseModel):
    scoring: ScoringWeights; thresholds: ThresholdConfig; geo: GeoConfig
    date: DateConfig; title: TitleConfig; cluster: ClusterConfig; canonical: CanonicalConfig
```

<!-- From Plan 02-02 -->

From src/event_dedup/matching/pipeline.py:
```python
@dataclass
class MatchDecisionRecord:
    event_id_a: str; event_id_b: str; signals: SignalScores
    combined_score_value: float; decision: str; tier: str = "deterministic"

@dataclass
class MatchResult:
    decisions: list[MatchDecisionRecord]; pair_stats: CandidatePairStats
    match_count: int; ambiguous_count: int; no_match_count: int

def score_candidate_pairs(events: list[dict], config: MatchingConfig) -> MatchResult: ...
def get_match_pairs(result: MatchResult) -> set[tuple[str, str]]: ...
```

<!-- From Plan 02-03 -->

From src/event_dedup/clustering/graph_cluster.py:
```python
@dataclass
class ClusterResult:
    clusters: list[set[str]]; flagged_clusters: list[set[str]]
    singleton_count: int; total_cluster_count: int

def cluster_matches(decisions: list[MatchDecisionRecord], all_event_ids: list[str],
                    config: ClusterConfig, events_by_id: dict[str, dict] | None = None) -> ClusterResult: ...
```

<!-- Existing evaluation harness -->

From src/event_dedup/evaluation/harness.py:
```python
def generate_predictions_from_events(events: list[dict], config: EvaluationConfig) -> set[tuple[str, str]]: ...
async def load_ground_truth(session: AsyncSession) -> tuple[set[tuple[str, str]], set[tuple[str, str]]]: ...
```

From src/event_dedup/evaluation/metrics.py:
```python
def compute_metrics(predicted_same, ground_truth_same, ground_truth_different) -> MetricsResult: ...
```

<!-- Source event dict shape (what scorers and synthesizer expect) -->
Event dict keys used across the pipeline:
```python
{
    "id": str,                           # Primary key
    "title": str,                        # Original title (for canonical synthesis)
    "title_normalized": str | None,      # For title_score
    "short_description": str | None,     # Original (for synthesis)
    "short_description_normalized": str | None,  # For description_score
    "description": str | None,           # Original (for synthesis)
    "highlights": list | None,           # For synthesis (union)
    "location_name": str | None,         # For synthesis
    "location_city": str | None,         # For synthesis
    "location_district": str | None,     # For synthesis
    "location_street": str | None,       # For synthesis
    "location_zipcode": str | None,      # For synthesis
    "geo_latitude": float | None,        # For geo_score and synthesis
    "geo_longitude": float | None,       # For geo_score and synthesis
    "geo_confidence": float | None,      # For geo_score and synthesis
    "dates": list[dict],                 # [{date, start_time, end_time, end_date}]
    "source_code": str,                  # For cross-source filtering
    "source_type": str,                  # For stats
    "blocking_keys": list[str],          # For candidate pair generation
    "categories": list | None,           # For synthesis (union)
    "is_family_event": bool | None,      # For synthesis (any_true)
    "is_child_focused": bool | None,     # For synthesis (any_true)
    "admission_free": bool | None,       # For synthesis (any_true)
}
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Canonical Event Synthesis with Field Strategies and Provenance</name>
  <files>
    src/event_dedup/canonical/__init__.py
    src/event_dedup/canonical/synthesizer.py
    src/event_dedup/canonical/enrichment.py
    tests/test_synthesizer.py
    tests/test_enrichment.py
  </files>
  <action>
1. **Create `src/event_dedup/canonical/__init__.py`**:
   - Re-export: `from .synthesizer import synthesize_canonical` and `from .enrichment import enrich_canonical`

2. **Create `src/event_dedup/canonical/synthesizer.py`**:
   - Import `CanonicalConfig` from `event_dedup.matching.config`
   - Function `synthesize_canonical(source_events: list[dict], config: CanonicalConfig | None = None) -> dict`:
     - If config is None, use `CanonicalConfig()` (defaults)
     - Must have at least 1 source event; if 1 event, just copy fields + set provenance to that event's id
     - Initialize `result = {}` and `provenance = {}`
     - **title**: `_select_longest_non_generic(source_events, "title", min_length=10)` -- prefer titles >= 10 chars (avoids generic "Fasnet" when a full title exists). Fallback to longest. Returns (value, source_id).
     - **short_description**: `_select_longest(source_events, "short_description")` -- longest non-empty value
     - **description**: `_select_longest(source_events, "description")` -- longest non-empty value
     - **highlights**: `_select_union_lists(source_events, "highlights")` -- union of all highlight items, deduplicated. Provenance = "union_all_sources"
     - **location_name**: `_select_most_complete(source_events, "location_name")` -- longest non-empty
     - **location_city**: `_select_most_frequent(source_events, "location_city")` -- most common non-empty value. Ties broken by first occurrence.
     - **location_district**: `_select_most_complete(source_events, "location_district")`
     - **location_street**: `_select_most_complete(source_events, "location_street")`
     - **location_zipcode**: `_select_most_complete(source_events, "location_zipcode")`
     - **geo (latitude, longitude, confidence)**: `_select_best_geo(source_events)` -- event with highest geo_confidence. Returns dict with geo_latitude, geo_longitude, geo_confidence, and provenance source_id.
     - **dates**: `_union_dates(source_events)` -- collect all unique dates (by date+start_time+end_time+end_date tuple), deduplicate. Provenance = "union_all_sources".
     - **categories**: `_select_union_lists(source_events, "categories")` -- union, deduplicated
     - **booleans** (is_family_event, is_child_focused, admission_free): `any(e.get(field) for e in source_events)`. Provenance = first event that has True, else first event.
     - Set `result["field_provenance"] = provenance`
     - Set `result["source_count"] = len(source_events)`
     - Return result
   - Helper functions (all pure, no DB access):
     - `_select_longest(events, field) -> tuple[str | None, str]` -- longest non-empty value. Returns (value, source_event_id).
     - `_select_longest_non_generic(events, field, min_length) -> tuple[str, str]` -- filter to values >= min_length first. If any exist, return longest of those. Else return longest regardless.
     - `_select_union_lists(events, field) -> tuple[list, str]` -- flatten all lists, deduplicate (preserving order). Provenance = "union_all_sources".
     - `_select_most_complete(events, field) -> tuple[str | None, str]` -- longest non-empty string value. Same as _select_longest but semantic naming for location fields.
     - `_select_most_frequent(events, field) -> tuple[str | None, str]` -- count non-empty values, return most common. Use `collections.Counter`.
     - `_select_best_geo(events) -> tuple[dict, str]` -- event with highest geo_confidence where lat/lon are not None. Returns (geo_dict, source_id).
     - `_union_dates(events) -> list[dict]` -- collect all date dicts from all events. Deduplicate by (date, start_time, end_time, end_date) tuple. Return as list of dicts.

3. **Create `src/event_dedup/canonical/enrichment.py`**:
   - Import `synthesize_canonical` from `.synthesizer`, `CanonicalConfig` from `event_dedup.matching.config`
   - Function `enrich_canonical(existing_canonical: dict, all_sources: list[dict], config: CanonicalConfig | None = None) -> dict`:
     - If config is None, use `CanonicalConfig()` (defaults)
     - Re-run `synthesize_canonical(all_sources, config)` with ALL sources (including new ones)
     - **Downgrade prevention**: For text fields (title, short_description, description):
       - Compare `len(existing_canonical.get(field) or "")` with `len(new_canonical.get(field) or "")`
       - If existing was longer: keep existing value AND its provenance entry
     - **Highlights/categories**: union can only grow, never shrink (synthesis does union already, so this is automatic)
     - **Geo**: highest confidence always wins (synthesis already picks highest)
     - **Dates**: union can only grow (synthesis does union already)
     - **Booleans**: any_true can only go from False->True, never True->False (synthesis already does any_true)
     - Set `new_canonical["version"] = existing_canonical.get("version", 1) + 1`
     - Set `new_canonical["source_count"] = len(all_sources)`
     - Return `new_canonical`

4. **Create `tests/test_synthesizer.py`** with comprehensive tests:
   - **Test: Single source event** -- all fields copied directly, provenance points to that event
   - **Test: Title longest_non_generic** -- 3 sources with titles "Fasnet" (6 chars), "Fasnet Waldkirch" (16 chars), "Grosse Fasnet Waldkirch 2026" (28 chars) -> selects 28-char title
   - **Test: Title fallback when all short** -- all titles < 10 chars -> picks longest anyway
   - **Test: Description longest** -- 3 sources, one with long description, two with shorter -> picks longest
   - **Test: Description when all None** -- returns None, provenance still set
   - **Test: Highlights union** -- source A has ["musik"], source B has ["tanz", "musik"] -> result = ["musik", "tanz"] (deduplicated)
   - **Test: Location city most_frequent** -- 3 sources: 2 say "Waldkirch", 1 says "Freiburg" -> "Waldkirch"
   - **Test: Geo highest_confidence** -- source A has confidence 0.7, source B has 0.9 -> picks B's geo
   - **Test: Geo when all missing** -- returns None coordinates
   - **Test: Dates union** -- source A has [Feb 12], source B has [Feb 12, Feb 13] -> union = [Feb 12, Feb 13]
   - **Test: Dates deduplication** -- same date from two sources appears once
   - **Test: Categories union** -- source A has ["carnival"], source B has ["music", "carnival"] -> ["carnival", "music"]
   - **Test: Boolean any_true** -- source A has is_family_event=False, source B has True -> True
   - **Test: Provenance tracking** -- verify every field in field_provenance maps to a source event id
   - **Test: source_count** -- 3 source events -> source_count=3

5. **Create `tests/test_enrichment.py`**:
   - **Test: Enrichment with better title** -- existing canonical has short title, new source has longer title -> canonical title updated
   - **Test: Enrichment preserves better existing title** -- existing has long title, new source has shorter -> title NOT downgraded
   - **Test: Enrichment preserves existing description** -- existing has long description, new source has none -> description kept
   - **Test: Enrichment adds new highlights** -- existing has ["musik"], new source adds ["tanz"] -> result = ["musik", "tanz"]
   - **Test: Enrichment adds new dates** -- existing covers Feb 12, new source adds Feb 13 -> both dates present
   - **Test: Enrichment upgrades geo** -- existing has confidence 0.7, new source has 0.9 -> geo updated
   - **Test: Version increment** -- existing version=1 -> enriched version=2
   - **Test: Source count update** -- existing source_count=2, 3 total sources -> source_count=3
  </action>
  <verify>
    <automated>cd /Users/svenkarl/workspaces/event-deduplication && uv run pytest tests/test_synthesizer.py tests/test_enrichment.py -x -v</automated>
  </verify>
  <done>
    - `synthesize_canonical` selects best field from each source using configured strategies
    - Field provenance tracks which source_event_id contributed each canonical field
    - `enrich_canonical` re-synthesizes without downgrading existing good data
    - Version increments on enrichment
    - All tests pass covering every field strategy and enrichment scenario
  </done>
</task>

<task type="auto">
  <name>Task 2: Full Pipeline Orchestrator + Updated Evaluation Harness + End-to-End Tests</name>
  <files>
    src/event_dedup/matching/pipeline.py
    src/event_dedup/evaluation/harness.py
    tests/test_end_to_end.py
  </files>
  <action>
1. **Update `src/event_dedup/matching/pipeline.py`** (ADD to existing file, do not remove existing functions):
   - Import `cluster_matches`, `ClusterResult` from `event_dedup.clustering.graph_cluster`
   - Import `synthesize_canonical` from `event_dedup.canonical.synthesizer`
   - Import `MatchingConfig` from `.config`
   - `@dataclass` class `PipelineResult`:
     - `match_result: MatchResult` (from scoring phase)
     - `cluster_result: ClusterResult` (from clustering phase)
     - `canonical_events: list[dict]` (synthesized canonicals)
     - `canonical_count: int`
     - `flagged_count: int`
   - Function `run_full_pipeline(events: list[dict], config: MatchingConfig) -> PipelineResult`:
     - This is a PURE FUNCTION. No DB access. Full pipeline in memory.
     - Step 1: Score candidate pairs: `match_result = score_candidate_pairs(events, config)`
     - Step 2: Cluster matches: `all_event_ids = [e["id"] for e in events]`, `events_by_id = {e["id"]: e for e in events}`, `cluster_result = cluster_matches(match_result.decisions, all_event_ids, config.cluster, events_by_id)`
     - Step 3: Synthesize canonical events for each cluster:
       - For each cluster in `cluster_result.clusters`:
         - `sources = [events_by_id[eid] for eid in cluster]`
         - `canonical = synthesize_canonical(sources, config.canonical)`
         - `canonical["needs_review"] = False`
         - `canonical["match_confidence"]` = average combined_score of match decisions within this cluster (from match_result.decisions where both event_ids are in the cluster). For singletons, set to None.
         - Append to canonical_events list
       - For each flagged cluster in `cluster_result.flagged_clusters`:
         - Same synthesis, but `canonical["needs_review"] = True`
         - Append to canonical_events list
     - Return `PipelineResult(match_result=match_result, cluster_result=cluster_result, canonical_events=canonical_events, canonical_count=len(canonical_events), flagged_count=len(cluster_result.flagged_clusters))`
   - Function `extract_predicted_pairs(result: PipelineResult) -> set[tuple[str, str]]`:
     - For each canonical event that has source_count > 1:
       - Get all source event IDs from the cluster (not from canonical dict -- we need to find the matching cluster)
     - Alternative simpler approach: just use `get_match_pairs(result.match_result)` -- this gives the pairs that were scored as "match", which is exactly what evaluation needs
     - Return the set of canonically ordered pairs

2. **Update `src/event_dedup/evaluation/harness.py`** (ADD a new function, keep existing):
   - Import `MatchingConfig`, `load_matching_config` from `event_dedup.matching.config`
   - Import `score_candidate_pairs`, `get_match_pairs` from `event_dedup.matching.pipeline`
   - Add function `generate_predictions_multisignal(events: list[dict], config: MatchingConfig) -> set[tuple[str, str]]`:
     - Call `score_candidate_pairs(events, config)` to get MatchResult
     - Return `get_match_pairs(match_result)` -- set of (id_a, id_b) where decision == "match"
     - This replaces the Phase 1 title-only `generate_predictions_from_events` for evaluation purposes
   - Add function `run_multisignal_evaluation(session: AsyncSession, config: MatchingConfig) -> EvaluationResult`:
     - Load ground truth: `gt_same, gt_diff = await load_ground_truth(session)`
     - Load all source events with dates from DB (same query pattern as existing `generate_predictions`)
     - Convert to event dicts (including ALL fields needed by scorers: id, title_normalized, short_description_normalized, source_code, blocking_keys, dates as list of dicts, geo_latitude, geo_longitude, geo_confidence)
     - The dates conversion is critical: for each SourceEvent, convert its `dates` relationship into `[{"date": str(d.date), "start_time": str(d.start_time) if d.start_time else None, "end_time": str(d.end_time) if d.end_time else None, "end_date": str(d.end_date) if d.end_date else None} for d in evt.dates]`
     - Call `generate_predictions_multisignal(events, config)` to get predictions
     - Compute metrics: `metrics = compute_metrics(predicted, gt_same, gt_diff)`
     - Return `EvaluationResult(config=config, metrics=metrics, false_positive_pairs=..., false_negative_pairs=...)`
     - Note: Reuse the false positive/negative analysis pattern from existing `run_evaluation`

3. **Create `tests/test_end_to_end.py`** with integration tests:
   - Helper `make_test_event(id, title, title_normalized, source_code, blocking_keys, dates, city=None, geo_lat=None, geo_lon=None, geo_conf=None, short_desc=None, short_desc_norm=None, description=None, highlights=None, categories=None, **kwargs)` -> dict with ALL fields the pipeline expects.
   - **Test: Full pipeline with 2 duplicate events**
     - Create event A (source "bwb") and event B (source "fvs") that represent the same real event: same date, same city, similar title ("Fasnetumzug Waldkirch" vs "Waldkircher Fasnetumzug"), same geo
     - Run `run_full_pipeline(events, MatchingConfig())`
     - Verify: 1 canonical event (not 2), source_count=2, field_provenance contains both source IDs
   - **Test: Full pipeline with 2 different events**
     - Create event A (source "bwb") and event B (source "fvs") on the same date but completely different titles and locations
     - Run `run_full_pipeline`
     - Verify: 2 canonical events (singletons), source_count=1 each
   - **Test: Full pipeline with 4 events, 2 clusters**
     - Events A+B are duplicates (same event from 2 sources), C+D are duplicates (different event from 2 sources)
     - Verify: 2 canonical events, each with source_count=2
   - **Test: Full pipeline with transitive match (A-B, B-C)**
     - A and C don't share a blocking key directly, but B shares keys with both
     - A-B match, B-C match -> one cluster {A,B,C}
     - Verify: 1 canonical with source_count=3
   - **Test: Canonical event has best fields**
     - Source A has short title but long description, source B has long title but no description
     - Verify canonical has B's title and A's description, with correct provenance
   - **Test: Blocking reduction stats**
     - Create 6 events from 3 sources with blocking keys that create limited pairs
     - Verify pair_stats.reduction_pct > 0 (blocking actually reduced pairs)
   - **Test: extract_predicted_pairs returns match pairs for evaluation**
     - Run pipeline on events with known matches
     - Verify extracted pairs match expected pairs
   - **Test: generate_predictions_multisignal pure function**
     - Create event dicts, run generate_predictions_multisignal
     - Verify it returns same pairs as get_match_pairs from pipeline
  </action>
  <verify>
    <automated>cd /Users/svenkarl/workspaces/event-deduplication && uv run pytest tests/test_end_to_end.py -x -v</automated>
  </verify>
  <done>
    - `run_full_pipeline` orchestrates: blocking -> scoring -> clustering -> synthesis in a single pure function
    - Each canonical event has best fields from all sources, with field_provenance
    - Flagged clusters produce canonical events with needs_review=True
    - match_confidence is set from average match decision scores within each cluster
    - `generate_predictions_multisignal` enables evaluation with multi-signal scoring
    - `run_multisignal_evaluation` connects to ground truth for F1 measurement
    - All integration tests pass
  </done>
</task>

</tasks>

<verification>
```bash
# All plan 02-04 tests pass
cd /Users/svenkarl/workspaces/event-deduplication && uv run pytest tests/test_synthesizer.py tests/test_enrichment.py tests/test_end_to_end.py -x -v

# Full pipeline import chain works
cd /Users/svenkarl/workspaces/event-deduplication && uv run python -c "
from event_dedup.matching.pipeline import run_full_pipeline, PipelineResult, extract_predicted_pairs
from event_dedup.canonical import synthesize_canonical, enrich_canonical
from event_dedup.evaluation.harness import generate_predictions_multisignal
print('Full pipeline import chain OK')
"

# ALL tests across the entire project pass
cd /Users/svenkarl/workspaces/event-deduplication && uv run pytest tests/ -x -v --timeout=60

# Quick smoke test: pipeline runs with minimal data
cd /Users/svenkarl/workspaces/event-deduplication && uv run python -c "
from event_dedup.matching.pipeline import run_full_pipeline
from event_dedup.matching.config import MatchingConfig

events = [
    {'id': 'ev-a', 'title': 'Test Event', 'title_normalized': 'test event',
     'short_description_normalized': '', 'source_code': 'src_a',
     'blocking_keys': ['dc|2026-02-12|testcity'], 'dates': [{'date': '2026-02-12'}],
     'geo_latitude': None, 'geo_longitude': None, 'geo_confidence': None,
     'source_type': 'artikel', 'short_description': None, 'description': None,
     'highlights': None, 'location_name': None, 'location_city': 'TestCity',
     'location_district': None, 'location_street': None, 'location_zipcode': None,
     'categories': None, 'is_family_event': None, 'is_child_focused': None,
     'admission_free': None},
    {'id': 'ev-b', 'title': 'Test Event', 'title_normalized': 'test event',
     'short_description_normalized': '', 'source_code': 'src_b',
     'blocking_keys': ['dc|2026-02-12|testcity'], 'dates': [{'date': '2026-02-12'}],
     'geo_latitude': None, 'geo_longitude': None, 'geo_confidence': None,
     'source_type': 'terminliste', 'short_description': None, 'description': None,
     'highlights': None, 'location_name': None, 'location_city': 'TestCity',
     'location_district': None, 'location_street': None, 'location_zipcode': None,
     'categories': None, 'is_family_event': None, 'is_child_focused': None,
     'admission_free': None},
]

result = run_full_pipeline(events, MatchingConfig())
print(f'Canonicals: {result.canonical_count}, Matches: {result.match_result.match_count}')
print(f'Clusters: {result.cluster_result.total_cluster_count}')
"
```
</verification>

<success_criteria>
1. `synthesize_canonical` correctly applies all field strategies: longest_non_generic for title, longest for descriptions, union for highlights/categories, most_frequent for city, highest_confidence for geo, any_true for booleans
2. `field_provenance` maps every field to a source_event_id (or "union_all_sources")
3. `enrich_canonical` re-synthesizes without downgrading text fields, version increments
4. `run_full_pipeline` is a pure function: events in, PipelineResult out (no DB access)
5. Canonical events from flagged clusters have needs_review=True
6. `generate_predictions_multisignal` enables F1 evaluation with multi-signal scoring
7. `run_multisignal_evaluation` connects to ground truth DB for end-to-end measurement
8. All tests pass: test_synthesizer, test_enrichment, test_end_to_end
9. ALL prior tests still pass (full test suite green)
</success_criteria>

<output>
After completion, create `.planning/phases/02/02-04-SUMMARY.md`
</output>
