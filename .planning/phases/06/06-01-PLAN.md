---
phase: 06-review-operations
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/event_dedup/models/audit_log.py
  - src/event_dedup/models/__init__.py
  - config/alembic/versions/004_add_audit_log.py
  - src/event_dedup/canonical/helpers.py
  - src/event_dedup/api/schemas.py
  - src/event_dedup/review/__init__.py
  - src/event_dedup/review/operations.py
  - src/event_dedup/api/routes/review.py
  - src/event_dedup/api/routes/dashboard.py
  - src/event_dedup/api/app.py
  - tests/test_review_api.py
autonomous: true
requirements: [REV-01, REV-02, REV-03, REV-04, REV-05]

must_haves:
  truths:
    - "POST /api/review/split detaches a source event from its canonical and creates a new canonical or assigns to an existing one, atomically in a single transaction"
    - "POST /api/review/merge combines two canonical events into one, re-synthesizing canonical fields from all combined sources, and deletes the donor canonical"
    - "GET /api/review/queue returns low-confidence and needs_review canonical events sorted by uncertainty (most ambiguous first), paginated"
    - "POST /api/review/queue/{id}/dismiss clears needs_review flag and logs the action"
    - "Every split, merge, and dismiss operation creates an AuditLog entry with action_type, operator, and details JSON"
    - "GET /api/audit-log returns paginated audit entries filterable by canonical_event_id and action_type"
    - "GET /api/dashboard/stats returns aggregate file processing stats, match decision distribution, and canonical event summary"
    - "GET /api/dashboard/processing-history returns daily time-series data for processing trends"
    - "Split handles edge cases: empty canonical after split (delete it), duplicate source links on target (skip)"
    - "Merge handles edge cases: duplicate source links between donor and target (skip duplicates)"
  artifacts:
    - path: "src/event_dedup/models/audit_log.py"
      provides: "AuditLog SQLAlchemy model"
      contains: "class AuditLog"
    - path: "config/alembic/versions/004_add_audit_log.py"
      provides: "Alembic migration creating audit_log table and dashboard indexes"
    - path: "src/event_dedup/canonical/helpers.py"
      provides: "source_event_to_dict reusable helper and update_canonical_from_dict helper"
      contains: "source_event_to_dict"
    - path: "src/event_dedup/review/operations.py"
      provides: "Core split and merge business logic as async functions"
      contains: "split_source_from_canonical, merge_canonical_events"
    - path: "src/event_dedup/api/routes/review.py"
      provides: "Review API endpoints: split, merge, queue, dismiss, audit-log"
      contains: "router"
    - path: "src/event_dedup/api/routes/dashboard.py"
      provides: "Dashboard API endpoints: stats, processing-history"
      contains: "router"
    - path: "src/event_dedup/api/schemas.py"
      provides: "Extended with SplitRequest, MergeRequest, SplitResponse, MergeResponse, AuditLogEntry, DashboardStats, ProcessingHistoryEntry"
    - path: "tests/test_review_api.py"
      provides: "API integration tests for all review and dashboard endpoints"
  key_links:
    - from: "src/event_dedup/review/operations.py"
      to: "src/event_dedup/canonical/synthesizer.py"
      via: "synthesize_canonical() called after link manipulation"
      pattern: "synthesize_canonical"
    - from: "src/event_dedup/review/operations.py"
      to: "src/event_dedup/canonical/helpers.py"
      via: "source_event_to_dict() converts ORM objects to dicts for synthesis"
      pattern: "source_event_to_dict"
    - from: "src/event_dedup/review/operations.py"
      to: "src/event_dedup/models/audit_log.py"
      via: "AuditLog entries created within same transaction"
      pattern: "AuditLog"
    - from: "src/event_dedup/api/routes/review.py"
      to: "src/event_dedup/review/operations.py"
      via: "Route handlers call operation functions"
      pattern: "split_source_from_canonical|merge_canonical_events"
    - from: "src/event_dedup/api/app.py"
      to: "src/event_dedup/api/routes/review.py"
      via: "app.include_router(review_router)"
      pattern: "include_router"
    - from: "src/event_dedup/api/app.py"
      to: "src/event_dedup/api/routes/dashboard.py"
      via: "app.include_router(dashboard_router)"
      pattern: "include_router"
---

<objective>
Build the complete backend for manual review operations: AuditLog model with migration, source_event_to_dict helper, review operations service (split/merge logic), and all API routes (review split/merge/queue/dismiss, audit-log, dashboard stats/processing-history) with integration tests.

Purpose: This plan creates all backend infrastructure that the frontend (Plan 06-02) will consume. Operators need to correct grouping mistakes through split/merge operations, work through a prioritized review queue, and monitor system health via a dashboard -- all powered by these API endpoints.

Output: AuditLog model + Alembic migration, reusable helpers in `canonical/helpers.py`, review operations service in `review/operations.py`, two new route modules (review.py, dashboard.py) registered in app.py, extended Pydantic schemas, and comprehensive API integration tests.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06/06-RESEARCH.md

@src/event_dedup/models/base.py
@src/event_dedup/models/__init__.py
@src/event_dedup/models/canonical_event.py
@src/event_dedup/models/canonical_event_source.py
@src/event_dedup/models/source_event.py
@src/event_dedup/models/match_decision.py
@src/event_dedup/models/file_ingestion.py
@src/event_dedup/canonical/synthesizer.py
@src/event_dedup/canonical/enrichment.py
@src/event_dedup/worker/persistence.py
@src/event_dedup/api/app.py
@src/event_dedup/api/deps.py
@src/event_dedup/api/schemas.py
@src/event_dedup/api/routes/canonical_events.py
@config/alembic/versions/003_add_ai_matching_tables.py
@tests/conftest.py
@tests/test_api.py

<interfaces>
<!-- Key types and contracts the executor needs. Extracted from codebase. -->

From src/event_dedup/models/base.py:
```python
class Base(DeclarativeBase):
    metadata = sa.MetaData(naming_convention={
        "ix": "ix_%(column_0_label)s",
        "uq": "uq_%(table_name)s_%(column_0_name)s",
        "ck": "ck_%(table_name)s_%(constraint_name)s",
        "fk": "fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s",
        "pk": "pk_%(table_name)s",
    })
```

From src/event_dedup/models/canonical_event.py:
```python
class CanonicalEvent(Base):
    __tablename__ = "canonical_events"
    id: Mapped[int] = mapped_column(sa.Integer, primary_key=True, autoincrement=True)
    title: Mapped[str] = mapped_column(sa.String)
    # ... all content fields ...
    field_provenance: Mapped[dict | None] = mapped_column(sa.JSON, nullable=True)
    source_count: Mapped[int] = mapped_column(sa.Integer, default=1)
    match_confidence: Mapped[float | None] = mapped_column(sa.Float, nullable=True)
    needs_review: Mapped[bool] = mapped_column(sa.Boolean, default=False)
    version: Mapped[int] = mapped_column(sa.Integer, default=1)
    sources: Mapped[list[CanonicalEventSource]] = relationship(
        "CanonicalEventSource", back_populates="canonical_event", cascade="all, delete-orphan"
    )
```

From src/event_dedup/models/canonical_event_source.py:
```python
class CanonicalEventSource(Base):
    __tablename__ = "canonical_event_sources"
    id: Mapped[int] = mapped_column(sa.Integer, primary_key=True, autoincrement=True)
    canonical_event_id: Mapped[int] = mapped_column(sa.Integer, sa.ForeignKey("canonical_events.id", ondelete="CASCADE"))
    source_event_id: Mapped[str] = mapped_column(sa.String, sa.ForeignKey("source_events.id"))
    canonical_event: Mapped[CanonicalEvent] = relationship("CanonicalEvent", back_populates="sources")
    source_event: Mapped[SourceEvent] = relationship("SourceEvent")
    __table_args__ = (
        sa.UniqueConstraint("canonical_event_id", "source_event_id", name="uq_canonical_event_sources_pair"),
    )
```

From src/event_dedup/canonical/synthesizer.py:
```python
def synthesize_canonical(source_events: list[dict], config: CanonicalConfig | None = None) -> dict:
    """Pure function: list[dict] -> dict with all merged fields + field_provenance + source_count."""
```

From src/event_dedup/canonical/enrichment.py:
```python
def enrich_canonical(existing_canonical: dict, all_sources: list[dict], config: CanonicalConfig | None = None) -> dict:
    """Re-synthesize with downgrade prevention. Returns updated dict with incremented version."""
```

From src/event_dedup/worker/persistence.py (source-to-dict pattern, lines 35-70):
```python
# This dict mapping is what source_event_to_dict should extract:
{
    "id": evt.id, "title": evt.title, "short_description": evt.short_description,
    "description": evt.description, "highlights": evt.highlights,
    "location_name": evt.location_name, "location_city": evt.location_city,
    "location_district": evt.location_district, "location_street": evt.location_street,
    "location_zipcode": evt.location_zipcode,
    "geo_latitude": evt.geo_latitude, "geo_longitude": evt.geo_longitude,
    "geo_confidence": evt.geo_confidence,
    "source_code": evt.source_code, "source_type": evt.source_type,
    "categories": evt.categories,
    "is_family_event": evt.is_family_event, "is_child_focused": evt.is_child_focused,
    "admission_free": evt.admission_free,
    "dates": [{"date": str(d.date), "start_time": str(d.start_time) if d.start_time else None,
               "end_time": str(d.end_time) if d.end_time else None,
               "end_date": str(d.end_date) if d.end_date else None} for d in evt.dates],
}
```

From src/event_dedup/api/schemas.py:
```python
class PaginatedResponse(BaseModel, Generic[T]):
    items: list[T]
    total: int
    page: int
    size: int
    pages: int

class CanonicalEventSummary(BaseModel):
    model_config = ConfigDict(from_attributes=True)
    id: int
    title: str
    location_city: str | None = None
    dates: list | None = None
    categories: list | None = None
    source_count: int
    match_confidence: float | None = None
    needs_review: bool
```

From src/event_dedup/api/deps.py:
```python
async def get_db() -> AsyncGenerator[AsyncSession, None]:
    factory = get_session_factory()
    async with factory() as session:
        yield session
```

From src/event_dedup/models/file_ingestion.py:
```python
class FileIngestion(Base):
    __tablename__ = "file_ingestions"
    id: Mapped[int]
    filename: Mapped[str]
    event_count: Mapped[int]
    status: Mapped[str]  # "completed", "error"
    ingested_at: Mapped[datetime]
```

From src/event_dedup/models/match_decision.py:
```python
class MatchDecision(Base):
    __tablename__ = "match_decisions"
    decision: Mapped[str]  # "match", "no_match", "ambiguous"
    tier: Mapped[str]  # "deterministic", "ai"
    decided_at: Mapped[datetime]
```

From tests/conftest.py:
```python
# Test pattern: SQLite in-memory, api_client fixture, seeded_db fixture
@pytest.fixture
async def api_client(test_engine, test_session_factory):
    async def override_get_db():
        async with test_session_factory() as session:
            yield session
    app.dependency_overrides[get_db] = override_get_db
    transport = ASGITransport(app=app)
    async with AsyncClient(transport=transport, base_url="http://test") as client:
        yield client
    app.dependency_overrides.clear()
```

Alembic revision chain: 003_ai_matching is the latest. New migration must use:
```python
revision = "004_audit_log"
down_revision = "003_ai_matching"
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: AuditLog model, migration, helpers, and review operations service</name>
  <files>
    src/event_dedup/models/audit_log.py
    src/event_dedup/models/__init__.py
    config/alembic/versions/004_add_audit_log.py
    src/event_dedup/canonical/helpers.py
    src/event_dedup/review/__init__.py
    src/event_dedup/review/operations.py
  </files>
  <action>
    **1. Create `src/event_dedup/models/audit_log.py`:**

    Follow the existing model pattern (Base, Mapped, mapped_column). Create an `AuditLog` model:

    ```python
    """Audit log model for tracking manual review operations."""
    from __future__ import annotations

    from datetime import datetime

    import sqlalchemy as sa
    from sqlalchemy.orm import Mapped, mapped_column

    from event_dedup.models.base import Base


    class AuditLog(Base):
        __tablename__ = "audit_log"

        id: Mapped[int] = mapped_column(sa.Integer, primary_key=True, autoincrement=True)
        action_type: Mapped[str] = mapped_column(sa.String)  # "split", "merge", "review_dismiss"
        canonical_event_id: Mapped[int | None] = mapped_column(
            sa.Integer, sa.ForeignKey("canonical_events.id", ondelete="SET NULL"), nullable=True
        )
        source_event_id: Mapped[str | None] = mapped_column(sa.String, nullable=True)
        operator: Mapped[str] = mapped_column(sa.String, default="anonymous")
        details: Mapped[dict | None] = mapped_column(sa.JSON, nullable=True)
        created_at: Mapped[datetime] = mapped_column(
            sa.DateTime, server_default=sa.text("CURRENT_TIMESTAMP")
        )
    ```

    Do NOT add a CheckConstraint for action_type -- SQLite compatibility issues with Alembic check constraints are not worth the complexity. Validation happens in the Pydantic schema layer.

    **2. Update `src/event_dedup/models/__init__.py`:**

    Add `from event_dedup.models.audit_log import AuditLog` import and add `"AuditLog"` to `__all__`.

    **3. Create `config/alembic/versions/004_add_audit_log.py`:**

    ```python
    """Add audit_log table and dashboard query indexes.

    Revision ID: 004_audit_log
    Revises: 003_ai_matching
    Create Date: 2026-02-28
    """
    from alembic import op
    import sqlalchemy as sa

    revision = "004_audit_log"
    down_revision = "003_ai_matching"
    branch_labels = None
    depends_on = None


    def upgrade() -> None:
        op.create_table(
            "audit_log",
            sa.Column("id", sa.Integer(), primary_key=True, autoincrement=True),
            sa.Column("action_type", sa.String(), nullable=False),
            sa.Column(
                "canonical_event_id",
                sa.Integer(),
                sa.ForeignKey("canonical_events.id", ondelete="SET NULL"),
                nullable=True,
            ),
            sa.Column("source_event_id", sa.String(), nullable=True),
            sa.Column("operator", sa.String(), nullable=False, server_default="anonymous"),
            sa.Column("details", sa.JSON(), nullable=True),
            sa.Column("created_at", sa.DateTime(), server_default=sa.text("CURRENT_TIMESTAMP")),
        )
        op.create_index("ix_audit_log_canonical_event_id", "audit_log", ["canonical_event_id"])
        op.create_index("ix_audit_log_created_at", "audit_log", ["created_at"])

        # Indexes for dashboard query performance
        op.create_index("ix_file_ingestions_ingested_at", "file_ingestions", ["ingested_at"])
        op.create_index("ix_match_decisions_decided_at", "match_decisions", ["decided_at"])


    def downgrade() -> None:
        op.drop_index("ix_match_decisions_decided_at")
        op.drop_index("ix_file_ingestions_ingested_at")
        op.drop_index("ix_audit_log_created_at")
        op.drop_index("ix_audit_log_canonical_event_id")
        op.drop_table("audit_log")
    ```

    **4. Create `src/event_dedup/canonical/helpers.py`:**

    Extract a reusable `source_event_to_dict` helper from the pattern in `worker/persistence.py:load_all_events_as_dicts`. This avoids duplicating the 30+ field mapping. Also add `update_canonical_from_dict` to apply a synthesized dict back to an ORM CanonicalEvent object.

    ```python
    """Reusable helpers for canonical event operations."""
    from __future__ import annotations

    import datetime as dt

    from event_dedup.models.canonical_event import CanonicalEvent
    from event_dedup.models.source_event import SourceEvent


    def source_event_to_dict(evt: SourceEvent) -> dict:
        """Convert a SourceEvent ORM object to a dict for synthesize_canonical().

        Matches the format expected by run_full_pipeline and synthesize_canonical.
        Does NOT include normalized fields or blocking_keys (not needed for synthesis).
        """
        return {
            "id": evt.id,
            "title": evt.title,
            "short_description": evt.short_description,
            "description": evt.description,
            "highlights": evt.highlights,
            "location_name": evt.location_name,
            "location_city": evt.location_city,
            "location_district": evt.location_district,
            "location_street": evt.location_street,
            "location_zipcode": evt.location_zipcode,
            "geo_latitude": evt.geo_latitude,
            "geo_longitude": evt.geo_longitude,
            "geo_confidence": evt.geo_confidence,
            "source_code": evt.source_code,
            "source_type": evt.source_type,
            "categories": evt.categories,
            "is_family_event": evt.is_family_event,
            "is_child_focused": evt.is_child_focused,
            "admission_free": evt.admission_free,
            "dates": [
                {
                    "date": str(d.date),
                    "start_time": str(d.start_time) if d.start_time else None,
                    "end_time": str(d.end_time) if d.end_time else None,
                    "end_date": str(d.end_date) if d.end_date else None,
                }
                for d in evt.dates
            ],
        }


    def update_canonical_from_dict(canonical: CanonicalEvent, data: dict) -> None:
        """Apply synthesized canonical dict fields to an ORM CanonicalEvent object.

        Updates all content fields, provenance, and source_count.
        Does NOT modify id, created_at, version, needs_review, or match_confidence.
        """
        canonical.title = data["title"]
        canonical.short_description = data.get("short_description")
        canonical.description = data.get("description")
        canonical.highlights = data.get("highlights")
        canonical.location_name = data.get("location_name")
        canonical.location_city = data.get("location_city")
        canonical.location_district = data.get("location_district")
        canonical.location_street = data.get("location_street")
        canonical.location_zipcode = data.get("location_zipcode")
        canonical.geo_latitude = data.get("geo_latitude")
        canonical.geo_longitude = data.get("geo_longitude")
        canonical.geo_confidence = data.get("geo_confidence")
        canonical.dates = data.get("dates")
        canonical.categories = data.get("categories")
        canonical.is_family_event = data.get("is_family_event")
        canonical.is_child_focused = data.get("is_child_focused")
        canonical.admission_free = data.get("admission_free")
        canonical.field_provenance = data.get("field_provenance")
        canonical.source_count = data.get("source_count", 1)

        # Parse first_date/last_date from string to date objects
        first = data.get("first_date")
        last = data.get("last_date")
        canonical.first_date = dt.date.fromisoformat(first) if first else None
        canonical.last_date = dt.date.fromisoformat(last) if last else None

        canonical.updated_at = dt.datetime.utcnow()
    ```

    **5. Create `src/event_dedup/review/__init__.py`** (empty file).

    **6. Create `src/event_dedup/review/operations.py`:**

    This is the core business logic. Two main async functions, each running inside a transaction:

    ```python
    """Review operations: split and merge canonical events."""
    from __future__ import annotations

    import sqlalchemy as sa
    from fastapi import HTTPException
    from sqlalchemy.ext.asyncio import AsyncSession
    from sqlalchemy.orm import selectinload

    from event_dedup.canonical.helpers import source_event_to_dict, update_canonical_from_dict
    from event_dedup.canonical.synthesizer import synthesize_canonical
    from event_dedup.models.audit_log import AuditLog
    from event_dedup.models.canonical_event import CanonicalEvent
    from event_dedup.models.canonical_event_source import CanonicalEventSource
    from event_dedup.models.source_event import SourceEvent
    ```

    **`split_source_from_canonical(session, canonical_event_id, source_event_id, target_canonical_id, operator)`:**

    Algorithm (all within `async with session.begin()`):

    1. Find and delete the CanonicalEventSource link for (canonical_event_id, source_event_id). Raise 404 if not found.
    2. Query remaining CanonicalEventSource links for the original canonical, eager-loading `.source_event` and `.source_event.dates`.
    3. If zero remaining links: delete the original CanonicalEvent (it's now empty). Set `original_deleted = True`.
    4. If 1+ remaining links: convert remaining source events to dicts via `source_event_to_dict()`, call `synthesize_canonical()` on them, apply result to the original CanonicalEvent via `update_canonical_from_dict()`. Set `needs_review = False` (operator has reviewed it).
    5. Handle the detached source event:
       - If `target_canonical_id` is provided: check that target exists (404 if not). Check if link already exists (skip if so, per pitfall 2). Create new CanonicalEventSource link. Re-synthesize target canonical from all its now-expanded sources.
       - If `target_canonical_id` is None: load the source event with dates. Create a new CanonicalEvent from `synthesize_canonical([source_dict])`. Flush to get the new ID. Create the CanonicalEventSource link. Set `needs_review = False`, `match_confidence = None` (singleton).
    6. Create AuditLog entry with action_type="split", canonical_event_id, source_event_id, operator, and details JSON containing: `{"target_canonical_id": ..., "original_deleted": bool, "new_canonical_id": ... (if created)}`.
    7. Return dict with: `original_canonical_id`, `new_canonical_id` (if created), `target_canonical_id` (if assigned), `original_deleted`.

    **`merge_canonical_events(session, source_canonical_id, target_canonical_id, operator)`:**

    Algorithm (all within `async with session.begin()`):

    1. Load both canonical events (404 if either not found). Verify they are different IDs (400 if same).
    2. Get all CanonicalEventSource links from the source (donor) canonical.
    3. Get existing source_event_ids on the target canonical (to detect duplicates per pitfall 2).
    4. For each link from donor: if source_event_id is NOT already linked to target, create a new CanonicalEventSource for the target. Then delete the link from donor.
    5. Delete the donor CanonicalEvent (its cascade will clean up any remaining orphan links).
    6. Load all source events for the target canonical (now including the moved ones), eager-load dates. Convert to dicts, call `synthesize_canonical()`, apply via `update_canonical_from_dict()`. Set `needs_review = False`, preserve the target's existing `match_confidence`.
    7. Create AuditLog entry with action_type="merge", canonical_event_id=target_canonical_id, operator, details JSON: `{"deleted_canonical_id": source_canonical_id, "sources_moved": count, "new_source_count": ...}`.
    8. Return dict with: `surviving_canonical_id`, `deleted_canonical_id`, `new_source_count`.

    **CRITICAL implementation notes:**
    - Use `async with session.begin()` to wrap each operation in a single transaction
    - Use `selectinload(CanonicalEventSource.source_event).selectinload(SourceEvent.dates)` when loading sources for synthesis -- `source_event_to_dict` accesses `evt.dates`
    - When deleting CanonicalEvent, first explicitly delete its CanonicalEventSource rows (SQLite doesn't enforce CASCADE without PRAGMA foreign_keys -- matches pattern in worker/persistence.py lines 92-94)
    - For the new CanonicalEvent in split, use the same field mapping pattern as `replace_canonical_events` in persistence.py (lines 105-129)
  </action>
  <verify>
    Run `uv run python -c "from event_dedup.models.audit_log import AuditLog; from event_dedup.review.operations import split_source_from_canonical, merge_canonical_events; from event_dedup.canonical.helpers import source_event_to_dict, update_canonical_from_dict; print('All imports OK')"` -- should print "All imports OK" with no errors.
  </verify>
  <done>
    AuditLog model exists with all fields (action_type, canonical_event_id, source_event_id, operator, details JSON, created_at). Migration 004 creates audit_log table and dashboard indexes. source_event_to_dict and update_canonical_from_dict helpers exist. split_source_from_canonical and merge_canonical_events functions exist with full transactional logic.
  </done>
</task>

<task type="auto">
  <name>Task 2: API routes, extended schemas, app registration, and tests</name>
  <files>
    src/event_dedup/api/schemas.py
    src/event_dedup/api/routes/review.py
    src/event_dedup/api/routes/dashboard.py
    src/event_dedup/api/app.py
    tests/test_review_api.py
  </files>
  <action>
    **1. Extend `src/event_dedup/api/schemas.py`:**

    Add the following Pydantic schemas at the bottom of the file (after the existing `PaginatedResponse` class). Follow the existing pattern of `model_config = ConfigDict(from_attributes=True)` for ORM-backed models:

    ```python
    # --- Review operation schemas ---

    class SplitRequest(BaseModel):
        canonical_event_id: int
        source_event_id: str
        target_canonical_id: int | None = None  # None = create new canonical
        operator: str = "anonymous"

    class MergeRequest(BaseModel):
        source_canonical_id: int   # donor -- gets deleted
        target_canonical_id: int   # survivor -- keeps all sources
        operator: str = "anonymous"

    class SplitResponse(BaseModel):
        original_canonical_id: int
        new_canonical_id: int | None = None      # if new canonical was created
        target_canonical_id: int | None = None    # if assigned to existing
        original_deleted: bool = False

    class MergeResponse(BaseModel):
        surviving_canonical_id: int
        deleted_canonical_id: int
        new_source_count: int

    class DismissRequest(BaseModel):
        operator: str = "anonymous"
        reason: str | None = None

    class AuditLogEntry(BaseModel):
        model_config = ConfigDict(from_attributes=True)
        id: int
        action_type: str
        canonical_event_id: int | None
        source_event_id: str | None
        operator: str
        details: dict | None
        created_at: str  # ISO string

    # --- Dashboard schemas ---

    class FileProcessingStats(BaseModel):
        total_files: int
        total_events: int
        completed: int
        errors: int

    class MatchDistribution(BaseModel):
        match: int = 0
        no_match: int = 0
        ambiguous: int = 0

    class CanonicalStats(BaseModel):
        total: int
        needs_review: int
        avg_confidence: float | None

    class DashboardStats(BaseModel):
        files: FileProcessingStats
        matches: MatchDistribution
        canonicals: CanonicalStats

    class ProcessingHistoryEntry(BaseModel):
        date: str
        files_processed: int
        events_ingested: int
        errors: int
    ```

    Note: `AuditLogEntry.created_at` is `str` (not datetime) because the existing pattern uses BeforeValidator for date coercion. For simplicity, convert to ISO string in the route handler or use `model_validate` which will call `str()` on the datetime.

    **2. Create `src/event_dedup/api/routes/review.py`:**

    ```python
    """API routes for manual review operations."""
    from __future__ import annotations

    import math

    import sqlalchemy as sa
    from fastapi import APIRouter, Depends, HTTPException, Query
    from sqlalchemy.ext.asyncio import AsyncSession

    from event_dedup.api.deps import get_db
    from event_dedup.api.schemas import (
        AuditLogEntry,
        CanonicalEventSummary,
        DismissRequest,
        MergeRequest,
        MergeResponse,
        PaginatedResponse,
        SplitRequest,
        SplitResponse,
    )
    from event_dedup.models.audit_log import AuditLog
    from event_dedup.models.canonical_event import CanonicalEvent
    from event_dedup.review.operations import merge_canonical_events, split_source_from_canonical

    router = APIRouter(prefix="/api/review", tags=["review"])
    ```

    **Endpoints:**

    **`POST /api/review/split`** -- accepts `SplitRequest` body, calls `split_source_from_canonical()`, returns `SplitResponse`. The operation function handles its own transaction, so the route just calls it and returns the result.

    **`POST /api/review/merge`** -- accepts `MergeRequest` body, calls `merge_canonical_events()`, returns `MergeResponse`.

    **`GET /api/review/queue`** -- paginated list of canonical events needing review. Query params: `page` (int, default 1), `size` (int, default 20, max 100), `min_sources` (int, default 1, filter out singletons). Query logic:
    ```python
    stmt = (
        sa.select(CanonicalEvent)
        .where(
            sa.or_(
                CanonicalEvent.needs_review == True,
                sa.and_(
                    CanonicalEvent.match_confidence.isnot(None),
                    CanonicalEvent.match_confidence < 0.8,
                    CanonicalEvent.source_count > 1,
                ),
            )
        )
        .where(CanonicalEvent.source_count >= min_sources)
        .order_by(
            CanonicalEvent.needs_review.desc(),
            sa.func.coalesce(CanonicalEvent.match_confidence, 0.0).asc(),
        )
    )
    ```
    Use the same pagination pattern as `list_canonical_events` in `routes/canonical_events.py` (count subquery, offset/limit, PaginatedResponse).

    **`POST /api/review/queue/{event_id}/dismiss`** -- accepts `DismissRequest` body. Load canonical by event_id (404 if not found). Set `needs_review = False`. Create AuditLog with action_type="review_dismiss", details containing the reason. Commit. Return `{"status": "dismissed"}`.

    **Also create a separate audit-log router in the same file** (or use a second `APIRouter` prefix):
    Actually, add the audit-log endpoint on the review router with full path override:

    **`GET /api/audit-log`** -- paginated audit log entries. Query params: `page`, `size`, optional `canonical_event_id` (int), optional `action_type` (str). Order by `created_at DESC`. Return `PaginatedResponse[AuditLogEntry]`. Handle the datetime-to-string conversion: when creating `AuditLogEntry`, convert `created_at` to ISO string using `str(entry.created_at)` or by adding a validator. Simplest approach: iterate results and construct AuditLogEntry manually with `created_at=entry.created_at.isoformat()`.

    **3. Create `src/event_dedup/api/routes/dashboard.py`:**

    ```python
    """API routes for the batch processing dashboard."""
    from __future__ import annotations

    import datetime as dt

    import sqlalchemy as sa
    from fastapi import APIRouter, Depends, Query
    from sqlalchemy.ext.asyncio import AsyncSession

    from event_dedup.api.deps import get_db
    from event_dedup.api.schemas import (
        CanonicalStats,
        DashboardStats,
        FileProcessingStats,
        MatchDistribution,
        ProcessingHistoryEntry,
    )
    from event_dedup.models.canonical_event import CanonicalEvent
    from event_dedup.models.file_ingestion import FileIngestion
    from event_dedup.models.match_decision import MatchDecision

    router = APIRouter(prefix="/api/dashboard", tags=["dashboard"])
    ```

    **`GET /api/dashboard/stats`** -- query param `days` (int, default 30, ge=1, le=365). Returns `DashboardStats`. Three queries:

    1. File processing stats: COUNT total files, SUM event_count, COUNT WHERE status='completed', COUNT WHERE status='error' -- all filtered by `ingested_at >= cutoff`.
    2. Match decision distribution: GROUP BY `decision`, COUNT. No time filter (reflects entire dataset).
    3. Canonical stats: COUNT total, COUNT WHERE needs_review=True, AVG match_confidence.

    Use `sa.func.count`, `sa.func.sum`, `sa.func.avg`, and `sa.case` following the research examples. Handle NULL results from empty tables gracefully (e.g., `total_events` might be None if no files, default to 0).

    **`GET /api/dashboard/processing-history`** -- query params: `days` (int, default 30), `granularity` (str, default "day", must be one of "day", "week"). Returns `list[ProcessingHistoryEntry]`.

    For daily granularity, query FileIngestion grouped by date. For SQLite compatibility, use `sa.func.date(FileIngestion.ingested_at)` instead of PostgreSQL's `DATE_TRUNC`. The query:
    ```python
    date_col = sa.func.date(FileIngestion.ingested_at).label("date")
    stmt = (
        sa.select(
            date_col,
            sa.func.count(FileIngestion.id).label("files_processed"),
            sa.func.coalesce(sa.func.sum(FileIngestion.event_count), 0).label("events_ingested"),
            sa.func.count(sa.case((FileIngestion.status == "error", 1))).label("errors"),
        )
        .where(FileIngestion.ingested_at >= cutoff)
        .group_by(date_col)
        .order_by(date_col)
    )
    ```

    **4. Update `src/event_dedup/api/app.py`:**

    Add imports and register the two new routers:
    ```python
    from event_dedup.api.routes.review import router as review_router
    from event_dedup.api.routes.dashboard import router as dashboard_router

    app.include_router(review_router)
    app.include_router(dashboard_router)
    ```

    **5. Create `tests/test_review_api.py`:**

    Follow the existing test pattern from `tests/test_api.py`. Use `api_client` and `seeded_db` fixtures. The seeded DB has:
    - Canonical event 1 (Fasching in Freiburg): 2 sources (pdf-aaa-0-0, pdf-bbb-0-0), match_confidence=0.85, needs_review=False
    - Canonical event 2 (Stadtfest Offenburg): 1 source, singleton

    **Extend the `seeded_db` fixture** in `tests/conftest.py` OR create a separate fixture in `test_review_api.py` that adds a 3rd canonical event with `needs_review=True` and low `match_confidence=0.45` (for testing the review queue). Also add a 3rd source event linked to it.

    Actually, better to create a `review_seeded_db` fixture in the test file itself that builds on the seeded_db pattern but adds the review-specific data. This keeps the existing fixture stable.

    **Tests to write (all `@pytest.mark.asyncio`):**

    1. `test_split_creates_new_canonical` -- POST split with target_canonical_id=None. Verify: 200 response, new_canonical_id in response, original canonical still exists with 1 source, new canonical has 1 source.
    2. `test_split_to_existing_canonical` -- POST split with target_canonical_id pointing to canonical 2. Verify: 200, canonical 2 now has 2 sources.
    3. `test_split_last_source_deletes_canonical` -- Create a singleton canonical, split its only source. Verify: original_deleted=True.
    4. `test_split_not_found` -- POST split with invalid canonical_event_id. Verify: 404.
    5. `test_merge_canonical_events` -- POST merge with source=1, target=2. Verify: 200, surviving_canonical_id, deleted_canonical_id, donor canonical no longer exists, target has combined sources.
    6. `test_merge_same_id_returns_400` -- POST merge with same source and target. Verify: 400.
    7. `test_review_queue_returns_low_confidence` -- GET /api/review/queue. Verify: items returned sorted by confidence ascending, needs_review items first.
    8. `test_review_queue_pagination` -- GET with page and size params.
    9. `test_dismiss_from_queue` -- POST dismiss on a needs_review event. Verify: needs_review set to False, audit log entry created.
    10. `test_audit_log_records_operations` -- After a split, GET /api/audit-log. Verify: entries exist with correct action_type.
    11. `test_audit_log_filter_by_action_type` -- GET /api/audit-log?action_type=split. Verify: only split entries returned.
    12. `test_dashboard_stats` -- GET /api/dashboard/stats. Verify: returns valid DashboardStats structure with counts.
    13. `test_dashboard_processing_history` -- GET /api/dashboard/processing-history. Verify: returns list with date, files_processed, events_ingested.

    Note: For the audit-log endpoint, use prefix `/api/audit-log` NOT `/api/review/audit-log`. Register this as a separate route on the review router with explicit path, or create a small additional router. Simplest: add the route to the review router with `@router.get("/audit-log", ...)` -- but that would make it `/api/review/audit-log`. Per the research design, it should be `/api/audit-log`. So either: (a) create it on the review router but use a full path override: not possible with APIRouter prefix. (b) Add it directly to the app in app.py. (c) Create the route in review.py but on a separate `audit_router = APIRouter(prefix="/api", tags=["audit"])` and register it in app.py.

    **Approach: Create `audit_router` in review.py, export it, register in app.py.** The review.py file exports both `router` (prefix=/api/review) and `audit_router` (prefix=/api). In app.py, register both.
  </action>
  <verify>
    Run `uv run pytest tests/test_review_api.py -x -v` -- all 13 tests should pass.

    Also run `uv run pytest tests/test_api.py -x -v` to verify no regression on existing API tests.
  </verify>
  <done>
    All review API endpoints respond correctly: POST /api/review/split returns SplitResponse, POST /api/review/merge returns MergeResponse, GET /api/review/queue returns paginated low-confidence events sorted by uncertainty, POST /api/review/queue/{id}/dismiss clears needs_review, GET /api/audit-log returns paginated audit entries, GET /api/dashboard/stats returns aggregate stats, GET /api/dashboard/processing-history returns daily time-series. All operations create audit log entries. Edge cases handled (empty canonical after split, duplicate links on merge, same-ID merge rejection). 13+ tests pass. Existing API tests still pass.
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/test_review_api.py tests/test_api.py -x -v` -- all review + existing API tests pass
2. `uv run python -c "from event_dedup.models import AuditLog; print(AuditLog.__tablename__)"` -- prints "audit_log"
3. `uv run python -c "from event_dedup.review.operations import split_source_from_canonical, merge_canonical_events; print('Operations OK')"` -- imports without error
4. `uv run python -c "from event_dedup.api.schemas import SplitRequest, MergeRequest, DashboardStats; print('Schemas OK')"` -- imports without error
</verification>

<success_criteria>
- AuditLog model exists and is registered in models/__init__.py
- Alembic migration 004 creates audit_log table and dashboard indexes (file_ingestions.ingested_at, match_decisions.decided_at)
- source_event_to_dict helper converts SourceEvent ORM to dict compatible with synthesize_canonical()
- Split operation atomically detaches a source, re-synthesizes affected canonicals, handles empty-canonical and assign-to-existing cases
- Merge operation atomically combines two canonicals, handles duplicate source links, re-synthesizes from all combined sources
- Review queue endpoint returns needs_review and low-confidence events sorted by uncertainty
- Dismiss endpoint clears needs_review and logs audit entry
- Dashboard stats endpoint returns file processing, match distribution, and canonical summary
- Dashboard processing-history endpoint returns daily time-series
- All operations log to audit_log table
- 13+ API integration tests pass
- Existing test_api.py tests are not broken
</success_criteria>

<output>
After completion, create `.planning/phases/06/06-01-SUMMARY.md`
</output>
