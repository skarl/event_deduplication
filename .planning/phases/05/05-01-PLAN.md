---
phase: 05-ai-matching
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - src/event_dedup/ai_matching/__init__.py
  - src/event_dedup/ai_matching/schemas.py
  - src/event_dedup/ai_matching/client.py
  - src/event_dedup/ai_matching/prompt.py
  - src/event_dedup/ai_matching/cache.py
  - src/event_dedup/ai_matching/cost_tracker.py
  - src/event_dedup/matching/config.py
  - src/event_dedup/models/ai_match_cache.py
  - src/event_dedup/models/ai_usage_log.py
  - src/event_dedup/models/__init__.py
  - config/alembic/versions/003_add_ai_matching_tables.py
  - tests/test_ai_matching.py
autonomous: true
requirements: [AI-01, AI-02, AI-03, AI-04, AI-05]

must_haves:
  truths:
    - "AI matching schemas define structured decision/confidence/reasoning output"
    - "Gemini client calls async API with structured output and returns parsed AIMatchResult"
    - "Content-hash cache computes deterministic hashes for event pairs and stores/retrieves results from DB"
    - "Cost tracker estimates USD cost from token counts and logs usage per request"
    - "AI matching configuration is loadable from matching.yaml with sensible defaults"
    - "Database tables for cache and usage logs exist and are created by Alembic migration"
  artifacts:
    - path: "src/event_dedup/ai_matching/schemas.py"
      provides: "Pydantic model for structured AI response"
      contains: "AIMatchResult"
    - path: "src/event_dedup/ai_matching/client.py"
      provides: "Async Gemini API wrapper"
      contains: "call_gemini"
    - path: "src/event_dedup/ai_matching/prompt.py"
      provides: "System prompt and event pair formatting"
      contains: "SYSTEM_PROMPT, format_event_pair"
    - path: "src/event_dedup/ai_matching/cache.py"
      provides: "Content-hash computation and DB cache operations"
      contains: "compute_pair_hash, lookup_cache, store_cache"
    - path: "src/event_dedup/ai_matching/cost_tracker.py"
      provides: "Token cost estimation and usage logging"
      contains: "estimate_cost, log_usage"
    - path: "src/event_dedup/matching/config.py"
      provides: "AIMatchingConfig section added to MatchingConfig"
      contains: "AIMatchingConfig"
    - path: "src/event_dedup/models/ai_match_cache.py"
      provides: "SQLAlchemy model for cached AI decisions"
      contains: "AIMatchCache"
    - path: "src/event_dedup/models/ai_usage_log.py"
      provides: "SQLAlchemy model for token usage tracking"
      contains: "AIUsageLog"
    - path: "config/alembic/versions/003_add_ai_matching_tables.py"
      provides: "Alembic migration creating ai_match_cache and ai_usage_log tables"
    - path: "tests/test_ai_matching.py"
      provides: "Unit tests for schemas, cache hashing, cost tracker, prompt formatting"
  key_links:
    - from: "src/event_dedup/ai_matching/client.py"
      to: "src/event_dedup/ai_matching/schemas.py"
      via: "response_schema=AIMatchResult"
      pattern: "AIMatchResult"
    - from: "src/event_dedup/ai_matching/cache.py"
      to: "src/event_dedup/models/ai_match_cache.py"
      via: "SQLAlchemy select/insert"
      pattern: "AIMatchCache"
    - from: "src/event_dedup/ai_matching/cost_tracker.py"
      to: "src/event_dedup/models/ai_usage_log.py"
      via: "SQLAlchemy insert"
      pattern: "AIUsageLog"
    - from: "src/event_dedup/matching/config.py"
      to: "src/event_dedup/ai_matching/client.py"
      via: "AIMatchingConfig passed to call_gemini"
      pattern: "ai_config\\.model"
---

<objective>
Build the AI matching infrastructure: Gemini client, structured output schemas, prompt engineering, content-hash cache, cost tracker, configuration, and database models with Alembic migration.

Purpose: This plan creates all the building blocks that Plan 05-02 will wire together into the pipeline. Each component is independently testable without a live Gemini API key -- the client wraps the SDK, and all other modules are pure logic or DB operations.

Output: Complete `ai_matching/` package with client, prompt, schemas, cache, cost tracker; new DB models and migration; AIMatchingConfig added to matching config; unit tests for all components.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05/05-RESEARCH.md

@src/event_dedup/matching/config.py
@src/event_dedup/matching/pipeline.py
@src/event_dedup/matching/combiner.py
@src/event_dedup/models/base.py
@src/event_dedup/models/__init__.py
@src/event_dedup/models/match_decision.py
@src/event_dedup/config/settings.py
@src/event_dedup/worker/persistence.py
@tests/conftest.py
@pyproject.toml

<interfaces>
<!-- Key types and contracts the executor needs. Extracted from codebase. -->

From src/event_dedup/matching/pipeline.py:
```python
@dataclass
class MatchDecisionRecord:
    event_id_a: str
    event_id_b: str
    signals: SignalScores
    combined_score_value: float
    decision: str          # "match", "no_match", "ambiguous"
    tier: str = "deterministic"

@dataclass
class MatchResult:
    decisions: list[MatchDecisionRecord]
    pair_stats: CandidatePairStats
    match_count: int
    ambiguous_count: int
    no_match_count: int
```

From src/event_dedup/matching/combiner.py:
```python
@dataclass(frozen=True)
class SignalScores:
    date: float
    geo: float
    title: float
    description: float
```

From src/event_dedup/matching/config.py:
```python
class MatchingConfig(BaseModel):
    scoring: ScoringWeights = ScoringWeights()
    thresholds: ThresholdConfig = ThresholdConfig()
    geo: GeoConfig = GeoConfig()
    date: DateConfig = DateConfig()
    title: TitleConfig = TitleConfig()
    cluster: ClusterConfig = ClusterConfig()
    canonical: CanonicalConfig = CanonicalConfig()
```

From src/event_dedup/models/base.py:
```python
class Base(DeclarativeBase):
    metadata = sa.MetaData(naming_convention={...})
```

From src/event_dedup/config/settings.py:
```python
class Settings(BaseSettings):
    model_config = SettingsConfigDict(env_prefix="EVENT_DEDUP_")
    database_url: str = "postgresql+asyncpg://..."
    matching_config_path: Path = Path("config/matching.yaml")
    # ... other fields
```

From src/event_dedup/models/__init__.py:
```python
# Registers all models with Base.metadata via imports
# New models MUST be imported here for Alembic to detect them
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Dependency, schemas, prompt, config, and DB models</name>
  <files>
    pyproject.toml
    src/event_dedup/ai_matching/__init__.py
    src/event_dedup/ai_matching/schemas.py
    src/event_dedup/ai_matching/prompt.py
    src/event_dedup/matching/config.py
    src/event_dedup/models/ai_match_cache.py
    src/event_dedup/models/ai_usage_log.py
    src/event_dedup/models/__init__.py
    config/alembic/versions/003_add_ai_matching_tables.py
  </files>
  <action>
    **1. Add google-genai dependency to pyproject.toml:**

    Run `uv add google-genai` to add to the `[project] dependencies` list. This is the official unified Google Gen AI SDK (NOT the deprecated `google-generativeai`).

    **2. Create `src/event_dedup/ai_matching/__init__.py`** (empty file).

    **3. Create `src/event_dedup/ai_matching/schemas.py`** with the Pydantic model for structured Gemini output:

    ```python
    """Pydantic schemas for AI matching structured output."""
    from __future__ import annotations

    from pydantic import BaseModel, Field


    class AIMatchResult(BaseModel):
        """Structured response from Gemini Flash for event pair comparison.

        Used as response_schema in the Gemini API call to constrain output.
        """
        decision: str = Field(
            description=(
                "Whether the two events are the same real-world event. "
                "Must be 'same' or 'different'."
            )
        )
        confidence: float = Field(
            description="Confidence in the decision, from 0.0 (no confidence) to 1.0 (certain).",
            ge=0.0,
            le=1.0,
        )
        reasoning: str = Field(
            description=(
                "Brief explanation of why the events are considered same or different, "
                "noting key matching or differentiating factors."
            )
        )
    ```

    **4. Create `src/event_dedup/ai_matching/prompt.py`** with the system prompt and event pair formatting function:

    ```python
    """Prompt template and event formatting for AI matching."""
    from __future__ import annotations

    from event_dedup.matching.combiner import SignalScores

    SYSTEM_PROMPT = """You are an expert event deduplication system analyzing German regional events.

    Your task: determine whether two event records describe the SAME real-world event
    (same gathering, same place, same time) or DIFFERENT events.

    Key considerations:
    - German compound words and regional dialects may describe the same thing differently
      (e.g., Fasnet/Fasching/Fastnacht/Karneval are all carnival)
    - Source types differ: "artikel" (newspaper articles) have journalistic headlines,
      "terminliste" (event listings) have formal event names
    - Same event may have slightly different dates if one source lists a multi-day range
    - Location names may vary (abbreviations, spelling differences, missing street details)
    - Description length/style varies by source -- focus on factual overlap, not style
    - Events at the same venue on the same day may still be DIFFERENT events (check titles carefully)

    Respond with ONLY a JSON object matching the required schema."""


    def format_event_pair(
        event_a: dict,
        event_b: dict,
        signals: SignalScores,
    ) -> str:
        """Format two events for AI comparison.

        Args:
            event_a: First event dict with all fields.
            event_b: Second event dict with all fields.
            signals: Pre-computed signal scores from deterministic matching.

        Returns:
            Formatted prompt string for the user message.
        """
        combined = signals.date + signals.geo + signals.title + signals.description

        return f"""Compare these two events:

    ## Event A (ID: {event_a['id']}, Source: {event_a.get('source_code', 'unknown')}, Type: {event_a.get('source_type', 'unknown')})
    Title: {event_a.get('title', 'N/A')}
    Description: {_truncate(event_a.get('description') or event_a.get('short_description') or 'N/A', 500)}
    Location: {event_a.get('location_name', '')}, {event_a.get('location_city', '')}
    Dates: {_format_dates(event_a.get('dates', []))}
    Categories: {', '.join(event_a.get('categories') or [])}

    ## Event B (ID: {event_b['id']}, Source: {event_b.get('source_code', 'unknown')}, Type: {event_b.get('source_type', 'unknown')})
    Title: {event_b.get('title', 'N/A')}
    Description: {_truncate(event_b.get('description') or event_b.get('short_description') or 'N/A', 500)}
    Location: {event_b.get('location_name', '')}, {event_b.get('location_city', '')}
    Dates: {_format_dates(event_b.get('dates', []))}
    Categories: {', '.join(event_b.get('categories') or [])}

    ## Deterministic Scoring Context
    Combined score: {combined / 4:.2f} (weighted average of signals below)
    - Date similarity: {signals.date:.2f}
    - Geo proximity: {signals.geo:.2f}
    - Title similarity: {signals.title:.2f}
    - Description similarity: {signals.description:.2f}

    These scores placed this pair in the "ambiguous" zone (between auto-match and auto-reject thresholds).

    Are these the SAME real-world event or DIFFERENT events?"""


    def _format_dates(dates: list[dict]) -> str:
        """Format a list of date dicts into a readable string."""
        if not dates:
            return "N/A"
        parts = []
        for d in dates[:5]:  # Limit to 5 dates to avoid prompt bloat
            s = d.get("date", "?")
            if d.get("start_time"):
                s += f" {d['start_time']}"
            if d.get("end_time"):
                s += f"-{d['end_time']}"
            if d.get("end_date"):
                s += f" to {d['end_date']}"
            parts.append(s)
        return "; ".join(parts)


    def _truncate(text: str, max_len: int) -> str:
        """Truncate text to max_len, adding ellipsis if needed."""
        if len(text) <= max_len:
            return text
        return text[:max_len - 3] + "..."
    ```

    **5. Add `AIMatchingConfig` to `src/event_dedup/matching/config.py`:**

    Add a new config class BEFORE `MatchingConfig`:

    ```python
    class AIMatchingConfig(BaseModel):
        """Configuration for AI-assisted matching tier."""

        enabled: bool = False
        api_key: str = ""
        model: str = "gemini-2.5-flash"
        temperature: float = 0.1
        max_output_tokens: int = 300
        max_concurrent_requests: int = 5
        confidence_threshold: float = 0.6
        cache_enabled: bool = True

        # Cost monitoring (Gemini 2.5 Flash pricing)
        cost_per_1m_input_tokens: float = 0.30
        cost_per_1m_output_tokens: float = 2.50
    ```

    Add `ai` field to `MatchingConfig`:

    ```python
    class MatchingConfig(BaseModel):
        scoring: ScoringWeights = ScoringWeights()
        thresholds: ThresholdConfig = ThresholdConfig()
        geo: GeoConfig = GeoConfig()
        date: DateConfig = DateConfig()
        title: TitleConfig = TitleConfig()
        cluster: ClusterConfig = ClusterConfig()
        canonical: CanonicalConfig = CanonicalConfig()
        ai: AIMatchingConfig = AIMatchingConfig()  # NEW
    ```

    **6. Create `src/event_dedup/models/ai_match_cache.py`:**

    ```python
    """SQLAlchemy model for caching AI match decisions."""
    from __future__ import annotations

    from datetime import datetime

    import sqlalchemy as sa
    from sqlalchemy.orm import Mapped, mapped_column

    from event_dedup.models.base import Base


    class AIMatchCache(Base):
        """Cached AI match result keyed by content hash of the event pair.

        The pair_hash is a SHA-256 of the matching-relevant fields of both events
        in canonical order (id_a < id_b). This ensures identical event pairs always
        produce the same cache key regardless of processing order.
        """

        __tablename__ = "ai_match_cache"

        id: Mapped[int] = mapped_column(sa.Integer, primary_key=True, autoincrement=True)
        pair_hash: Mapped[str] = mapped_column(sa.String(64), unique=True, index=True)
        event_id_a: Mapped[str] = mapped_column(sa.String)
        event_id_b: Mapped[str] = mapped_column(sa.String)
        decision: Mapped[str] = mapped_column(sa.String)  # "same" or "different"
        confidence: Mapped[float] = mapped_column(sa.Float)
        reasoning: Mapped[str] = mapped_column(sa.Text)
        model: Mapped[str] = mapped_column(sa.String)
        created_at: Mapped[datetime] = mapped_column(
            sa.DateTime, server_default=sa.text("CURRENT_TIMESTAMP")
        )

        __table_args__ = (
            sa.CheckConstraint(
                "decision IN ('same', 'different')", name="valid_ai_decision"
            ),
        )
    ```

    **7. Create `src/event_dedup/models/ai_usage_log.py`:**

    ```python
    """SQLAlchemy model for tracking AI API usage and costs."""
    from __future__ import annotations

    from datetime import datetime

    import sqlalchemy as sa
    from sqlalchemy.orm import Mapped, mapped_column

    from event_dedup.models.base import Base


    class AIUsageLog(Base):
        """Logs each AI API call with token counts and estimated cost.

        Entries with cached=True record cache hits (zero API cost, zero tokens).
        Entries with cached=False record actual API calls.
        batch_id groups all requests from a single pipeline run.
        """

        __tablename__ = "ai_usage_log"

        id: Mapped[int] = mapped_column(sa.Integer, primary_key=True, autoincrement=True)
        batch_id: Mapped[str] = mapped_column(sa.String, index=True)
        event_id_a: Mapped[str] = mapped_column(sa.String)
        event_id_b: Mapped[str] = mapped_column(sa.String)
        model: Mapped[str] = mapped_column(sa.String)
        prompt_tokens: Mapped[int] = mapped_column(sa.Integer, default=0)
        completion_tokens: Mapped[int] = mapped_column(sa.Integer, default=0)
        total_tokens: Mapped[int] = mapped_column(sa.Integer, default=0)
        estimated_cost_usd: Mapped[float] = mapped_column(sa.Float, default=0.0)
        cached: Mapped[bool] = mapped_column(sa.Boolean, default=False)
        created_at: Mapped[datetime] = mapped_column(
            sa.DateTime, server_default=sa.text("CURRENT_TIMESTAMP")
        )
    ```

    **8. Update `src/event_dedup/models/__init__.py`** to import the two new models:

    Add these imports (maintaining alphabetical order with existing imports):

    ```python
    from event_dedup.models.ai_match_cache import AIMatchCache
    from event_dedup.models.ai_usage_log import AIUsageLog
    ```

    And add `"AIMatchCache"` and `"AIUsageLog"` to the `__all__` list.

    This is critical -- Alembic discovers models through Base.metadata, which requires the models to be imported somewhere that runs before migration.

    **9. Create Alembic migration `config/alembic/versions/003_add_ai_matching_tables.py`:**

    ```python
    """Add AI matching cache and usage log tables.

    Revision ID: 003_ai_matching
    Revises: 002_pg_trgm_dates
    Create Date: 2026-02-28
    """
    from alembic import op
    import sqlalchemy as sa

    revision = "003_ai_matching"
    down_revision = "002_pg_trgm_dates"
    branch_labels = None
    depends_on = None


    def upgrade() -> None:
        op.create_table(
            "ai_match_cache",
            sa.Column("id", sa.Integer(), primary_key=True, autoincrement=True),
            sa.Column("pair_hash", sa.String(64), nullable=False),
            sa.Column("event_id_a", sa.String(), nullable=False),
            sa.Column("event_id_b", sa.String(), nullable=False),
            sa.Column("decision", sa.String(), nullable=False),
            sa.Column("confidence", sa.Float(), nullable=False),
            sa.Column("reasoning", sa.Text(), nullable=False),
            sa.Column("model", sa.String(), nullable=False),
            sa.Column("created_at", sa.DateTime(), server_default=sa.text("CURRENT_TIMESTAMP")),
            sa.CheckConstraint("decision IN ('same', 'different')", name="ck_ai_match_cache_valid_ai_decision"),
        )
        op.create_index("ix_ai_match_cache_pair_hash", "ai_match_cache", ["pair_hash"], unique=True)

        op.create_table(
            "ai_usage_log",
            sa.Column("id", sa.Integer(), primary_key=True, autoincrement=True),
            sa.Column("batch_id", sa.String(), nullable=False),
            sa.Column("event_id_a", sa.String(), nullable=False),
            sa.Column("event_id_b", sa.String(), nullable=False),
            sa.Column("model", sa.String(), nullable=False),
            sa.Column("prompt_tokens", sa.Integer(), nullable=False, server_default="0"),
            sa.Column("completion_tokens", sa.Integer(), nullable=False, server_default="0"),
            sa.Column("total_tokens", sa.Integer(), nullable=False, server_default="0"),
            sa.Column("estimated_cost_usd", sa.Float(), nullable=False, server_default="0.0"),
            sa.Column("cached", sa.Boolean(), nullable=False, server_default=sa.text("0")),
            sa.Column("created_at", sa.DateTime(), server_default=sa.text("CURRENT_TIMESTAMP")),
        )
        op.create_index("ix_ai_usage_log_batch_id", "ai_usage_log", ["batch_id"])


    def downgrade() -> None:
        op.drop_index("ix_ai_usage_log_batch_id")
        op.drop_table("ai_usage_log")
        op.drop_index("ix_ai_match_cache_pair_hash")
        op.drop_table("ai_match_cache")
    ```
  </action>
  <verify>
    <automated>cd /Users/svenkarl/workspaces/event-deduplication && uv run python -c "
from event_dedup.ai_matching.schemas import AIMatchResult
from event_dedup.ai_matching.prompt import SYSTEM_PROMPT, format_event_pair
from event_dedup.matching.config import AIMatchingConfig, MatchingConfig
from event_dedup.models.ai_match_cache import AIMatchCache
from event_dedup.models.ai_usage_log import AIUsageLog
from event_dedup.models import AIMatchCache as MC, AIUsageLog as UL
cfg = MatchingConfig()
assert cfg.ai.enabled == False
assert cfg.ai.model == 'gemini-2.5-flash'
r = AIMatchResult(decision='same', confidence=0.9, reasoning='test')
assert r.decision == 'same'
print('All imports and basic assertions OK')
"</automated>
  </verify>
  <done>
    - google-genai added to pyproject.toml dependencies
    - AIMatchResult Pydantic schema importable with decision/confidence/reasoning fields
    - System prompt mentions German regional events, source type differences, compound words
    - format_event_pair produces formatted prompt with both events and signal scores
    - AIMatchingConfig added to MatchingConfig with enabled=False default
    - AIMatchCache and AIUsageLog SQLAlchemy models importable
    - Both models registered in models/__init__.py
    - Alembic migration 003 creates both tables with correct down_revision chain
  </done>
</task>

<task type="auto">
  <name>Task 2: Cache, cost tracker, Gemini client, and unit tests</name>
  <files>
    src/event_dedup/ai_matching/cache.py
    src/event_dedup/ai_matching/cost_tracker.py
    src/event_dedup/ai_matching/client.py
    tests/test_ai_matching.py
  </files>
  <action>
    **1. Create `src/event_dedup/ai_matching/cache.py`** with content-hash computation and DB cache operations:

    ```python
    """Content-hash cache for AI match results.

    Computes deterministic hashes from event pair content and manages
    cache lookup/storage in the ai_match_cache table.
    """
    from __future__ import annotations

    import hashlib
    import json

    import structlog
    from sqlalchemy import select
    from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker

    from event_dedup.ai_matching.schemas import AIMatchResult
    from event_dedup.models.ai_match_cache import AIMatchCache

    logger = structlog.get_logger()


    def compute_pair_hash(event_a: dict, event_b: dict) -> str:
        """Compute a deterministic SHA-256 hash of an event pair's content.

        Uses canonical ordering (id_a < id_b) and only matching-relevant
        fields so that the same real-world event pair always produces the
        same hash regardless of processing order.

        Args:
            event_a: First event dict.
            event_b: Second event dict.

        Returns:
            64-character hex SHA-256 hash string.
        """
        # Canonical ordering
        if event_a["id"] > event_b["id"]:
            event_a, event_b = event_b, event_a

        def extract_fields(e: dict) -> dict:
            return {
                "title": e.get("title", ""),
                "description": e.get("description", ""),
                "short_description": e.get("short_description", ""),
                "location_city": e.get("location_city", ""),
                "location_name": e.get("location_name", ""),
                "dates": sorted(
                    str(d.get("date", "")) for d in (e.get("dates") or [])
                ),
                "categories": sorted(e.get("categories") or []),
            }

        content = json.dumps(
            [extract_fields(event_a), extract_fields(event_b)],
            sort_keys=True,
            ensure_ascii=False,
        )
        return hashlib.sha256(content.encode()).hexdigest()


    async def lookup_cache(
        session_factory: async_sessionmaker,
        pair_hash: str,
        current_model: str,
    ) -> AIMatchResult | None:
        """Look up a cached AI match result by pair hash.

        Returns None on cache miss or if the cached result was generated
        by a different model (stale cache on model upgrade).

        Args:
            session_factory: Async session factory.
            pair_hash: SHA-256 hash of the event pair content.
            current_model: The model ID currently configured (for staleness check).

        Returns:
            AIMatchResult if cache hit with matching model, else None.
        """
        async with session_factory() as session:
            stmt = select(AIMatchCache).where(AIMatchCache.pair_hash == pair_hash)
            result = await session.execute(stmt)
            cached = result.scalar_one_or_none()

        if cached is None:
            return None

        # Check model staleness
        if cached.model != current_model:
            logger.debug(
                "cache_model_stale",
                pair_hash=pair_hash[:12],
                cached_model=cached.model,
                current_model=current_model,
            )
            return None

        return AIMatchResult(
            decision=cached.decision,
            confidence=cached.confidence,
            reasoning=cached.reasoning,
        )


    async def store_cache(
        session_factory: async_sessionmaker,
        pair_hash: str,
        event_id_a: str,
        event_id_b: str,
        result: AIMatchResult,
        model: str,
    ) -> None:
        """Store an AI match result in the cache.

        Uses the pair_hash as the unique key. Silently ignores duplicate
        inserts (race condition between concurrent workers).

        Args:
            session_factory: Async session factory.
            pair_hash: SHA-256 hash of the event pair content.
            event_id_a: First event ID (canonical order).
            event_id_b: Second event ID (canonical order).
            result: The AI match result to cache.
            model: The model ID that produced this result.
        """
        async with session_factory() as session, session.begin():
            entry = AIMatchCache(
                pair_hash=pair_hash,
                event_id_a=event_id_a,
                event_id_b=event_id_b,
                decision=result.decision,
                confidence=result.confidence,
                reasoning=result.reasoning,
                model=model,
            )
            session.add(entry)
            try:
                await session.flush()
            except Exception:
                # Unique constraint violation = concurrent insert, ignore
                await session.rollback()
                logger.debug("cache_store_duplicate", pair_hash=pair_hash[:12])
    ```

    **2. Create `src/event_dedup/ai_matching/cost_tracker.py`:**

    ```python
    """Token usage tracking and cost estimation for AI matching.

    Logs each API call (or cache hit) to the ai_usage_log table and provides
    cost aggregation queries for per-batch and per-period reporting.
    """
    from __future__ import annotations

    from datetime import datetime

    import structlog
    from sqlalchemy import func, select
    from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker

    from event_dedup.matching.config import AIMatchingConfig
    from event_dedup.models.ai_usage_log import AIUsageLog

    logger = structlog.get_logger()


    def estimate_cost(
        prompt_tokens: int,
        completion_tokens: int,
        config: AIMatchingConfig,
    ) -> float:
        """Estimate cost in USD for a single API call.

        Args:
            prompt_tokens: Number of input tokens.
            completion_tokens: Number of output tokens.
            config: AI config with per-1M-token pricing.

        Returns:
            Estimated cost in USD.
        """
        input_cost = (prompt_tokens / 1_000_000) * config.cost_per_1m_input_tokens
        output_cost = (completion_tokens / 1_000_000) * config.cost_per_1m_output_tokens
        return input_cost + output_cost


    async def log_usage(
        session_factory: async_sessionmaker,
        batch_id: str,
        event_id_a: str,
        event_id_b: str,
        model: str,
        prompt_tokens: int,
        completion_tokens: int,
        estimated_cost_usd: float,
        cached: bool,
    ) -> None:
        """Log a single AI matching usage entry.

        Args:
            session_factory: Async session factory.
            batch_id: Identifier grouping requests from one pipeline run.
            event_id_a: First event ID.
            event_id_b: Second event ID.
            model: Model name used.
            prompt_tokens: Input token count (0 for cache hits).
            completion_tokens: Output token count (0 for cache hits).
            estimated_cost_usd: Estimated cost in USD (0.0 for cache hits).
            cached: Whether this was a cache hit.
        """
        async with session_factory() as session, session.begin():
            entry = AIUsageLog(
                batch_id=batch_id,
                event_id_a=event_id_a,
                event_id_b=event_id_b,
                model=model,
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=prompt_tokens + completion_tokens,
                estimated_cost_usd=estimated_cost_usd,
                cached=cached,
            )
            session.add(entry)


    async def get_batch_summary(
        session_factory: async_sessionmaker,
        batch_id: str,
    ) -> dict:
        """Get usage summary for a specific batch.

        Args:
            session_factory: Async session factory.
            batch_id: The batch identifier.

        Returns:
            Dict with total_requests, cached_requests, api_requests,
            total_tokens, estimated_cost_usd.
        """
        async with session_factory() as session:
            stmt = select(
                func.count(AIUsageLog.id).label("total_requests"),
                func.sum(
                    func.cast(AIUsageLog.cached, sa.Integer)
                ).label("cached_requests"),
                func.sum(AIUsageLog.total_tokens).label("total_tokens"),
                func.sum(AIUsageLog.prompt_tokens).label("prompt_tokens"),
                func.sum(AIUsageLog.completion_tokens).label("completion_tokens"),
                func.sum(AIUsageLog.estimated_cost_usd).label("estimated_cost_usd"),
            ).where(AIUsageLog.batch_id == batch_id)

            result = await session.execute(stmt)
            row = result.one()

        total = row.total_requests or 0
        cached = row.cached_requests or 0

        return {
            "batch_id": batch_id,
            "total_requests": total,
            "cached_requests": cached,
            "api_requests": total - cached,
            "total_tokens": row.total_tokens or 0,
            "prompt_tokens": row.prompt_tokens or 0,
            "completion_tokens": row.completion_tokens or 0,
            "estimated_cost_usd": round(row.estimated_cost_usd or 0.0, 6),
        }


    async def get_period_summary(
        session_factory: async_sessionmaker,
        since: datetime,
    ) -> dict:
        """Get usage summary for a time period.

        Args:
            session_factory: Async session factory.
            since: Start of the reporting period.

        Returns:
            Dict with batch_count, total_requests, total_tokens, estimated_cost_usd.
        """
        async with session_factory() as session:
            stmt = select(
                func.count(func.distinct(AIUsageLog.batch_id)).label("batch_count"),
                func.count(AIUsageLog.id).label("total_requests"),
                func.sum(
                    func.cast(AIUsageLog.cached, sa.Integer)
                ).label("cached_requests"),
                func.sum(AIUsageLog.total_tokens).label("total_tokens"),
                func.sum(AIUsageLog.estimated_cost_usd).label("estimated_cost_usd"),
            ).where(AIUsageLog.created_at >= since)

            result = await session.execute(stmt)
            row = result.one()

        total = row.total_requests or 0
        cached = row.cached_requests or 0

        return {
            "since": since.isoformat(),
            "batch_count": row.batch_count or 0,
            "total_requests": total,
            "cached_requests": cached,
            "api_requests": total - cached,
            "total_tokens": row.total_tokens or 0,
            "estimated_cost_usd": round(row.estimated_cost_usd or 0.0, 6),
        }
    ```

    IMPORTANT: The `get_batch_summary` function uses `sa.Integer` -- add `import sqlalchemy as sa` at the top of the file.

    **3. Create `src/event_dedup/ai_matching/client.py`:**

    ```python
    """Gemini API client wrapper for AI matching.

    Provides async API call with structured output, using the google-genai
    SDK. The SDK handles retries (up to 4 with exponential backoff) and
    rate limit (429) responses automatically.
    """
    from __future__ import annotations

    import structlog
    from google import genai
    from google.genai import types

    from event_dedup.ai_matching.prompt import SYSTEM_PROMPT, format_event_pair
    from event_dedup.ai_matching.schemas import AIMatchResult
    from event_dedup.matching.combiner import SignalScores
    from event_dedup.matching.config import AIMatchingConfig

    logger = structlog.get_logger()


    def create_client(api_key: str) -> genai.Client:
        """Create a Gemini API client.

        Args:
            api_key: Google AI Studio API key.

        Returns:
            Configured genai.Client instance.
        """
        return genai.Client(api_key=api_key)


    async def call_gemini(
        client: genai.Client,
        event_a: dict,
        event_b: dict,
        signals: SignalScores,
        ai_config: AIMatchingConfig,
    ) -> tuple[AIMatchResult, int, int]:
        """Call Gemini Flash for a single event pair comparison.

        Args:
            client: Gemini API client.
            event_a: First event dict.
            event_b: Second event dict.
            signals: Pre-computed signal scores.
            ai_config: AI matching configuration.

        Returns:
            Tuple of (AIMatchResult, prompt_tokens, completion_tokens).

        Raises:
            Exception: On API error (caller should handle gracefully).
        """
        prompt = format_event_pair(event_a, event_b, signals)

        response = await client.aio.models.generate_content(
            model=ai_config.model,
            contents=prompt,
            config=types.GenerateContentConfig(
                system_instruction=SYSTEM_PROMPT,
                response_mime_type="application/json",
                response_schema=AIMatchResult,
                temperature=ai_config.temperature,
                max_output_tokens=ai_config.max_output_tokens,
            ),
        )

        result = AIMatchResult.model_validate_json(response.text)

        # Extract token usage from response metadata
        usage = response.usage_metadata
        prompt_tokens = getattr(usage, "prompt_token_count", 0) or 0
        completion_tokens = getattr(usage, "candidates_token_count", 0) or 0

        logger.debug(
            "gemini_call_complete",
            decision=result.decision,
            confidence=result.confidence,
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
        )

        return result, prompt_tokens, completion_tokens
    ```

    **4. Create `tests/test_ai_matching.py`** with unit tests that do NOT require a live Gemini API key:

    ```python
    """Tests for AI matching infrastructure.

    All tests are self-contained -- no Gemini API key required.
    Tests cover schemas, cache hashing, cost estimation, prompt formatting,
    and DB cache operations.
    """
    import hashlib
    import json

    import pytest

    from event_dedup.ai_matching.cache import compute_pair_hash, lookup_cache, store_cache
    from event_dedup.ai_matching.cost_tracker import estimate_cost, get_batch_summary, log_usage
    from event_dedup.ai_matching.prompt import SYSTEM_PROMPT, format_event_pair
    from event_dedup.ai_matching.schemas import AIMatchResult
    from event_dedup.matching.combiner import SignalScores
    from event_dedup.matching.config import AIMatchingConfig, MatchingConfig


    # ---- Schema tests ----

    class TestAIMatchResult:
        def test_valid_same(self):
            r = AIMatchResult(decision="same", confidence=0.95, reasoning="Same event")
            assert r.decision == "same"
            assert r.confidence == 0.95

        def test_valid_different(self):
            r = AIMatchResult(decision="different", confidence=0.8, reasoning="Different events")
            assert r.decision == "different"

        def test_confidence_bounds(self):
            with pytest.raises(Exception):
                AIMatchResult(decision="same", confidence=1.5, reasoning="test")
            with pytest.raises(Exception):
                AIMatchResult(decision="same", confidence=-0.1, reasoning="test")

        def test_json_roundtrip(self):
            r = AIMatchResult(decision="same", confidence=0.9, reasoning="Title match")
            j = r.model_dump_json()
            r2 = AIMatchResult.model_validate_json(j)
            assert r2.decision == r.decision
            assert r2.confidence == r.confidence


    # ---- Config tests ----

    class TestAIMatchingConfig:
        def test_defaults(self):
            cfg = AIMatchingConfig()
            assert cfg.enabled is False
            assert cfg.model == "gemini-2.5-flash"
            assert cfg.confidence_threshold == 0.6
            assert cfg.max_concurrent_requests == 5

        def test_in_matching_config(self):
            cfg = MatchingConfig()
            assert cfg.ai.enabled is False

        def test_from_yaml_dict(self):
            cfg = MatchingConfig(ai={"enabled": True, "api_key": "test-key"})
            assert cfg.ai.enabled is True
            assert cfg.ai.api_key == "test-key"
            assert cfg.ai.model == "gemini-2.5-flash"  # default preserved


    # ---- Cache hash tests ----

    class TestComputePairHash:
        def test_deterministic(self):
            a = {"id": "aaa", "title": "Event A", "description": "Desc A", "dates": [{"date": "2026-01-01"}], "categories": ["cat1"]}
            b = {"id": "bbb", "title": "Event B", "description": "Desc B", "dates": [{"date": "2026-01-02"}], "categories": ["cat2"]}
            h1 = compute_pair_hash(a, b)
            h2 = compute_pair_hash(a, b)
            assert h1 == h2
            assert len(h1) == 64  # SHA-256 hex

        def test_canonical_ordering(self):
            """Hash is the same regardless of argument order."""
            a = {"id": "aaa", "title": "Event A"}
            b = {"id": "bbb", "title": "Event B"}
            assert compute_pair_hash(a, b) == compute_pair_hash(b, a)

        def test_different_content_different_hash(self):
            a = {"id": "aaa", "title": "Event A"}
            b = {"id": "bbb", "title": "Event B"}
            c = {"id": "ccc", "title": "Event C"}
            assert compute_pair_hash(a, b) != compute_pair_hash(a, c)

        def test_ignores_non_content_fields(self):
            """Fields like batch_index or created_at don't affect the hash."""
            base = {"id": "aaa", "title": "Event A"}
            with_extra = {"id": "aaa", "title": "Event A", "batch_index": 5, "created_at": "2026-01-01"}
            b = {"id": "bbb", "title": "Event B"}
            assert compute_pair_hash(base, b) == compute_pair_hash(with_extra, b)


    # ---- Cost estimation tests ----

    class TestEstimateCost:
        def test_basic_cost(self):
            cfg = AIMatchingConfig()
            cost = estimate_cost(800, 100, cfg)
            # (800/1M * 0.30) + (100/1M * 2.50) = 0.00024 + 0.00025 = 0.00049
            assert abs(cost - 0.00049) < 0.00001

        def test_zero_tokens(self):
            cfg = AIMatchingConfig()
            assert estimate_cost(0, 0, cfg) == 0.0


    # ---- Prompt formatting tests ----

    class TestFormatEventPair:
        def test_basic_formatting(self):
            a = {
                "id": "aaa",
                "title": "Fasching in Freiburg",
                "description": "Annual carnival event",
                "source_code": "bwb",
                "source_type": "artikel",
                "location_name": "Marktplatz",
                "location_city": "Freiburg",
                "dates": [{"date": "2026-02-15", "start_time": "14:00"}],
                "categories": ["fasching"],
            }
            b = {
                "id": "bbb",
                "title": "Karneval Freiburg",
                "description": "Carnival celebration",
                "source_code": "rek",
                "source_type": "terminliste",
                "location_name": "Marktplatz",
                "location_city": "Freiburg",
                "dates": [{"date": "2026-02-15", "start_time": "14:30"}],
                "categories": ["karneval"],
            }
            signals = SignalScores(date=0.95, geo=1.0, title=0.65, description=0.50)
            prompt = format_event_pair(a, b, signals)

            assert "Fasching in Freiburg" in prompt
            assert "Karneval Freiburg" in prompt
            assert "Event A" in prompt
            assert "Event B" in prompt
            assert "Date similarity: 0.95" in prompt
            assert "Title similarity: 0.65" in prompt
            assert "ambiguous" in prompt.lower()

        def test_system_prompt_mentions_german(self):
            assert "German" in SYSTEM_PROMPT
            assert "artikel" in SYSTEM_PROMPT
            assert "terminliste" in SYSTEM_PROMPT


    # ---- DB cache operations tests ----

    class TestCacheDB:
        @pytest.fixture
        def sample_result(self):
            return AIMatchResult(decision="same", confidence=0.9, reasoning="Events match")

        async def test_store_and_lookup(self, test_session_factory, sample_result):
            pair_hash = "a" * 64
            model = "gemini-2.5-flash"

            await store_cache(
                test_session_factory, pair_hash,
                "evt-a", "evt-b", sample_result, model,
            )

            cached = await lookup_cache(test_session_factory, pair_hash, model)
            assert cached is not None
            assert cached.decision == "same"
            assert cached.confidence == 0.9

        async def test_lookup_miss(self, test_session_factory):
            cached = await lookup_cache(test_session_factory, "nonexistent" * 4, "gemini-2.5-flash")
            assert cached is None

        async def test_model_staleness(self, test_session_factory, sample_result):
            pair_hash = "b" * 64
            await store_cache(
                test_session_factory, pair_hash,
                "evt-a", "evt-b", sample_result, "gemini-2.0-flash",
            )
            # Lookup with different model returns None (stale)
            cached = await lookup_cache(test_session_factory, pair_hash, "gemini-2.5-flash")
            assert cached is None


    # ---- Usage logging tests ----

    class TestUsageLogging:
        async def test_log_and_summarize(self, test_session_factory):
            batch_id = "batch-001"

            await log_usage(
                test_session_factory, batch_id,
                "evt-a", "evt-b", "gemini-2.5-flash",
                prompt_tokens=800, completion_tokens=100,
                estimated_cost_usd=0.00049, cached=False,
            )
            await log_usage(
                test_session_factory, batch_id,
                "evt-c", "evt-d", "gemini-2.5-flash",
                prompt_tokens=0, completion_tokens=0,
                estimated_cost_usd=0.0, cached=True,
            )

            summary = await get_batch_summary(test_session_factory, batch_id)
            assert summary["total_requests"] == 2
            assert summary["cached_requests"] == 1
            assert summary["api_requests"] == 1
            assert summary["prompt_tokens"] == 800
            assert summary["estimated_cost_usd"] > 0
    ```
  </action>
  <verify>
    <automated>cd /Users/svenkarl/workspaces/event-deduplication && uv run pytest tests/test_ai_matching.py -x -v 2>&1 | tail -40</automated>
  </verify>
  <done>
    - compute_pair_hash produces deterministic, canonical-ordered, content-only hashes
    - lookup_cache returns cached AIMatchResult on hit, None on miss or model staleness
    - store_cache persists to ai_match_cache table
    - estimate_cost correctly computes USD from token counts and pricing config
    - log_usage writes to ai_usage_log table; get_batch_summary aggregates correctly
    - call_gemini wraps the async Gemini API with structured output and returns (AIMatchResult, prompt_tokens, completion_tokens)
    - All unit tests pass (no Gemini API key required)
    - No regressions in existing test suite
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/test_ai_matching.py -x -v` -- all AI matching unit tests pass
2. `uv run python -c "from event_dedup.ai_matching.client import call_gemini, create_client; print('client OK')"` -- client module importable
3. `uv run python -c "from event_dedup.matching.config import MatchingConfig; c = MatchingConfig(); print(c.ai.model)"` -- prints "gemini-2.5-flash"
4. `uv run pytest tests/ -x --timeout=60` -- all existing tests still pass (no regressions)
</verification>

<success_criteria>
- google-genai is installed as a dependency
- Complete ai_matching/ package with schemas, prompt, client, cache, cost_tracker modules
- AIMatchingConfig integrated into MatchingConfig with all defaults
- AIMatchCache and AIUsageLog DB models with Alembic migration (003)
- Content-hash cache computes deterministic hashes and stores/retrieves from DB
- Cost tracker estimates USD and logs per-request usage
- All AI matching unit tests pass without a Gemini API key
- No regressions in existing test suite
</success_criteria>

<output>
After completion, create `.planning/phases/05/05-01-SUMMARY.md`
</output>
