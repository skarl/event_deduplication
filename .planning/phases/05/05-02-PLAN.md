---
phase: 05-ai-matching
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - src/event_dedup/ai_matching/resolver.py
  - src/event_dedup/worker/orchestrator.py
  - src/event_dedup/config/settings.py
  - src/event_dedup/worker/__main__.py
  - src/event_dedup/evaluation/harness.py
  - docker-compose.yml
  - tests/test_ai_resolver.py
  - tests/test_orchestrator.py
autonomous: true
requirements: [AI-01, AI-02, AI-03, AI-04, AI-05]

must_haves:
  truths:
    - "Ambiguous match decisions from deterministic scoring are filtered and sent to Gemini for resolution"
    - "AI decisions with confidence >= 0.6 override ambiguous to match/no_match with tier='ai'"
    - "AI decisions with confidence < 0.6 remain as ambiguous for human review"
    - "Cache hits skip the API call; cache misses call Gemini and store the result"
    - "Each resolve_ambiguous_pairs call logs token usage and cost per request and produces a batch summary"
    - "Pipeline continues working with AI disabled (no regression on existing behavior)"
    - "Concurrent API calls are limited by asyncio.Semaphore"
    - "API failures for individual pairs are handled gracefully (pair stays ambiguous, no pipeline crash)"
    - "Evaluation harness can measure F1 with and without AI matching for comparison"
  artifacts:
    - path: "src/event_dedup/ai_matching/resolver.py"
      provides: "Orchestrator: filter ambiguous -> check cache -> call AI -> update decisions"
      contains: "resolve_ambiguous_pairs"
    - path: "src/event_dedup/worker/orchestrator.py"
      provides: "Updated pipeline with AI resolution step between scoring and clustering"
      contains: "resolve_ambiguous_pairs"
    - path: "src/event_dedup/evaluation/harness.py"
      provides: "Updated evaluation with AI-assisted matching comparison"
      contains: "generate_predictions_with_ai"
    - path: "tests/test_ai_resolver.py"
      provides: "Tests for resolver with mocked Gemini client"
    - path: "tests/test_orchestrator.py"
      provides: "Updated orchestrator tests verifying AI step integration"
  key_links:
    - from: "src/event_dedup/ai_matching/resolver.py"
      to: "src/event_dedup/ai_matching/client.py"
      via: "call_gemini for API calls"
      pattern: "call_gemini"
    - from: "src/event_dedup/ai_matching/resolver.py"
      to: "src/event_dedup/ai_matching/cache.py"
      via: "lookup_cache / store_cache for caching"
      pattern: "lookup_cache|store_cache"
    - from: "src/event_dedup/ai_matching/resolver.py"
      to: "src/event_dedup/ai_matching/cost_tracker.py"
      via: "log_usage for each resolution"
      pattern: "log_usage"
    - from: "src/event_dedup/worker/orchestrator.py"
      to: "src/event_dedup/ai_matching/resolver.py"
      via: "resolve_ambiguous_pairs call between scoring and clustering"
      pattern: "resolve_ambiguous_pairs"
    - from: "src/event_dedup/worker/orchestrator.py"
      to: "src/event_dedup/matching/config.py"
      via: "matching_config.ai.enabled check"
      pattern: "matching_config\\.ai\\.enabled"
---

<objective>
Wire the AI matching infrastructure (from Plan 05-01) into the pipeline: build the resolver orchestrator, integrate into the worker, update the evaluation harness to measure F1 improvement, and add Docker/env configuration.

Purpose: This plan makes AI matching operational. After this plan, dropping a JSON file into the watch directory triggers deterministic scoring, AI resolution of ambiguous pairs, and clustering -- the full tiered matching pipeline.

Output: Working AI-assisted matching pipeline; evaluation harness that can compare deterministic-only vs AI-assisted F1 scores; Docker environment with Gemini API key configuration.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05/05-RESEARCH.md
@.planning/phases/05/05-01-SUMMARY.md

@src/event_dedup/ai_matching/__init__.py
@src/event_dedup/ai_matching/schemas.py
@src/event_dedup/ai_matching/client.py
@src/event_dedup/ai_matching/prompt.py
@src/event_dedup/ai_matching/cache.py
@src/event_dedup/ai_matching/cost_tracker.py
@src/event_dedup/matching/config.py
@src/event_dedup/matching/pipeline.py
@src/event_dedup/matching/combiner.py
@src/event_dedup/worker/orchestrator.py
@src/event_dedup/worker/__main__.py
@src/event_dedup/worker/persistence.py
@src/event_dedup/config/settings.py
@src/event_dedup/evaluation/harness.py
@docker-compose.yml
@tests/conftest.py
@tests/test_orchestrator.py

<interfaces>
<!-- Key types and contracts from Plan 05-01. -->

From src/event_dedup/ai_matching/schemas.py:
```python
class AIMatchResult(BaseModel):
    decision: str   # "same" or "different"
    confidence: float  # 0.0 to 1.0
    reasoning: str
```

From src/event_dedup/ai_matching/client.py:
```python
def create_client(api_key: str) -> genai.Client
async def call_gemini(
    client: genai.Client,
    event_a: dict, event_b: dict,
    signals: SignalScores, ai_config: AIMatchingConfig,
) -> tuple[AIMatchResult, int, int]  # (result, prompt_tokens, completion_tokens)
```

From src/event_dedup/ai_matching/cache.py:
```python
def compute_pair_hash(event_a: dict, event_b: dict) -> str  # 64-char hex
async def lookup_cache(session_factory, pair_hash, current_model) -> AIMatchResult | None
async def store_cache(session_factory, pair_hash, event_id_a, event_id_b, result, model) -> None
```

From src/event_dedup/ai_matching/cost_tracker.py:
```python
def estimate_cost(prompt_tokens, completion_tokens, config) -> float
async def log_usage(session_factory, batch_id, event_id_a, event_id_b, model, prompt_tokens, completion_tokens, estimated_cost_usd, cached) -> None
async def get_batch_summary(session_factory, batch_id) -> dict
```

From src/event_dedup/matching/config.py:
```python
class AIMatchingConfig(BaseModel):
    enabled: bool = False
    api_key: str = ""
    model: str = "gemini-2.5-flash"
    temperature: float = 0.1
    max_output_tokens: int = 300
    max_concurrent_requests: int = 5
    confidence_threshold: float = 0.6
    cache_enabled: bool = True
    cost_per_1m_input_tokens: float = 0.30
    cost_per_1m_output_tokens: float = 2.50

class MatchingConfig(BaseModel):
    scoring: ScoringWeights
    thresholds: ThresholdConfig
    geo: GeoConfig
    date: DateConfig
    title: TitleConfig
    cluster: ClusterConfig
    canonical: CanonicalConfig
    ai: AIMatchingConfig = AIMatchingConfig()
```

From src/event_dedup/matching/pipeline.py:
```python
@dataclass
class MatchDecisionRecord:
    event_id_a: str
    event_id_b: str
    signals: SignalScores
    combined_score_value: float
    decision: str          # "match", "no_match", "ambiguous"
    tier: str = "deterministic"

@dataclass
class MatchResult:
    decisions: list[MatchDecisionRecord]
    pair_stats: CandidatePairStats
    match_count: int
    ambiguous_count: int
    no_match_count: int

def score_candidate_pairs(events, config) -> MatchResult
def run_full_pipeline(events, config) -> PipelineResult
def get_match_pairs(result: MatchResult) -> set[tuple[str, str]]
```

From src/event_dedup/worker/orchestrator.py:
```python
async def process_new_file(file_path, file_processor, session_factory, matching_config) -> dict
async def process_file_batch(file_paths, file_processor, session_factory, matching_config) -> list[dict]
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: AI resolver and pipeline integration</name>
  <files>
    src/event_dedup/ai_matching/resolver.py
    src/event_dedup/worker/orchestrator.py
    src/event_dedup/config/settings.py
    src/event_dedup/worker/__main__.py
    docker-compose.yml
  </files>
  <action>
    **1. Create `src/event_dedup/ai_matching/resolver.py`** -- the core orchestrator that filters ambiguous pairs, checks cache, calls Gemini, and returns updated decisions:

    ```python
    """AI matching resolver: orchestrates ambiguous pair resolution.

    Filters ambiguous decisions from deterministic scoring, checks the
    content-hash cache, calls Gemini Flash for uncached pairs, applies
    confidence threshold, and returns an updated MatchResult.
    """
    from __future__ import annotations

    import asyncio
    import uuid

    import structlog
    from sqlalchemy.ext.asyncio import async_sessionmaker

    from event_dedup.ai_matching.cache import compute_pair_hash, lookup_cache, store_cache
    from event_dedup.ai_matching.client import call_gemini, create_client
    from event_dedup.ai_matching.cost_tracker import estimate_cost, get_batch_summary, log_usage
    from event_dedup.ai_matching.schemas import AIMatchResult
    from event_dedup.matching.config import AIMatchingConfig
    from event_dedup.matching.pipeline import MatchDecisionRecord, MatchResult

    logger = structlog.get_logger()


    async def resolve_ambiguous_pairs(
        match_result: MatchResult,
        events: list[dict],
        ai_config: AIMatchingConfig,
        session_factory: async_sessionmaker,
    ) -> MatchResult:
        """Resolve ambiguous match decisions via Gemini Flash AI.

        Filters for decisions where decision=="ambiguous", resolves each
        via cache lookup or API call, and returns an updated MatchResult
        with resolved decisions.

        This function is called from the worker orchestrator AFTER
        deterministic scoring and BEFORE clustering.

        Args:
            match_result: The deterministic scoring result.
            events: All event dicts (needed for prompt formatting).
            ai_config: AI matching configuration.
            session_factory: Async session factory for cache/usage DB access.

        Returns:
            Updated MatchResult with ambiguous pairs resolved (or still
            ambiguous if AI confidence is below threshold or API failed).
        """
        events_by_id = {e["id"]: e for e in events}
        ambiguous = [d for d in match_result.decisions if d.decision == "ambiguous"]

        if not ambiguous:
            logger.info("ai_resolver_skip", reason="no_ambiguous_pairs")
            return match_result

        batch_id = str(uuid.uuid4())[:8]
        log = logger.bind(batch_id=batch_id, ambiguous_count=len(ambiguous))
        log.info("ai_resolver_start")

        client = create_client(ai_config.api_key)
        semaphore = asyncio.Semaphore(ai_config.max_concurrent_requests)

        async def resolve_one(decision: MatchDecisionRecord) -> MatchDecisionRecord:
            """Resolve a single ambiguous decision."""
            async with semaphore:
                event_a = events_by_id[decision.event_id_a]
                event_b = events_by_id[decision.event_id_b]

                # Compute cache key
                pair_hash = compute_pair_hash(event_a, event_b)

                # Check cache
                if ai_config.cache_enabled:
                    cached = await lookup_cache(
                        session_factory, pair_hash, ai_config.model
                    )
                    if cached is not None:
                        # Log cache hit
                        await log_usage(
                            session_factory, batch_id,
                            decision.event_id_a, decision.event_id_b,
                            ai_config.model,
                            prompt_tokens=0, completion_tokens=0,
                            estimated_cost_usd=0.0, cached=True,
                        )
                        return _apply_ai_result(
                            decision, cached, ai_config.confidence_threshold
                        )

                # Call Gemini API
                try:
                    result, prompt_tokens, completion_tokens = await call_gemini(
                        client, event_a, event_b, decision.signals, ai_config,
                    )
                except Exception as e:
                    logger.warning(
                        "gemini_call_failed",
                        pair=f"{decision.event_id_a}:{decision.event_id_b}",
                        error=str(e),
                    )
                    # Log failed attempt (0 tokens, 0 cost)
                    await log_usage(
                        session_factory, batch_id,
                        decision.event_id_a, decision.event_id_b,
                        ai_config.model,
                        prompt_tokens=0, completion_tokens=0,
                        estimated_cost_usd=0.0, cached=False,
                    )
                    return decision  # Keep as ambiguous on failure

                # Calculate cost
                cost = estimate_cost(prompt_tokens, completion_tokens, ai_config)

                # Log API call
                await log_usage(
                    session_factory, batch_id,
                    decision.event_id_a, decision.event_id_b,
                    ai_config.model,
                    prompt_tokens=prompt_tokens,
                    completion_tokens=completion_tokens,
                    estimated_cost_usd=cost,
                    cached=False,
                )

                # Store in cache
                if ai_config.cache_enabled:
                    # Canonical ordering for cache storage
                    id_a, id_b = decision.event_id_a, decision.event_id_b
                    await store_cache(
                        session_factory, pair_hash, id_a, id_b,
                        result, ai_config.model,
                    )

                return _apply_ai_result(
                    decision, result, ai_config.confidence_threshold
                )

        # Resolve all ambiguous pairs concurrently (with semaphore limiting)
        resolved = await asyncio.gather(
            *[resolve_one(d) for d in ambiguous],
            return_exceptions=True,
        )

        # Build updated decisions list
        ambiguous_ids = {
            (d.event_id_a, d.event_id_b) for d in ambiguous
        }
        updated_decisions = [
            d for d in match_result.decisions
            if (d.event_id_a, d.event_id_b) not in ambiguous_ids
        ]

        for original, result in zip(ambiguous, resolved):
            if isinstance(result, Exception):
                logger.error(
                    "resolve_one_exception",
                    pair=f"{original.event_id_a}:{original.event_id_b}",
                    error=str(result),
                )
                updated_decisions.append(original)  # Keep as ambiguous
            else:
                updated_decisions.append(result)

        # Recount
        new_match = sum(1 for d in updated_decisions if d.decision == "match")
        new_ambiguous = sum(1 for d in updated_decisions if d.decision == "ambiguous")
        new_no_match = sum(1 for d in updated_decisions if d.decision == "no_match")

        # Log batch summary
        try:
            summary = await get_batch_summary(session_factory, batch_id)
            log.info(
                "ai_resolver_complete",
                resolved=len(ambiguous) - new_ambiguous + match_result.ambiguous_count - len(ambiguous),
                remaining_ambiguous=new_ambiguous,
                api_calls=summary["api_requests"],
                cache_hits=summary["cached_requests"],
                total_tokens=summary["total_tokens"],
                estimated_cost_usd=summary["estimated_cost_usd"],
            )
        except Exception:
            log.info("ai_resolver_complete", resolved=len(ambiguous) - new_ambiguous)

        return MatchResult(
            decisions=updated_decisions,
            pair_stats=match_result.pair_stats,
            match_count=new_match,
            ambiguous_count=new_ambiguous,
            no_match_count=new_no_match,
        )


    def _apply_ai_result(
        decision: MatchDecisionRecord,
        ai_result: AIMatchResult,
        confidence_threshold: float,
    ) -> MatchDecisionRecord:
        """Apply an AI result to a match decision.

        Maps AI decision "same" -> "match", "different" -> "no_match".
        Only overrides if AI confidence >= threshold; otherwise keeps
        as "ambiguous" for human review.

        Args:
            decision: Original ambiguous decision.
            ai_result: AI judgment (same/different + confidence).
            confidence_threshold: Minimum confidence to override.

        Returns:
            Updated MatchDecisionRecord (new instance).
        """
        if ai_result.confidence < confidence_threshold:
            # Low confidence -- keep as ambiguous for review queue
            return MatchDecisionRecord(
                event_id_a=decision.event_id_a,
                event_id_b=decision.event_id_b,
                signals=decision.signals,
                combined_score_value=decision.combined_score_value,
                decision="ambiguous",
                tier="ai_low_confidence",
            )

        # Map AI decision to pipeline decision
        if ai_result.decision == "same":
            new_decision = "match"
        elif ai_result.decision == "different":
            new_decision = "no_match"
        else:
            # Unexpected value -- keep as ambiguous
            return MatchDecisionRecord(
                event_id_a=decision.event_id_a,
                event_id_b=decision.event_id_b,
                signals=decision.signals,
                combined_score_value=decision.combined_score_value,
                decision="ambiguous",
                tier="ai_unexpected",
            )

        return MatchDecisionRecord(
            event_id_a=decision.event_id_a,
            event_id_b=decision.event_id_b,
            signals=decision.signals,
            combined_score_value=decision.combined_score_value,
            decision=new_decision,
            tier="ai",
        )
    ```

    **2. Update `src/event_dedup/worker/orchestrator.py`** to add the AI resolution step.

    Add import at top:
    ```python
    from event_dedup.ai_matching.resolver import resolve_ambiguous_pairs
    ```

    In `process_new_file()`, after `pipeline_result = run_full_pipeline(events, matching_config)` and its logging, add the AI resolution step. The key change is that `run_full_pipeline` does scoring + clustering + synthesis in one call. To insert AI resolution between scoring and clustering, we need to change the approach:

    Replace the call to `run_full_pipeline` with the individual steps. Modify `process_new_file`:

    ```python
    async def process_new_file(
        file_path: Path,
        file_processor: FileProcessor,
        session_factory: async_sessionmaker,
        matching_config: MatchingConfig,
    ) -> dict:
        # ... Steps 1-2 unchanged (ingest + load events) ...

        try:
            async with session_factory() as session:
                events = await load_all_events_as_dicts(session)
            log.info("events_loaded", total_events=len(events))

            # Step 3: Run matching pipeline (pure function)
            pipeline_result = run_full_pipeline(events, matching_config)

            # Step 3.5: AI-assisted resolution of ambiguous pairs
            if matching_config.ai.enabled and matching_config.ai.api_key:
                original_ambiguous = pipeline_result.match_result.ambiguous_count
                pipeline_result.match_result = await resolve_ambiguous_pairs(
                    pipeline_result.match_result,
                    events,
                    matching_config.ai,
                    session_factory,
                )
                # If ambiguous count changed, re-run clustering and synthesis
                if pipeline_result.match_result.ambiguous_count != original_ambiguous:
                    from event_dedup.canonical.synthesizer import synthesize_canonical
                    from event_dedup.clustering.graph_cluster import cluster_matches
                    from event_dedup.matching.pipeline import (
                        PipelineResult,
                        _avg_cluster_confidence,
                        get_match_pairs,
                    )

                    all_event_ids = [e["id"] for e in events]
                    events_by_id = {e["id"]: e for e in events}
                    cluster_result = cluster_matches(
                        pipeline_result.match_result.decisions,
                        all_event_ids,
                        matching_config.cluster,
                        events_by_id,
                    )

                    canonical_events = []
                    for cluster in cluster_result.clusters:
                        sources = [events_by_id[eid] for eid in cluster]
                        canonical = synthesize_canonical(sources, matching_config.canonical)
                        canonical["needs_review"] = False
                        canonical["match_confidence"] = _avg_cluster_confidence(
                            cluster, pipeline_result.match_result.decisions
                        )
                        canonical_events.append(canonical)

                    for cluster in cluster_result.flagged_clusters:
                        sources = [events_by_id[eid] for eid in cluster]
                        canonical = synthesize_canonical(sources, matching_config.canonical)
                        canonical["needs_review"] = True
                        canonical["match_confidence"] = _avg_cluster_confidence(
                            cluster, pipeline_result.match_result.decisions
                        )
                        canonical_events.append(canonical)

                    pipeline_result = PipelineResult(
                        match_result=pipeline_result.match_result,
                        cluster_result=cluster_result,
                        canonical_events=canonical_events,
                        canonical_count=len(canonical_events),
                        flagged_count=len(cluster_result.flagged_clusters),
                    )

            log.info(
                "matching_complete",
                matches=pipeline_result.match_result.match_count,
                ambiguous=pipeline_result.match_result.ambiguous_count,
                canonical_count=pipeline_result.canonical_count,
                flagged_count=pipeline_result.flagged_count,
                reduction_pct=pipeline_result.match_result.pair_stats.reduction_pct,
            )

            # Step 4: Persist canonical events (unchanged)
            async with session_factory() as session, session.begin():
                count = await replace_canonical_events(session, pipeline_result)
            log.info("pipeline_complete", canonical_events_written=count)

            return {
                "status": "completed",
                "file": file_path.name,
                "events_ingested": result.event_count,
                "total_events": len(events),
                "matches_found": pipeline_result.match_result.match_count,
                "ambiguous": pipeline_result.match_result.ambiguous_count,
                "canonicals_created": pipeline_result.canonical_count,
                "flagged_for_review": pipeline_result.flagged_count,
            }
        except Exception as e:
            log.error("pipeline_failed", error=str(e), exc_info=True)
            return {"status": "error", "file": file_path.name, "error": str(e)}
    ```

    Apply the same pattern to `process_file_batch()`: after `run_full_pipeline`, check `matching_config.ai.enabled` and call `resolve_ambiguous_pairs` followed by re-clustering if needed. The logic is identical -- extract it into a helper function `_maybe_resolve_ai` that both functions call:

    ```python
    async def _maybe_resolve_ai(
        pipeline_result: PipelineResult,
        events: list[dict],
        matching_config: MatchingConfig,
        session_factory: async_sessionmaker,
    ) -> PipelineResult:
        """Apply AI resolution to ambiguous pairs if enabled.

        Re-runs clustering and synthesis if any ambiguous pairs were resolved.
        Returns the updated PipelineResult (or the original if AI is disabled).
        """
        if not matching_config.ai.enabled or not matching_config.ai.api_key:
            return pipeline_result

        original_ambiguous = pipeline_result.match_result.ambiguous_count
        updated_match_result = await resolve_ambiguous_pairs(
            pipeline_result.match_result,
            events,
            matching_config.ai,
            session_factory,
        )

        if updated_match_result.ambiguous_count == original_ambiguous:
            return pipeline_result  # No changes

        # Re-cluster and re-synthesize with updated decisions
        from event_dedup.canonical.synthesizer import synthesize_canonical
        from event_dedup.clustering.graph_cluster import cluster_matches
        from event_dedup.matching.pipeline import (
            PipelineResult,
            _avg_cluster_confidence,
        )

        all_event_ids = [e["id"] for e in events]
        events_by_id = {e["id"]: e for e in events}
        cluster_result = cluster_matches(
            updated_match_result.decisions,
            all_event_ids,
            matching_config.cluster,
            events_by_id,
        )

        canonical_events = []
        for cluster in cluster_result.clusters:
            sources = [events_by_id[eid] for eid in cluster]
            canonical = synthesize_canonical(sources, matching_config.canonical)
            canonical["needs_review"] = False
            canonical["match_confidence"] = _avg_cluster_confidence(
                cluster, updated_match_result.decisions
            )
            canonical_events.append(canonical)

        for cluster in cluster_result.flagged_clusters:
            sources = [events_by_id[eid] for eid in cluster]
            canonical = synthesize_canonical(sources, matching_config.canonical)
            canonical["needs_review"] = True
            canonical["match_confidence"] = _avg_cluster_confidence(
                cluster, updated_match_result.decisions
            )
            canonical_events.append(canonical)

        return PipelineResult(
            match_result=updated_match_result,
            cluster_result=cluster_result,
            canonical_events=canonical_events,
            canonical_count=len(canonical_events),
            flagged_count=len(cluster_result.flagged_clusters),
        )
    ```

    Then simplify both `process_new_file` and `process_file_batch` to call `pipeline_result = await _maybe_resolve_ai(pipeline_result, events, matching_config, session_factory)` after `run_full_pipeline`.

    **3. Update `src/event_dedup/config/settings.py`** to add Gemini API key env var:

    Add to `Settings` class:
    ```python
    gemini_api_key: str = ""
    ```

    This maps to `EVENT_DEDUP_GEMINI_API_KEY` env var (via the `EVENT_DEDUP_` prefix).

    **4. Update `src/event_dedup/worker/__main__.py`** to pass the Gemini API key from settings to matching config:

    After `matching_config = load_matching_config(settings.matching_config_path)`, add:
    ```python
    # Override AI config from environment
    if settings.gemini_api_key:
        matching_config.ai.enabled = True
        matching_config.ai.api_key = settings.gemini_api_key
    ```

    **5. Update `docker-compose.yml`** to add optional Gemini API key environment variable to the worker service:

    In the `worker` service `environment` section, add:
    ```yaml
    EVENT_DEDUP_GEMINI_API_KEY: "${GEMINI_API_KEY:-}"
    ```

    This passes the host's `GEMINI_API_KEY` env var into the container. When empty/unset, AI matching stays disabled (the default). Users enable it by setting `GEMINI_API_KEY=your-key` in a `.env` file or shell environment before running `docker-compose up`.
  </action>
  <verify>
    <automated>cd /Users/svenkarl/workspaces/event-deduplication && uv run python -c "
from event_dedup.ai_matching.resolver import resolve_ambiguous_pairs, _apply_ai_result
from event_dedup.ai_matching.schemas import AIMatchResult
from event_dedup.matching.pipeline import MatchDecisionRecord
from event_dedup.matching.combiner import SignalScores
from event_dedup.config.settings import Settings

# Test _apply_ai_result
signals = SignalScores(date=0.5, geo=0.5, title=0.5, description=0.5)
d = MatchDecisionRecord('a', 'b', signals, 0.5, 'ambiguous')

# High confidence same -> match
r = _apply_ai_result(d, AIMatchResult(decision='same', confidence=0.9, reasoning='test'), 0.6)
assert r.decision == 'match' and r.tier == 'ai'

# High confidence different -> no_match
r = _apply_ai_result(d, AIMatchResult(decision='different', confidence=0.8, reasoning='test'), 0.6)
assert r.decision == 'no_match' and r.tier == 'ai'

# Low confidence -> stays ambiguous
r = _apply_ai_result(d, AIMatchResult(decision='same', confidence=0.4, reasoning='test'), 0.6)
assert r.decision == 'ambiguous' and r.tier == 'ai_low_confidence'

# Settings has gemini_api_key
s = Settings(gemini_api_key='test')
assert s.gemini_api_key == 'test'

print('All resolver and settings assertions OK')
"</automated>
  </verify>
  <done>
    - resolve_ambiguous_pairs filters ambiguous, checks cache, calls Gemini, updates decisions
    - _apply_ai_result maps same->match, different->no_match with confidence threshold
    - Low-confidence AI results keep decision as ambiguous with tier="ai_low_confidence"
    - API failures keep individual pair as ambiguous (no pipeline crash)
    - Concurrent requests limited by asyncio.Semaphore
    - Worker orchestrator calls resolve_ambiguous_pairs between scoring and clustering
    - Re-clusters and re-synthesizes if ambiguous pairs were resolved
    - Settings.gemini_api_key maps to EVENT_DEDUP_GEMINI_API_KEY env var
    - Worker __main__ enables AI matching when API key is provided
    - Docker-compose passes GEMINI_API_KEY through to worker container
  </done>
</task>

<task type="auto">
  <name>Task 2: Evaluation harness update and resolver tests</name>
  <files>
    src/event_dedup/evaluation/harness.py
    tests/test_ai_resolver.py
    tests/test_orchestrator.py
  </files>
  <action>
    **1. Update `src/event_dedup/evaluation/harness.py`** to add AI-assisted evaluation:

    Add a new function `run_multisignal_evaluation_with_ai` that runs the full pipeline including AI resolution. This allows comparing F1 scores with and without AI matching.

    Add the following at the bottom of the file (after the existing `run_multisignal_evaluation`):

    ```python
    async def run_ai_comparison_evaluation(
        session: AsyncSession,
        matching_config: MatchingConfig,
        session_factory: async_sessionmaker | None = None,
    ) -> dict:
        """Run evaluation comparing deterministic-only vs AI-assisted matching.

        Produces a side-by-side comparison showing how AI matching improves
        (or changes) the F1 score on the ground truth dataset.

        Args:
            session: Async SQLAlchemy session for loading events/ground truth.
            matching_config: Full matching config (must have ai section configured).
            session_factory: Async session factory for AI cache/usage DB access.
                Required when AI is enabled.

        Returns:
            Dict with deterministic_metrics, ai_metrics, and improvement delta.
        """
        gt_same, gt_diff = await load_ground_truth(session)

        # Load all source events
        result = await session.execute(
            select(SourceEvent).options(selectinload(SourceEvent.dates))
        )
        source_events = result.scalars().all()

        events = []
        for evt in source_events:
            events.append(
                {
                    "id": evt.id,
                    "title": evt.title,
                    "title_normalized": evt.title_normalized,
                    "short_description": evt.short_description,
                    "short_description_normalized": evt.short_description_normalized,
                    "description": evt.description,
                    "highlights": evt.highlights,
                    "location_name": evt.location_name,
                    "location_city": evt.location_city,
                    "location_district": evt.location_district,
                    "location_street": evt.location_street,
                    "location_zipcode": evt.location_zipcode,
                    "geo_latitude": evt.geo_latitude,
                    "geo_longitude": evt.geo_longitude,
                    "geo_confidence": evt.geo_confidence,
                    "source_code": evt.source_code,
                    "source_type": evt.source_type,
                    "blocking_keys": evt.blocking_keys,
                    "categories": evt.categories,
                    "is_family_event": evt.is_family_event,
                    "is_child_focused": evt.is_child_focused,
                    "admission_free": evt.admission_free,
                    "dates": [
                        {
                            "date": str(d.date),
                            "start_time": str(d.start_time) if d.start_time else None,
                            "end_time": str(d.end_time) if d.end_time else None,
                            "end_date": str(d.end_date) if d.end_date else None,
                        }
                        for d in evt.dates
                    ],
                }
            )

        # --- Deterministic-only ---
        det_predicted = generate_predictions_multisignal(events, matching_config)
        det_metrics = compute_metrics(det_predicted, gt_same, gt_diff)

        # --- With AI assistance ---
        if matching_config.ai.enabled and session_factory is not None:
            from event_dedup.ai_matching.resolver import resolve_ambiguous_pairs
            from event_dedup.matching.pipeline import get_match_pairs, score_candidate_pairs

            match_result = score_candidate_pairs(events, matching_config)
            match_result = await resolve_ambiguous_pairs(
                match_result, events, matching_config.ai, session_factory,
            )
            ai_predicted = get_match_pairs(match_result)
        else:
            ai_predicted = det_predicted

        ai_metrics = compute_metrics(ai_predicted, gt_same, gt_diff)

        # Print comparison
        print("\n" + "=" * 80)
        print("  Deterministic vs AI-Assisted Matching Comparison")
        print("=" * 80)
        print(f"  {'Metric':>15s}  {'Deterministic':>15s}  {'AI-Assisted':>15s}  {'Delta':>10s}")
        print("-" * 80)
        for name, det_val, ai_val in [
            ("Precision", det_metrics.precision, ai_metrics.precision),
            ("Recall", det_metrics.recall, ai_metrics.recall),
            ("F1", det_metrics.f1, ai_metrics.f1),
            ("True Pos", det_metrics.true_positives, ai_metrics.true_positives),
            ("False Pos", det_metrics.false_positives, ai_metrics.false_positives),
            ("False Neg", det_metrics.false_negatives, ai_metrics.false_negatives),
        ]:
            if isinstance(det_val, int):
                delta = ai_val - det_val
                print(f"  {name:>15s}  {det_val:>15d}  {ai_val:>15d}  {delta:>+10d}")
            else:
                delta = ai_val - det_val
                print(f"  {name:>15s}  {det_val:>15.4f}  {ai_val:>15.4f}  {delta:>+10.4f}")
        print("=" * 80 + "\n")

        return {
            "deterministic": {
                "precision": det_metrics.precision,
                "recall": det_metrics.recall,
                "f1": det_metrics.f1,
                "true_positives": det_metrics.true_positives,
                "false_positives": det_metrics.false_positives,
                "false_negatives": det_metrics.false_negatives,
            },
            "ai_assisted": {
                "precision": ai_metrics.precision,
                "recall": ai_metrics.recall,
                "f1": ai_metrics.f1,
                "true_positives": ai_metrics.true_positives,
                "false_positives": ai_metrics.false_positives,
                "false_negatives": ai_metrics.false_negatives,
            },
            "f1_improvement": ai_metrics.f1 - det_metrics.f1,
        }
    ```

    Add the necessary import at the top of the file (add `async_sessionmaker` to the sqlalchemy imports):
    ```python
    from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker
    ```

    NOTE: The `async_sessionmaker` import may already be available or may need to be added. Check the existing imports and add only what is missing.

    **2. Create `tests/test_ai_resolver.py`** with tests using a mock Gemini client:

    ```python
    """Tests for AI matching resolver.

    Uses mocked Gemini client to test resolution logic without API calls.
    """
    from __future__ import annotations

    from unittest.mock import AsyncMock, patch

    import pytest

    from event_dedup.ai_matching.resolver import _apply_ai_result, resolve_ambiguous_pairs
    from event_dedup.ai_matching.schemas import AIMatchResult
    from event_dedup.matching.combiner import SignalScores
    from event_dedup.matching.config import AIMatchingConfig
    from event_dedup.matching.pipeline import MatchDecisionRecord, MatchResult
    from event_dedup.matching.candidate_pairs import CandidatePairStats


    # ---- Helpers ----

    def _make_signals(score: float = 0.5) -> SignalScores:
        return SignalScores(date=score, geo=score, title=score, description=score)


    def _make_decision(
        id_a: str, id_b: str, decision: str = "ambiguous", score: float = 0.5
    ) -> MatchDecisionRecord:
        return MatchDecisionRecord(
            event_id_a=id_a,
            event_id_b=id_b,
            signals=_make_signals(score),
            combined_score_value=score,
            decision=decision,
        )


    def _make_match_result(decisions: list[MatchDecisionRecord]) -> MatchResult:
        return MatchResult(
            decisions=decisions,
            pair_stats=CandidatePairStats(
                total_events=10, total_pairs=5, blocked_pairs=5, reduction_pct=50.0
            ),
            match_count=sum(1 for d in decisions if d.decision == "match"),
            ambiguous_count=sum(1 for d in decisions if d.decision == "ambiguous"),
            no_match_count=sum(1 for d in decisions if d.decision == "no_match"),
        )


    def _make_event(id: str, title: str = "Event") -> dict:
        return {
            "id": id,
            "title": title,
            "description": "Description",
            "short_description": "Short",
            "source_code": "bwb",
            "source_type": "artikel",
            "location_name": "Marktplatz",
            "location_city": "Freiburg",
            "dates": [{"date": "2026-01-01"}],
            "categories": ["test"],
        }


    # ---- _apply_ai_result tests ----

    class TestApplyAIResult:
        def test_high_confidence_same(self):
            d = _make_decision("a", "b")
            r = _apply_ai_result(d, AIMatchResult(decision="same", confidence=0.9, reasoning="match"), 0.6)
            assert r.decision == "match"
            assert r.tier == "ai"

        def test_high_confidence_different(self):
            d = _make_decision("a", "b")
            r = _apply_ai_result(d, AIMatchResult(decision="different", confidence=0.8, reasoning="no match"), 0.6)
            assert r.decision == "no_match"
            assert r.tier == "ai"

        def test_low_confidence_stays_ambiguous(self):
            d = _make_decision("a", "b")
            r = _apply_ai_result(d, AIMatchResult(decision="same", confidence=0.3, reasoning="unsure"), 0.6)
            assert r.decision == "ambiguous"
            assert r.tier == "ai_low_confidence"

        def test_exactly_at_threshold(self):
            d = _make_decision("a", "b")
            r = _apply_ai_result(d, AIMatchResult(decision="same", confidence=0.6, reasoning="borderline"), 0.6)
            assert r.decision == "match"
            assert r.tier == "ai"

        def test_preserves_signal_scores(self):
            signals = SignalScores(date=0.9, geo=0.8, title=0.7, description=0.6)
            d = MatchDecisionRecord("a", "b", signals, 0.75, "ambiguous")
            r = _apply_ai_result(d, AIMatchResult(decision="same", confidence=0.9, reasoning="match"), 0.6)
            assert r.signals == signals
            assert r.combined_score_value == 0.75


    # ---- resolve_ambiguous_pairs tests ----

    class TestResolveAmbiguousPairs:
        @pytest.fixture
        def ai_config(self):
            return AIMatchingConfig(
                enabled=True,
                api_key="test-key",
                model="gemini-2.5-flash",
                confidence_threshold=0.6,
                cache_enabled=False,  # Disable cache for simpler testing
                max_concurrent_requests=5,
            )

        async def test_no_ambiguous_noop(self, test_session_factory, ai_config):
            """When no ambiguous pairs exist, result is returned unchanged."""
            decisions = [_make_decision("a", "b", "match")]
            match_result = _make_match_result(decisions)
            events = [_make_event("a"), _make_event("b")]

            result = await resolve_ambiguous_pairs(
                match_result, events, ai_config, test_session_factory
            )
            assert result.match_count == 1
            assert result.ambiguous_count == 0

        @patch("event_dedup.ai_matching.resolver.call_gemini")
        @patch("event_dedup.ai_matching.resolver.create_client")
        async def test_resolves_ambiguous_to_match(
            self, mock_create, mock_call, test_session_factory, ai_config
        ):
            """Ambiguous pair resolved to match by AI."""
            mock_create.return_value = AsyncMock()
            mock_call.return_value = (
                AIMatchResult(decision="same", confidence=0.9, reasoning="Same event"),
                800, 100,
            )

            decisions = [
                _make_decision("a", "b", "match"),
                _make_decision("c", "d", "ambiguous"),
            ]
            match_result = _make_match_result(decisions)
            events = [_make_event("a"), _make_event("b"), _make_event("c"), _make_event("d")]

            result = await resolve_ambiguous_pairs(
                match_result, events, ai_config, test_session_factory
            )
            assert result.match_count == 2  # Original match + AI match
            assert result.ambiguous_count == 0
            assert mock_call.call_count == 1

        @patch("event_dedup.ai_matching.resolver.call_gemini")
        @patch("event_dedup.ai_matching.resolver.create_client")
        async def test_resolves_ambiguous_to_no_match(
            self, mock_create, mock_call, test_session_factory, ai_config
        ):
            """Ambiguous pair resolved to no_match by AI."""
            mock_create.return_value = AsyncMock()
            mock_call.return_value = (
                AIMatchResult(decision="different", confidence=0.85, reasoning="Different events"),
                800, 100,
            )

            decisions = [_make_decision("a", "b", "ambiguous")]
            match_result = _make_match_result(decisions)
            events = [_make_event("a"), _make_event("b")]

            result = await resolve_ambiguous_pairs(
                match_result, events, ai_config, test_session_factory
            )
            assert result.match_count == 0
            assert result.no_match_count == 1
            assert result.ambiguous_count == 0

        @patch("event_dedup.ai_matching.resolver.call_gemini")
        @patch("event_dedup.ai_matching.resolver.create_client")
        async def test_low_confidence_stays_ambiguous(
            self, mock_create, mock_call, test_session_factory, ai_config
        ):
            """Low-confidence AI result keeps pair as ambiguous."""
            mock_create.return_value = AsyncMock()
            mock_call.return_value = (
                AIMatchResult(decision="same", confidence=0.3, reasoning="Unsure"),
                800, 100,
            )

            decisions = [_make_decision("a", "b", "ambiguous")]
            match_result = _make_match_result(decisions)
            events = [_make_event("a"), _make_event("b")]

            result = await resolve_ambiguous_pairs(
                match_result, events, ai_config, test_session_factory
            )
            assert result.ambiguous_count == 1
            assert result.match_count == 0

        @patch("event_dedup.ai_matching.resolver.call_gemini")
        @patch("event_dedup.ai_matching.resolver.create_client")
        async def test_api_failure_keeps_ambiguous(
            self, mock_create, mock_call, test_session_factory, ai_config
        ):
            """API failure keeps pair as ambiguous without crashing."""
            mock_create.return_value = AsyncMock()
            mock_call.side_effect = Exception("API rate limit exceeded")

            decisions = [_make_decision("a", "b", "ambiguous")]
            match_result = _make_match_result(decisions)
            events = [_make_event("a"), _make_event("b")]

            result = await resolve_ambiguous_pairs(
                match_result, events, ai_config, test_session_factory
            )
            assert result.ambiguous_count == 1
            assert result.decisions[0].decision == "ambiguous"

        @patch("event_dedup.ai_matching.resolver.call_gemini")
        @patch("event_dedup.ai_matching.resolver.create_client")
        async def test_non_ambiguous_unchanged(
            self, mock_create, mock_call, test_session_factory, ai_config
        ):
            """Match and no_match decisions are never sent to AI."""
            mock_create.return_value = AsyncMock()
            mock_call.return_value = (
                AIMatchResult(decision="same", confidence=0.9, reasoning="Same"),
                800, 100,
            )

            decisions = [
                _make_decision("a", "b", "match"),
                _make_decision("c", "d", "no_match"),
                _make_decision("e", "f", "ambiguous"),
            ]
            match_result = _make_match_result(decisions)
            events = [_make_event(id) for id in ["a", "b", "c", "d", "e", "f"]]

            result = await resolve_ambiguous_pairs(
                match_result, events, ai_config, test_session_factory
            )

            # Only the ambiguous pair was sent to AI
            assert mock_call.call_count == 1
            # Original match and no_match preserved
            assert result.match_count == 2  # original + AI resolved
            assert result.no_match_count == 1

        @patch("event_dedup.ai_matching.resolver.call_gemini")
        @patch("event_dedup.ai_matching.resolver.create_client")
        async def test_cache_hit_skips_api(
            self, mock_create, mock_call, test_session_factory
        ):
            """Cache hits avoid API calls."""
            ai_config = AIMatchingConfig(
                enabled=True, api_key="test-key", model="gemini-2.5-flash",
                confidence_threshold=0.6, cache_enabled=True,
            )

            # Pre-populate cache
            from event_dedup.ai_matching.cache import compute_pair_hash, store_cache
            from event_dedup.ai_matching.schemas import AIMatchResult as AIR

            evt_a = _make_event("a", "Event A")
            evt_b = _make_event("b", "Event B")
            pair_hash = compute_pair_hash(evt_a, evt_b)
            await store_cache(
                test_session_factory, pair_hash, "a", "b",
                AIR(decision="same", confidence=0.9, reasoning="Cached match"),
                "gemini-2.5-flash",
            )

            mock_create.return_value = AsyncMock()
            decisions = [_make_decision("a", "b", "ambiguous")]
            match_result = _make_match_result(decisions)

            result = await resolve_ambiguous_pairs(
                match_result, [evt_a, evt_b], ai_config, test_session_factory
            )

            # API was never called
            assert mock_call.call_count == 0
            assert result.match_count == 1
    ```

    **3. Update `tests/test_orchestrator.py`** to verify AI integration:

    Read the existing test file first. Add ONE new test that verifies the orchestrator correctly calls AI resolution when enabled. The test mocks both the file processor and the AI resolver:

    Add a test at the end of the file:

    ```python
    @patch("event_dedup.worker.orchestrator.resolve_ambiguous_pairs")
    async def test_process_new_file_with_ai_enabled(
        mock_resolve_ai, test_session_factory, tmp_path
    ):
        """When AI is enabled, resolve_ambiguous_pairs is called after scoring."""
        # ... set up file_processor mock, seed DB, etc. following existing test patterns ...
        # The key assertion is that mock_resolve_ai was called when matching_config.ai.enabled = True
    ```

    The exact implementation follows the existing test patterns in the file. Read the existing `test_orchestrator.py` to match the fixture usage, mock patterns, and event seeding approach. The test should:
    1. Create a matching config with `ai=AIMatchingConfig(enabled=True, api_key="test-key")`
    2. Mock `resolve_ambiguous_pairs` to return the input match_result unchanged
    3. Run `process_new_file`
    4. Assert `resolve_ambiguous_pairs` was called once
    5. Also add a test that verifies AI is NOT called when `ai.enabled=False` (the mock should NOT be called)
  </action>
  <verify>
    <automated>cd /Users/svenkarl/workspaces/event-deduplication && uv run pytest tests/test_ai_resolver.py tests/test_ai_matching.py -x -v 2>&1 | tail -40</automated>
  </verify>
  <done>
    - Evaluation harness has run_ai_comparison_evaluation for deterministic vs AI-assisted F1 comparison
    - Comparison prints side-by-side precision/recall/F1/TP/FP/FN table
    - Resolver tests cover: no ambiguous noop, resolve to match, resolve to no_match, low confidence stays ambiguous, API failure stays ambiguous, non-ambiguous unchanged, cache hit skips API
    - Orchestrator tests verify AI is called when enabled and skipped when disabled
    - All new tests pass
    - No regressions in existing test suite
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/test_ai_resolver.py -x -v` -- all resolver tests pass with mocked Gemini
2. `uv run pytest tests/test_ai_matching.py -x -v` -- all infrastructure tests pass
3. `uv run pytest tests/test_orchestrator.py -x -v` -- orchestrator tests pass including AI integration
4. `uv run python -c "from event_dedup.ai_matching.resolver import resolve_ambiguous_pairs; print('resolver OK')"` -- resolver importable
5. `uv run python -c "from event_dedup.evaluation.harness import run_ai_comparison_evaluation; print('eval OK')"` -- evaluation harness updated
6. `uv run pytest tests/ -x --timeout=60` -- full test suite passes (no regressions)
7. `grep GEMINI_API_KEY docker-compose.yml` -- env var configured for worker container
</verification>

<success_criteria>
- resolve_ambiguous_pairs orchestrates cache -> API -> update for each ambiguous pair
- Confidence threshold (0.6) determines whether AI decision overrides ambiguous
- Pipeline works identically when AI is disabled (ai.enabled=False default)
- Worker orchestrator calls AI resolution between scoring and clustering
- Re-clustering happens only when ambiguous pairs are actually resolved
- Evaluation harness can compare deterministic-only vs AI-assisted F1 scores
- GEMINI_API_KEY env var flows from host -> docker-compose -> worker -> matching config
- All tests pass including resolver, infrastructure, and orchestrator tests
- No regressions in existing test suite
</success_criteria>

<output>
After completion, create `.planning/phases/05/05-02-SUMMARY.md`
</output>
