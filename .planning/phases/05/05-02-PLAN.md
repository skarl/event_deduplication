---
phase: 05-ai-matching
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - src/event_dedup/ai_matching/resolver.py
  - src/event_dedup/matching/pipeline.py
  - src/event_dedup/worker/orchestrator.py
  - src/event_dedup/config/settings.py
  - src/event_dedup/worker/__main__.py
  - src/event_dedup/evaluation/harness.py
  - docker-compose.yml
  - tests/test_ai_resolver.py
  - tests/test_orchestrator.py
autonomous: true
requirements: [AI-01, AI-02, AI-03, AI-04, AI-05]

must_haves:
  truths:
    - "Ambiguous match decisions from deterministic scoring are filtered and sent to Gemini for resolution"
    - "AI decisions with confidence >= 0.6 override ambiguous to match/no_match with tier='ai'"
    - "AI decisions with confidence < 0.6 remain as ambiguous for human review"
    - "Cache hits skip the API call; cache misses call Gemini and store the result"
    - "Each resolve_ambiguous_pairs call logs token usage and cost per request and produces a batch summary"
    - "Pipeline continues working with AI disabled (no regression on existing behavior)"
    - "Concurrent API calls are limited by asyncio.Semaphore"
    - "API failures for individual pairs are handled gracefully (pair stays ambiguous, no pipeline crash)"
    - "Evaluation harness can measure F1 with and without AI matching for comparison"
  artifacts:
    - path: "src/event_dedup/ai_matching/resolver.py"
      provides: "Orchestrator: filter ambiguous -> check cache -> call AI -> update decisions"
      contains: "resolve_ambiguous_pairs"
    - path: "src/event_dedup/matching/pipeline.py"
      provides: "Public rebuild_pipeline_result function for re-clustering after AI resolution"
      contains: "rebuild_pipeline_result"
    - path: "src/event_dedup/worker/orchestrator.py"
      provides: "Updated pipeline with AI resolution step between scoring and clustering"
      contains: "_maybe_resolve_ai"
    - path: "src/event_dedup/evaluation/harness.py"
      provides: "Updated evaluation with AI-assisted matching comparison"
      contains: "run_ai_comparison_evaluation"
    - path: "tests/test_ai_resolver.py"
      provides: "Tests for resolver with mocked Gemini client"
    - path: "tests/test_orchestrator.py"
      provides: "Updated orchestrator tests verifying AI step integration"
  key_links:
    - from: "src/event_dedup/ai_matching/resolver.py"
      to: "src/event_dedup/ai_matching/client.py"
      via: "call_gemini for API calls"
      pattern: "call_gemini"
    - from: "src/event_dedup/ai_matching/resolver.py"
      to: "src/event_dedup/ai_matching/cache.py"
      via: "lookup_cache / store_cache for caching"
      pattern: "lookup_cache|store_cache"
    - from: "src/event_dedup/ai_matching/resolver.py"
      to: "src/event_dedup/ai_matching/cost_tracker.py"
      via: "log_usage for each resolution"
      pattern: "log_usage"
    - from: "src/event_dedup/worker/orchestrator.py"
      to: "src/event_dedup/ai_matching/resolver.py"
      via: "resolve_ambiguous_pairs call in _maybe_resolve_ai"
      pattern: "resolve_ambiguous_pairs"
    - from: "src/event_dedup/worker/orchestrator.py"
      to: "src/event_dedup/matching/pipeline.py"
      via: "rebuild_pipeline_result for re-clustering"
      pattern: "rebuild_pipeline_result"
    - from: "src/event_dedup/worker/orchestrator.py"
      to: "src/event_dedup/matching/config.py"
      via: "matching_config.ai.enabled check"
      pattern: "matching_config\\.ai\\.enabled"
---

<objective>
Wire the AI matching infrastructure (from Plan 05-01) into the pipeline: build the resolver orchestrator, integrate into the worker, update the evaluation harness to measure F1 improvement, and add Docker/env configuration.

Purpose: This plan makes AI matching operational. After this plan, dropping a JSON file into the watch directory triggers deterministic scoring, AI resolution of ambiguous pairs, and clustering -- the full tiered matching pipeline.

Output: Working AI-assisted matching pipeline; evaluation harness that can compare deterministic-only vs AI-assisted F1 scores; Docker environment with Gemini API key configuration.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05/05-RESEARCH.md
@.planning/phases/05/05-01-SUMMARY.md

@src/event_dedup/ai_matching/__init__.py
@src/event_dedup/ai_matching/schemas.py
@src/event_dedup/ai_matching/client.py
@src/event_dedup/ai_matching/prompt.py
@src/event_dedup/ai_matching/cache.py
@src/event_dedup/ai_matching/cost_tracker.py
@src/event_dedup/matching/config.py
@src/event_dedup/matching/pipeline.py
@src/event_dedup/matching/combiner.py
@src/event_dedup/worker/orchestrator.py
@src/event_dedup/worker/__main__.py
@src/event_dedup/worker/persistence.py
@src/event_dedup/config/settings.py
@src/event_dedup/evaluation/harness.py
@docker-compose.yml
@tests/conftest.py
@tests/test_orchestrator.py

<interfaces>
<!-- Key types and contracts from Plan 05-01. -->

From src/event_dedup/ai_matching/schemas.py:
```python
class AIMatchResult(BaseModel):
    decision: str   # "same" or "different"
    confidence: float  # 0.0 to 1.0
    reasoning: str
```

From src/event_dedup/ai_matching/client.py:
```python
def create_client(api_key: str) -> genai.Client
async def call_gemini(
    client: genai.Client,
    event_a: dict, event_b: dict,
    signals: SignalScores, ai_config: AIMatchingConfig,
) -> tuple[AIMatchResult, int, int]  # (result, prompt_tokens, completion_tokens)
```

From src/event_dedup/ai_matching/cache.py:
```python
def compute_pair_hash(event_a: dict, event_b: dict) -> str  # 64-char hex
async def lookup_cache(session_factory, pair_hash, current_model) -> AIMatchResult | None
async def store_cache(session_factory, pair_hash, event_id_a, event_id_b, result, model) -> None
```

From src/event_dedup/ai_matching/cost_tracker.py:
```python
def estimate_cost(prompt_tokens, completion_tokens, config) -> float
async def log_usage(session_factory, batch_id, event_id_a, event_id_b, model, prompt_tokens, completion_tokens, estimated_cost_usd, cached) -> None
async def get_batch_summary(session_factory, batch_id) -> dict
```

From src/event_dedup/matching/config.py:
```python
class AIMatchingConfig(BaseModel):
    enabled: bool = False
    api_key: str = ""
    model: str = "gemini-2.5-flash"
    temperature: float = 0.1
    max_output_tokens: int = 300
    max_concurrent_requests: int = 5
    confidence_threshold: float = 0.6
    cache_enabled: bool = True
    cost_per_1m_input_tokens: float = 0.30
    cost_per_1m_output_tokens: float = 2.50

class MatchingConfig(BaseModel):
    scoring: ScoringWeights
    thresholds: ThresholdConfig
    geo: GeoConfig
    date: DateConfig
    title: TitleConfig
    cluster: ClusterConfig
    canonical: CanonicalConfig
    ai: AIMatchingConfig = AIMatchingConfig()
```

From src/event_dedup/matching/pipeline.py:
```python
@dataclass
class MatchDecisionRecord:
    event_id_a: str
    event_id_b: str
    signals: SignalScores
    combined_score_value: float
    decision: str          # "match", "no_match", "ambiguous"
    tier: str = "deterministic"

@dataclass
class MatchResult:
    decisions: list[MatchDecisionRecord]
    pair_stats: CandidatePairStats
    match_count: int
    ambiguous_count: int
    no_match_count: int

@dataclass
class PipelineResult:
    match_result: MatchResult
    cluster_result: ClusterResult
    canonical_events: list[dict]
    canonical_count: int
    flagged_count: int

def score_candidate_pairs(events, config) -> MatchResult
def run_full_pipeline(events, config) -> PipelineResult
def get_match_pairs(result: MatchResult) -> set[tuple[str, str]]
def _avg_cluster_confidence(cluster, decisions) -> float | None  # private
```

From src/event_dedup/worker/orchestrator.py:
```python
async def process_new_file(file_path, file_processor, session_factory, matching_config) -> dict
async def process_file_batch(file_paths, file_processor, session_factory, matching_config) -> list[dict]
async def process_existing_files(data_dir, file_processor, session_factory, matching_config) -> int
# NOTE: process_existing_files delegates to process_new_file per file, so it inherits AI behavior automatically.
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: AI resolver, pipeline rebuild function, and pipeline integration</name>
  <files>
    src/event_dedup/ai_matching/resolver.py
    src/event_dedup/matching/pipeline.py
    src/event_dedup/worker/orchestrator.py
    src/event_dedup/config/settings.py
    src/event_dedup/worker/__main__.py
    docker-compose.yml
  </files>
  <action>
    **1. Create `src/event_dedup/ai_matching/resolver.py`** -- the core orchestrator that filters ambiguous pairs, checks cache, calls Gemini, and returns updated decisions:

    ```python
    """AI matching resolver: orchestrates ambiguous pair resolution.

    Filters ambiguous decisions from deterministic scoring, checks the
    content-hash cache, calls Gemini Flash for uncached pairs, applies
    confidence threshold, and returns an updated MatchResult.
    """
    from __future__ import annotations

    import asyncio
    import uuid

    import structlog
    from sqlalchemy.ext.asyncio import async_sessionmaker

    from event_dedup.ai_matching.cache import compute_pair_hash, lookup_cache, store_cache
    from event_dedup.ai_matching.client import call_gemini, create_client
    from event_dedup.ai_matching.cost_tracker import estimate_cost, get_batch_summary, log_usage
    from event_dedup.ai_matching.schemas import AIMatchResult
    from event_dedup.matching.config import AIMatchingConfig
    from event_dedup.matching.pipeline import MatchDecisionRecord, MatchResult

    logger = structlog.get_logger()


    async def resolve_ambiguous_pairs(
        match_result: MatchResult,
        events: list[dict],
        ai_config: AIMatchingConfig,
        session_factory: async_sessionmaker,
    ) -> MatchResult:
        """Resolve ambiguous match decisions via Gemini Flash AI.

        Filters for decisions where decision=="ambiguous", resolves each
        via cache lookup or API call, and returns an updated MatchResult
        with resolved decisions.

        This function is called from the worker orchestrator AFTER
        deterministic scoring and BEFORE clustering.

        Args:
            match_result: The deterministic scoring result.
            events: All event dicts (needed for prompt formatting).
            ai_config: AI matching configuration.
            session_factory: Async session factory for cache/usage DB access.

        Returns:
            Updated MatchResult with ambiguous pairs resolved (or still
            ambiguous if AI confidence is below threshold or API failed).
        """
        events_by_id = {e["id"]: e for e in events}
        ambiguous = [d for d in match_result.decisions if d.decision == "ambiguous"]

        if not ambiguous:
            logger.info("ai_resolver_skip", reason="no_ambiguous_pairs")
            return match_result

        batch_id = str(uuid.uuid4())[:8]
        log = logger.bind(batch_id=batch_id, ambiguous_count=len(ambiguous))
        log.info("ai_resolver_start")

        client = create_client(ai_config.api_key)
        semaphore = asyncio.Semaphore(ai_config.max_concurrent_requests)

        async def resolve_one(decision: MatchDecisionRecord) -> MatchDecisionRecord:
            """Resolve a single ambiguous decision."""
            async with semaphore:
                event_a = events_by_id[decision.event_id_a]
                event_b = events_by_id[decision.event_id_b]

                # Compute cache key
                pair_hash = compute_pair_hash(event_a, event_b)

                # Check cache
                if ai_config.cache_enabled:
                    cached = await lookup_cache(
                        session_factory, pair_hash, ai_config.model
                    )
                    if cached is not None:
                        # Log cache hit
                        await log_usage(
                            session_factory, batch_id,
                            decision.event_id_a, decision.event_id_b,
                            ai_config.model,
                            prompt_tokens=0, completion_tokens=0,
                            estimated_cost_usd=0.0, cached=True,
                        )
                        return _apply_ai_result(
                            decision, cached, ai_config.confidence_threshold
                        )

                # Call Gemini API
                try:
                    result, prompt_tokens, completion_tokens = await call_gemini(
                        client, event_a, event_b, decision.signals, ai_config,
                    )
                except Exception as e:
                    logger.warning(
                        "gemini_call_failed",
                        pair=f"{decision.event_id_a}:{decision.event_id_b}",
                        error=str(e),
                    )
                    # Log failed attempt (0 tokens, 0 cost)
                    await log_usage(
                        session_factory, batch_id,
                        decision.event_id_a, decision.event_id_b,
                        ai_config.model,
                        prompt_tokens=0, completion_tokens=0,
                        estimated_cost_usd=0.0, cached=False,
                    )
                    return decision  # Keep as ambiguous on failure

                # Calculate cost
                cost = estimate_cost(prompt_tokens, completion_tokens, ai_config)

                # Log API call
                await log_usage(
                    session_factory, batch_id,
                    decision.event_id_a, decision.event_id_b,
                    ai_config.model,
                    prompt_tokens=prompt_tokens,
                    completion_tokens=completion_tokens,
                    estimated_cost_usd=cost,
                    cached=False,
                )

                # Store in cache
                if ai_config.cache_enabled:
                    id_a, id_b = decision.event_id_a, decision.event_id_b
                    await store_cache(
                        session_factory, pair_hash, id_a, id_b,
                        result, ai_config.model,
                    )

                return _apply_ai_result(
                    decision, result, ai_config.confidence_threshold
                )

        # Resolve all ambiguous pairs concurrently (with semaphore limiting)
        resolved = await asyncio.gather(
            *[resolve_one(d) for d in ambiguous],
            return_exceptions=True,
        )

        # Build updated decisions list
        ambiguous_ids = {
            (d.event_id_a, d.event_id_b) for d in ambiguous
        }
        updated_decisions = [
            d for d in match_result.decisions
            if (d.event_id_a, d.event_id_b) not in ambiguous_ids
        ]

        for original, result in zip(ambiguous, resolved):
            if isinstance(result, Exception):
                logger.error(
                    "resolve_one_exception",
                    pair=f"{original.event_id_a}:{original.event_id_b}",
                    error=str(result),
                )
                updated_decisions.append(original)  # Keep as ambiguous
            else:
                updated_decisions.append(result)

        # Recount
        new_match = sum(1 for d in updated_decisions if d.decision == "match")
        new_ambiguous = sum(1 for d in updated_decisions if d.decision == "ambiguous")
        new_no_match = sum(1 for d in updated_decisions if d.decision == "no_match")

        # Log batch summary
        try:
            summary = await get_batch_summary(session_factory, batch_id)
            log.info(
                "ai_resolver_complete",
                resolved=len(ambiguous) - new_ambiguous,
                remaining_ambiguous=new_ambiguous,
                api_calls=summary["api_requests"],
                cache_hits=summary["cached_requests"],
                total_tokens=summary["total_tokens"],
                estimated_cost_usd=summary["estimated_cost_usd"],
            )
        except Exception:
            log.info("ai_resolver_complete", resolved=len(ambiguous) - new_ambiguous)

        return MatchResult(
            decisions=updated_decisions,
            pair_stats=match_result.pair_stats,
            match_count=new_match,
            ambiguous_count=new_ambiguous,
            no_match_count=new_no_match,
        )


    def _apply_ai_result(
        decision: MatchDecisionRecord,
        ai_result: AIMatchResult,
        confidence_threshold: float,
    ) -> MatchDecisionRecord:
        """Apply an AI result to a match decision.

        Maps AI decision "same" -> "match", "different" -> "no_match".
        Only overrides if AI confidence >= threshold; otherwise keeps
        as "ambiguous" for human review.

        Args:
            decision: Original ambiguous decision.
            ai_result: AI judgment (same/different + confidence).
            confidence_threshold: Minimum confidence to override.

        Returns:
            Updated MatchDecisionRecord (new instance).
        """
        if ai_result.confidence < confidence_threshold:
            # Low confidence -- keep as ambiguous for review queue
            return MatchDecisionRecord(
                event_id_a=decision.event_id_a,
                event_id_b=decision.event_id_b,
                signals=decision.signals,
                combined_score_value=decision.combined_score_value,
                decision="ambiguous",
                tier="ai_low_confidence",
            )

        # Map AI decision to pipeline decision
        if ai_result.decision == "same":
            new_decision = "match"
        elif ai_result.decision == "different":
            new_decision = "no_match"
        else:
            # Unexpected value -- keep as ambiguous
            return MatchDecisionRecord(
                event_id_a=decision.event_id_a,
                event_id_b=decision.event_id_b,
                signals=decision.signals,
                combined_score_value=decision.combined_score_value,
                decision="ambiguous",
                tier="ai_unexpected",
            )

        return MatchDecisionRecord(
            event_id_a=decision.event_id_a,
            event_id_b=decision.event_id_b,
            signals=decision.signals,
            combined_score_value=decision.combined_score_value,
            decision=new_decision,
            tier="ai",
        )
    ```

    **2. Add `rebuild_pipeline_result` to `src/event_dedup/matching/pipeline.py`.**

    This is a new public function that encapsulates the re-clustering and re-synthesis logic. It allows the orchestrator to rebuild the pipeline result after AI resolution modifies the match decisions, without duplicating the clustering/synthesis code from `run_full_pipeline`.

    Add this function at the end of the file (after `_avg_cluster_confidence`):

    ```python
    def rebuild_pipeline_result(
        match_result: MatchResult,
        events: list[dict],
        config: MatchingConfig,
    ) -> PipelineResult:
        """Rebuild clustering and synthesis from an updated MatchResult.

        Use this after AI resolution modifies match decisions. It re-runs
        clustering and canonical synthesis using the updated decisions
        while preserving the same logic as ``run_full_pipeline``.

        Args:
            match_result: Updated MatchResult (e.g., after AI resolution).
            events: All event dicts.
            config: Full matching configuration.

        Returns:
            A new ``PipelineResult`` with re-clustered and re-synthesized data.
        """
        # Lazy imports to avoid circular dependency (same as run_full_pipeline)
        from event_dedup.canonical.synthesizer import synthesize_canonical
        from event_dedup.clustering.graph_cluster import cluster_matches

        all_event_ids = [e["id"] for e in events]
        events_by_id = {e["id"]: e for e in events}

        cluster_result = cluster_matches(
            match_result.decisions, all_event_ids, config.cluster, events_by_id
        )

        canonical_events: list[dict] = []

        for cluster in cluster_result.clusters:
            sources = [events_by_id[eid] for eid in cluster]
            canonical = synthesize_canonical(sources, config.canonical)
            canonical["needs_review"] = False
            canonical["match_confidence"] = _avg_cluster_confidence(
                cluster, match_result.decisions
            )
            canonical_events.append(canonical)

        for cluster in cluster_result.flagged_clusters:
            sources = [events_by_id[eid] for eid in cluster]
            canonical = synthesize_canonical(sources, config.canonical)
            canonical["needs_review"] = True
            canonical["match_confidence"] = _avg_cluster_confidence(
                cluster, match_result.decisions
            )
            canonical_events.append(canonical)

        return PipelineResult(
            match_result=match_result,
            cluster_result=cluster_result,
            canonical_events=canonical_events,
            canonical_count=len(canonical_events),
            flagged_count=len(cluster_result.flagged_clusters),
        )
    ```

    **3. Update `src/event_dedup/worker/orchestrator.py`** to add the AI resolution step.

    Add these imports at the top of the file (after existing imports):

    ```python
    from event_dedup.ai_matching.resolver import resolve_ambiguous_pairs
    from event_dedup.matching.pipeline import rebuild_pipeline_result
    ```

    Add a private helper function `_maybe_resolve_ai` that both `process_new_file` and `process_file_batch` will call. Place it before `process_new_file`:

    ```python
    async def _maybe_resolve_ai(
        pipeline_result: PipelineResult,
        events: list[dict],
        matching_config: MatchingConfig,
        session_factory: async_sessionmaker,
    ) -> PipelineResult:
        """Apply AI resolution to ambiguous pairs if enabled.

        Calls resolve_ambiguous_pairs on the match result. If any ambiguous
        pairs were resolved, rebuilds clustering and synthesis via
        rebuild_pipeline_result. Returns the original result unchanged if
        AI is disabled or no ambiguous pairs were resolved.
        """
        if not matching_config.ai.enabled or not matching_config.ai.api_key:
            return pipeline_result

        original_ambiguous = pipeline_result.match_result.ambiguous_count
        updated_match_result = await resolve_ambiguous_pairs(
            pipeline_result.match_result,
            events,
            matching_config.ai,
            session_factory,
        )

        if updated_match_result.ambiguous_count == original_ambiguous:
            return pipeline_result  # No changes, skip re-clustering

        # Re-cluster and re-synthesize with updated decisions
        return rebuild_pipeline_result(updated_match_result, events, matching_config)
    ```

    Then update `process_new_file` -- add ONE line after `pipeline_result = run_full_pipeline(events, matching_config)`:

    ```python
        # Step 3: Run matching pipeline (pure function)
        pipeline_result = run_full_pipeline(events, matching_config)

        # Step 3.5: AI-assisted resolution of ambiguous pairs
        pipeline_result = await _maybe_resolve_ai(
            pipeline_result, events, matching_config, session_factory
        )

        log.info(
            "matching_complete",
            # ... (existing log fields unchanged) ...
        )
    ```

    The rest of `process_new_file` is unchanged -- it already logs and persists `pipeline_result`.

    Apply the identical change to `process_file_batch` -- add the same `_maybe_resolve_ai` call after `pipeline_result = run_full_pipeline(events, matching_config)` and before the existing logging.

    NOTE: `process_existing_files` delegates to `process_new_file` per file, so it inherits AI resolution behavior automatically with no code changes needed.

    **4. Update `src/event_dedup/config/settings.py`** to add Gemini API key env var:

    Add to `Settings` class:
    ```python
    gemini_api_key: str = ""
    ```

    This maps to `EVENT_DEDUP_GEMINI_API_KEY` env var (via the `EVENT_DEDUP_` prefix).

    **5. Update `src/event_dedup/worker/__main__.py`** to pass the Gemini API key from settings to matching config:

    After `matching_config = load_matching_config(settings.matching_config_path)`, add:
    ```python
    # Override AI config from environment
    if settings.gemini_api_key:
        matching_config.ai.enabled = True
        matching_config.ai.api_key = settings.gemini_api_key
    ```

    **6. Update `docker-compose.yml`** to add optional Gemini API key environment variable to the worker service:

    In the `worker` service `environment` section, add:
    ```yaml
    EVENT_DEDUP_GEMINI_API_KEY: "${GEMINI_API_KEY:-}"
    ```

    This passes the host's `GEMINI_API_KEY` env var into the container. When empty/unset, AI matching stays disabled (the default). Users enable it by setting `GEMINI_API_KEY=your-key` in a `.env` file or shell environment before running `docker-compose up`.
  </action>
  <verify>
    <automated>cd /Users/svenkarl/workspaces/event-deduplication && uv run python -c "
from event_dedup.ai_matching.resolver import resolve_ambiguous_pairs, _apply_ai_result
from event_dedup.ai_matching.schemas import AIMatchResult
from event_dedup.matching.pipeline import MatchDecisionRecord, rebuild_pipeline_result
from event_dedup.matching.combiner import SignalScores
from event_dedup.config.settings import Settings

# Test _apply_ai_result
signals = SignalScores(date=0.5, geo=0.5, title=0.5, description=0.5)
d = MatchDecisionRecord('a', 'b', signals, 0.5, 'ambiguous')

# High confidence same -> match
r = _apply_ai_result(d, AIMatchResult(decision='same', confidence=0.9, reasoning='test'), 0.6)
assert r.decision == 'match' and r.tier == 'ai'

# High confidence different -> no_match
r = _apply_ai_result(d, AIMatchResult(decision='different', confidence=0.8, reasoning='test'), 0.6)
assert r.decision == 'no_match' and r.tier == 'ai'

# Low confidence -> stays ambiguous
r = _apply_ai_result(d, AIMatchResult(decision='same', confidence=0.4, reasoning='test'), 0.6)
assert r.decision == 'ambiguous' and r.tier == 'ai_low_confidence'

# Settings has gemini_api_key
s = Settings(gemini_api_key='test')
assert s.gemini_api_key == 'test'

# rebuild_pipeline_result is importable
assert callable(rebuild_pipeline_result)

print('All resolver, rebuild, and settings assertions OK')
"</automated>
  </verify>
  <done>
    - resolve_ambiguous_pairs filters ambiguous, checks cache, calls Gemini, updates decisions
    - _apply_ai_result maps same->match, different->no_match with confidence threshold
    - Low-confidence AI results keep decision as ambiguous with tier="ai_low_confidence"
    - API failures keep individual pair as ambiguous (no pipeline crash)
    - Concurrent requests limited by asyncio.Semaphore
    - rebuild_pipeline_result is a public function in pipeline.py that re-clusters and re-synthesizes
    - Worker orchestrator uses _maybe_resolve_ai helper (calls resolve_ambiguous_pairs + rebuild_pipeline_result)
    - Both process_new_file and process_file_batch call _maybe_resolve_ai after run_full_pipeline
    - process_existing_files inherits AI behavior via delegation to process_new_file
    - Settings.gemini_api_key maps to EVENT_DEDUP_GEMINI_API_KEY env var
    - Worker __main__ enables AI matching when API key is provided
    - Docker-compose passes GEMINI_API_KEY through to worker container
  </done>
</task>

<task type="auto">
  <name>Task 2: Evaluation harness update and resolver tests</name>
  <files>
    src/event_dedup/evaluation/harness.py
    tests/test_ai_resolver.py
    tests/test_orchestrator.py
  </files>
  <action>
    **1. Update `src/event_dedup/evaluation/harness.py`** to add AI-assisted evaluation:

    Add a new function `run_ai_comparison_evaluation` that runs the full pipeline including AI resolution. This allows comparing F1 scores with and without AI matching.

    Add the necessary import at the top of the file -- add `async_sessionmaker` to the sqlalchemy imports:
    ```python
    from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker
    ```

    NOTE: The existing file already imports `AsyncSession`. Check and add only `async_sessionmaker` if missing.

    Add the following at the bottom of the file (after the existing `run_multisignal_evaluation`):

    ```python
    async def run_ai_comparison_evaluation(
        session: AsyncSession,
        matching_config: MatchingConfig,
        session_factory: async_sessionmaker | None = None,
    ) -> dict:
        """Run evaluation comparing deterministic-only vs AI-assisted matching.

        Produces a side-by-side comparison showing how AI matching improves
        (or changes) the F1 score on the ground truth dataset.

        Args:
            session: Async SQLAlchemy session for loading events/ground truth.
            matching_config: Full matching config (must have ai section configured).
            session_factory: Async session factory for AI cache/usage DB access.
                Required when AI is enabled.

        Returns:
            Dict with deterministic_metrics, ai_metrics, and improvement delta.
        """
        gt_same, gt_diff = await load_ground_truth(session)

        # Load all source events
        result = await session.execute(
            select(SourceEvent).options(selectinload(SourceEvent.dates))
        )
        source_events = result.scalars().all()

        events = []
        for evt in source_events:
            events.append(
                {
                    "id": evt.id,
                    "title": evt.title,
                    "title_normalized": evt.title_normalized,
                    "short_description": evt.short_description,
                    "short_description_normalized": evt.short_description_normalized,
                    "description": evt.description,
                    "highlights": evt.highlights,
                    "location_name": evt.location_name,
                    "location_city": evt.location_city,
                    "location_district": evt.location_district,
                    "location_street": evt.location_street,
                    "location_zipcode": evt.location_zipcode,
                    "geo_latitude": evt.geo_latitude,
                    "geo_longitude": evt.geo_longitude,
                    "geo_confidence": evt.geo_confidence,
                    "source_code": evt.source_code,
                    "source_type": evt.source_type,
                    "blocking_keys": evt.blocking_keys,
                    "categories": evt.categories,
                    "is_family_event": evt.is_family_event,
                    "is_child_focused": evt.is_child_focused,
                    "admission_free": evt.admission_free,
                    "dates": [
                        {
                            "date": str(d.date),
                            "start_time": str(d.start_time) if d.start_time else None,
                            "end_time": str(d.end_time) if d.end_time else None,
                            "end_date": str(d.end_date) if d.end_date else None,
                        }
                        for d in evt.dates
                    ],
                }
            )

        # --- Deterministic-only ---
        det_predicted = generate_predictions_multisignal(events, matching_config)
        det_metrics = compute_metrics(det_predicted, gt_same, gt_diff)

        # --- With AI assistance ---
        if matching_config.ai.enabled and session_factory is not None:
            from event_dedup.ai_matching.resolver import resolve_ambiguous_pairs
            from event_dedup.matching.pipeline import get_match_pairs, score_candidate_pairs

            match_result = score_candidate_pairs(events, matching_config)
            match_result = await resolve_ambiguous_pairs(
                match_result, events, matching_config.ai, session_factory,
            )
            ai_predicted = get_match_pairs(match_result)
        else:
            ai_predicted = det_predicted

        ai_metrics = compute_metrics(ai_predicted, gt_same, gt_diff)

        # Print comparison
        print("\n" + "=" * 80)
        print("  Deterministic vs AI-Assisted Matching Comparison")
        print("=" * 80)
        print(f"  {'Metric':>15s}  {'Deterministic':>15s}  {'AI-Assisted':>15s}  {'Delta':>10s}")
        print("-" * 80)
        for name, det_val, ai_val in [
            ("Precision", det_metrics.precision, ai_metrics.precision),
            ("Recall", det_metrics.recall, ai_metrics.recall),
            ("F1", det_metrics.f1, ai_metrics.f1),
            ("True Pos", det_metrics.true_positives, ai_metrics.true_positives),
            ("False Pos", det_metrics.false_positives, ai_metrics.false_positives),
            ("False Neg", det_metrics.false_negatives, ai_metrics.false_negatives),
        ]:
            if isinstance(det_val, int):
                delta = ai_val - det_val
                print(f"  {name:>15s}  {det_val:>15d}  {ai_val:>15d}  {delta:>+10d}")
            else:
                delta = ai_val - det_val
                print(f"  {name:>15s}  {det_val:>15.4f}  {ai_val:>15.4f}  {delta:>+10.4f}")
        print("=" * 80 + "\n")

        return {
            "deterministic": {
                "precision": det_metrics.precision,
                "recall": det_metrics.recall,
                "f1": det_metrics.f1,
                "true_positives": det_metrics.true_positives,
                "false_positives": det_metrics.false_positives,
                "false_negatives": det_metrics.false_negatives,
            },
            "ai_assisted": {
                "precision": ai_metrics.precision,
                "recall": ai_metrics.recall,
                "f1": ai_metrics.f1,
                "true_positives": ai_metrics.true_positives,
                "false_positives": ai_metrics.false_positives,
                "false_negatives": ai_metrics.false_negatives,
            },
            "f1_improvement": ai_metrics.f1 - det_metrics.f1,
        }
    ```

    **2. Create `tests/test_ai_resolver.py`** with tests using a mock Gemini client:

    ```python
    """Tests for AI matching resolver.

    Uses mocked Gemini client to test resolution logic without API calls.
    """
    from __future__ import annotations

    from unittest.mock import AsyncMock, patch

    import pytest

    from event_dedup.ai_matching.resolver import _apply_ai_result, resolve_ambiguous_pairs
    from event_dedup.ai_matching.schemas import AIMatchResult
    from event_dedup.matching.combiner import SignalScores
    from event_dedup.matching.config import AIMatchingConfig
    from event_dedup.matching.pipeline import MatchDecisionRecord, MatchResult
    from event_dedup.matching.candidate_pairs import CandidatePairStats


    # ---- Helpers ----

    def _make_signals(score: float = 0.5) -> SignalScores:
        return SignalScores(date=score, geo=score, title=score, description=score)


    def _make_decision(
        id_a: str, id_b: str, decision: str = "ambiguous", score: float = 0.5
    ) -> MatchDecisionRecord:
        return MatchDecisionRecord(
            event_id_a=id_a,
            event_id_b=id_b,
            signals=_make_signals(score),
            combined_score_value=score,
            decision=decision,
        )


    def _make_match_result(decisions: list[MatchDecisionRecord]) -> MatchResult:
        return MatchResult(
            decisions=decisions,
            pair_stats=CandidatePairStats(
                total_events=10, total_pairs=5, blocked_pairs=5, reduction_pct=50.0
            ),
            match_count=sum(1 for d in decisions if d.decision == "match"),
            ambiguous_count=sum(1 for d in decisions if d.decision == "ambiguous"),
            no_match_count=sum(1 for d in decisions if d.decision == "no_match"),
        )


    def _make_event(id: str, title: str = "Event") -> dict:
        return {
            "id": id,
            "title": title,
            "description": "Description",
            "short_description": "Short",
            "source_code": "bwb",
            "source_type": "artikel",
            "location_name": "Marktplatz",
            "location_city": "Freiburg",
            "dates": [{"date": "2026-01-01"}],
            "categories": ["test"],
        }


    # ---- _apply_ai_result tests ----

    class TestApplyAIResult:
        def test_high_confidence_same(self):
            d = _make_decision("a", "b")
            r = _apply_ai_result(d, AIMatchResult(decision="same", confidence=0.9, reasoning="match"), 0.6)
            assert r.decision == "match"
            assert r.tier == "ai"

        def test_high_confidence_different(self):
            d = _make_decision("a", "b")
            r = _apply_ai_result(d, AIMatchResult(decision="different", confidence=0.8, reasoning="no match"), 0.6)
            assert r.decision == "no_match"
            assert r.tier == "ai"

        def test_low_confidence_stays_ambiguous(self):
            d = _make_decision("a", "b")
            r = _apply_ai_result(d, AIMatchResult(decision="same", confidence=0.3, reasoning="unsure"), 0.6)
            assert r.decision == "ambiguous"
            assert r.tier == "ai_low_confidence"

        def test_exactly_at_threshold(self):
            d = _make_decision("a", "b")
            r = _apply_ai_result(d, AIMatchResult(decision="same", confidence=0.6, reasoning="borderline"), 0.6)
            assert r.decision == "match"
            assert r.tier == "ai"

        def test_preserves_signal_scores(self):
            signals = SignalScores(date=0.9, geo=0.8, title=0.7, description=0.6)
            d = MatchDecisionRecord("a", "b", signals, 0.75, "ambiguous")
            r = _apply_ai_result(d, AIMatchResult(decision="same", confidence=0.9, reasoning="match"), 0.6)
            assert r.signals == signals
            assert r.combined_score_value == 0.75


    # ---- resolve_ambiguous_pairs tests ----

    class TestResolveAmbiguousPairs:
        @pytest.fixture
        def ai_config(self):
            return AIMatchingConfig(
                enabled=True,
                api_key="test-key",
                model="gemini-2.5-flash",
                confidence_threshold=0.6,
                cache_enabled=False,  # Disable cache for simpler testing
                max_concurrent_requests=5,
            )

        async def test_no_ambiguous_noop(self, test_session_factory, ai_config):
            """When no ambiguous pairs exist, result is returned unchanged."""
            decisions = [_make_decision("a", "b", "match")]
            match_result = _make_match_result(decisions)
            events = [_make_event("a"), _make_event("b")]

            result = await resolve_ambiguous_pairs(
                match_result, events, ai_config, test_session_factory
            )
            assert result.match_count == 1
            assert result.ambiguous_count == 0

        @patch("event_dedup.ai_matching.resolver.call_gemini")
        @patch("event_dedup.ai_matching.resolver.create_client")
        async def test_resolves_ambiguous_to_match(
            self, mock_create, mock_call, test_session_factory, ai_config
        ):
            """Ambiguous pair resolved to match by AI."""
            mock_create.return_value = AsyncMock()
            mock_call.return_value = (
                AIMatchResult(decision="same", confidence=0.9, reasoning="Same event"),
                800, 100,
            )

            decisions = [
                _make_decision("a", "b", "match"),
                _make_decision("c", "d", "ambiguous"),
            ]
            match_result = _make_match_result(decisions)
            events = [_make_event("a"), _make_event("b"), _make_event("c"), _make_event("d")]

            result = await resolve_ambiguous_pairs(
                match_result, events, ai_config, test_session_factory
            )
            assert result.match_count == 2  # Original match + AI match
            assert result.ambiguous_count == 0
            assert mock_call.call_count == 1

        @patch("event_dedup.ai_matching.resolver.call_gemini")
        @patch("event_dedup.ai_matching.resolver.create_client")
        async def test_resolves_ambiguous_to_no_match(
            self, mock_create, mock_call, test_session_factory, ai_config
        ):
            """Ambiguous pair resolved to no_match by AI."""
            mock_create.return_value = AsyncMock()
            mock_call.return_value = (
                AIMatchResult(decision="different", confidence=0.85, reasoning="Different events"),
                800, 100,
            )

            decisions = [_make_decision("a", "b", "ambiguous")]
            match_result = _make_match_result(decisions)
            events = [_make_event("a"), _make_event("b")]

            result = await resolve_ambiguous_pairs(
                match_result, events, ai_config, test_session_factory
            )
            assert result.match_count == 0
            assert result.no_match_count == 1
            assert result.ambiguous_count == 0

        @patch("event_dedup.ai_matching.resolver.call_gemini")
        @patch("event_dedup.ai_matching.resolver.create_client")
        async def test_low_confidence_stays_ambiguous(
            self, mock_create, mock_call, test_session_factory, ai_config
        ):
            """Low-confidence AI result keeps pair as ambiguous."""
            mock_create.return_value = AsyncMock()
            mock_call.return_value = (
                AIMatchResult(decision="same", confidence=0.3, reasoning="Unsure"),
                800, 100,
            )

            decisions = [_make_decision("a", "b", "ambiguous")]
            match_result = _make_match_result(decisions)
            events = [_make_event("a"), _make_event("b")]

            result = await resolve_ambiguous_pairs(
                match_result, events, ai_config, test_session_factory
            )
            assert result.ambiguous_count == 1
            assert result.match_count == 0

        @patch("event_dedup.ai_matching.resolver.call_gemini")
        @patch("event_dedup.ai_matching.resolver.create_client")
        async def test_api_failure_keeps_ambiguous(
            self, mock_create, mock_call, test_session_factory, ai_config
        ):
            """API failure keeps pair as ambiguous without crashing."""
            mock_create.return_value = AsyncMock()
            mock_call.side_effect = Exception("API rate limit exceeded")

            decisions = [_make_decision("a", "b", "ambiguous")]
            match_result = _make_match_result(decisions)
            events = [_make_event("a"), _make_event("b")]

            result = await resolve_ambiguous_pairs(
                match_result, events, ai_config, test_session_factory
            )
            assert result.ambiguous_count == 1
            assert result.decisions[0].decision == "ambiguous"

        @patch("event_dedup.ai_matching.resolver.call_gemini")
        @patch("event_dedup.ai_matching.resolver.create_client")
        async def test_non_ambiguous_unchanged(
            self, mock_create, mock_call, test_session_factory, ai_config
        ):
            """Match and no_match decisions are never sent to AI."""
            mock_create.return_value = AsyncMock()
            mock_call.return_value = (
                AIMatchResult(decision="same", confidence=0.9, reasoning="Same"),
                800, 100,
            )

            decisions = [
                _make_decision("a", "b", "match"),
                _make_decision("c", "d", "no_match"),
                _make_decision("e", "f", "ambiguous"),
            ]
            match_result = _make_match_result(decisions)
            events = [_make_event(id) for id in ["a", "b", "c", "d", "e", "f"]]

            result = await resolve_ambiguous_pairs(
                match_result, events, ai_config, test_session_factory
            )

            # Only the ambiguous pair was sent to AI
            assert mock_call.call_count == 1
            # Original match and no_match preserved
            assert result.match_count == 2  # original + AI resolved
            assert result.no_match_count == 1

        @patch("event_dedup.ai_matching.resolver.call_gemini")
        @patch("event_dedup.ai_matching.resolver.create_client")
        async def test_cache_hit_skips_api(
            self, mock_create, mock_call, test_session_factory
        ):
            """Cache hits avoid API calls."""
            ai_config = AIMatchingConfig(
                enabled=True, api_key="test-key", model="gemini-2.5-flash",
                confidence_threshold=0.6, cache_enabled=True,
            )

            # Pre-populate cache
            from event_dedup.ai_matching.cache import compute_pair_hash, store_cache
            from event_dedup.ai_matching.schemas import AIMatchResult as AIR

            evt_a = _make_event("a", "Event A")
            evt_b = _make_event("b", "Event B")
            pair_hash = compute_pair_hash(evt_a, evt_b)
            await store_cache(
                test_session_factory, pair_hash, "a", "b",
                AIR(decision="same", confidence=0.9, reasoning="Cached match"),
                "gemini-2.5-flash",
            )

            mock_create.return_value = AsyncMock()
            decisions = [_make_decision("a", "b", "ambiguous")]
            match_result = _make_match_result(decisions)

            result = await resolve_ambiguous_pairs(
                match_result, [evt_a, evt_b], ai_config, test_session_factory
            )

            # API was never called
            assert mock_call.call_count == 0
            assert result.match_count == 1
    ```

    **3. Update `tests/test_orchestrator.py`** to verify AI integration.

    Add two new tests at the end of the file. These follow the existing test patterns: they use the `processor` and `test_session_factory` fixtures, create JSON test files with `_write_json_file`, and verify orchestrator behavior.

    Add these imports at the top of the file (alongside existing imports):
    ```python
    from unittest.mock import AsyncMock, patch
    from event_dedup.matching.config import AIMatchingConfig
    ```

    Add these tests at the end of the file:

    ```python
    # ---- Tests for AI integration ----


    @patch("event_dedup.worker.orchestrator.resolve_ambiguous_pairs")
    async def test_process_new_file_calls_ai_when_enabled(
        mock_resolve_ai: AsyncMock,
        processor: FileProcessor,
        test_session_factory: async_sessionmaker[AsyncSession],
        tmp_path: Path,
    ) -> None:
        """When AI is enabled and API key is set, resolve_ambiguous_pairs is called."""
        # Configure AI matching
        matching_config = MatchingConfig(
            ai=AIMatchingConfig(enabled=True, api_key="test-key-123"),
        )

        # Mock resolve_ambiguous_pairs to return its input unchanged
        # (it receives a MatchResult and should return a MatchResult)
        async def passthrough(match_result, events, ai_config, session_factory):
            return match_result

        mock_resolve_ai.side_effect = passthrough

        # Create a JSON file with one event
        events = [_make_event("pdf-ai-test-0-0", "AI Test Event")]
        file_path = _write_json_file(tmp_path, "srcAI_test.json", events)

        stats = await process_new_file(
            file_path, processor, test_session_factory, matching_config
        )

        assert stats["status"] == "completed"
        # resolve_ambiguous_pairs was called exactly once
        assert mock_resolve_ai.call_count == 1
        # Verify it was called with the right AI config
        call_args = mock_resolve_ai.call_args
        assert call_args[1].get("ai_config", call_args[0][2] if len(call_args[0]) > 2 else None) is not None or len(call_args[0]) >= 3


    @patch("event_dedup.worker.orchestrator.resolve_ambiguous_pairs")
    async def test_process_new_file_skips_ai_when_disabled(
        mock_resolve_ai: AsyncMock,
        processor: FileProcessor,
        test_session_factory: async_sessionmaker[AsyncSession],
        matching_config: MatchingConfig,
        tmp_path: Path,
    ) -> None:
        """When AI is disabled (default), resolve_ambiguous_pairs is never called."""
        # matching_config fixture uses defaults: ai.enabled=False
        assert matching_config.ai.enabled is False

        events = [_make_event("pdf-noai-test-0-0", "No AI Test Event")]
        file_path = _write_json_file(tmp_path, "srcNoAI_test.json", events)

        stats = await process_new_file(
            file_path, processor, test_session_factory, matching_config
        )

        assert stats["status"] == "completed"
        # resolve_ambiguous_pairs was never called
        assert mock_resolve_ai.call_count == 0
    ```

    NOTE: The mock patches `event_dedup.worker.orchestrator.resolve_ambiguous_pairs` (where it is imported in the orchestrator module) rather than `event_dedup.ai_matching.resolver.resolve_ambiguous_pairs` (where it is defined). This follows the standard Python mock patching rule: patch where the name is looked up, not where it is defined. The `_maybe_resolve_ai` helper calls `resolve_ambiguous_pairs` which is imported at module level in orchestrator.py.
  </action>
  <verify>
    <automated>cd /Users/svenkarl/workspaces/event-deduplication && uv run pytest tests/test_ai_resolver.py tests/test_ai_matching.py -x -v 2>&1 | tail -40</automated>
  </verify>
  <done>
    - Evaluation harness has run_ai_comparison_evaluation for deterministic vs AI-assisted F1 comparison
    - Comparison prints side-by-side precision/recall/F1/TP/FP/FN table
    - Resolver tests cover: no ambiguous noop, resolve to match, resolve to no_match, low confidence stays ambiguous, API failure stays ambiguous, non-ambiguous unchanged, cache hit skips API
    - Orchestrator tests verify AI is called when enabled (mock_resolve_ai.call_count == 1) and skipped when disabled (call_count == 0)
    - All new tests pass
    - No regressions in existing test suite
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/test_ai_resolver.py -x -v` -- all resolver tests pass with mocked Gemini
2. `uv run pytest tests/test_ai_matching.py -x -v` -- all infrastructure tests pass
3. `uv run pytest tests/test_orchestrator.py -x -v` -- orchestrator tests pass including AI integration
4. `uv run python -c "from event_dedup.ai_matching.resolver import resolve_ambiguous_pairs; print('resolver OK')"` -- resolver importable
5. `uv run python -c "from event_dedup.matching.pipeline import rebuild_pipeline_result; print('rebuild OK')"` -- rebuild function importable
6. `uv run python -c "from event_dedup.evaluation.harness import run_ai_comparison_evaluation; print('eval OK')"` -- evaluation harness updated
7. `uv run pytest tests/ -x --timeout=60` -- full test suite passes (no regressions)
8. `grep GEMINI_API_KEY docker-compose.yml` -- env var configured for worker container
</verification>

<success_criteria>
- resolve_ambiguous_pairs orchestrates cache -> API -> update for each ambiguous pair
- Confidence threshold (0.6) determines whether AI decision overrides ambiguous
- Pipeline works identically when AI is disabled (ai.enabled=False default)
- rebuild_pipeline_result is a public function in pipeline.py (no private function imports in orchestrator)
- Worker orchestrator uses _maybe_resolve_ai helper called by both process_new_file and process_file_batch
- process_existing_files inherits AI behavior via delegation to process_new_file
- Re-clustering happens only when ambiguous pairs are actually resolved
- Evaluation harness can compare deterministic-only vs AI-assisted F1 scores
- GEMINI_API_KEY env var flows from host -> docker-compose -> worker -> matching config
- All tests pass including resolver, infrastructure, and orchestrator tests
- No regressions in existing test suite
</success_criteria>

<output>
After completion, create `.planning/phases/05/05-02-SUMMARY.md`
</output>
