---
phase: 01-foundation
plan: 04
type: execute
wave: 4
depends_on: ["01-03"]
files_modified:
  - src/event_dedup/ground_truth/auto_labeler.py
  - scripts/generate_ground_truth.py
  - tests/test_auto_labeler.py
autonomous: true
requirements:
  - EVAL-01
  - EVAL-02

must_haves:
  truths:
    - "Auto-labeler produces 200+ labeled pairs using conservative multi-signal heuristics without manual intervention"
    - "Auto-labeled 'same' pairs use stricter criteria than the matching algorithm (title_sim >= 0.90 + same city, or title_sim >= 0.70 + same city + desc_sim >= 0.80)"
    - "Auto-labeled 'different' pairs are clearly unrelated events (title_sim < 0.40, or different city + title_sim < 0.70)"
    - "Ambiguous pairs (0.40-0.90 with mixed signals) are excluded from ground truth to avoid unreliable labels"
    - "Ground truth dataset is persisted to SQLite database and loadable by the evaluation harness"
    - "Generation script produces distribution stats showing label counts, similarity ranges, and confidence breakdown"
  artifacts:
    - path: "src/event_dedup/ground_truth/auto_labeler.py"
      provides: "Conservative multi-signal auto-labeling heuristics"
      exports: ["auto_label_candidates", "AutoLabelResult", "LabelDecision"]
    - path: "scripts/generate_ground_truth.py"
      provides: "End-to-end ground truth generation script"
    - path: "tests/test_auto_labeler.py"
      provides: "Tests for auto-labeling heuristics"
  key_links:
    - from: "src/event_dedup/ground_truth/auto_labeler.py"
      to: "src/event_dedup/ground_truth/candidate_generator.py"
      via: "Takes candidate pairs as input, adds multi-signal scoring"
      pattern: "CandidatePair"
    - from: "src/event_dedup/ground_truth/auto_labeler.py"
      to: "src/event_dedup/models/ground_truth.py"
      via: "Creates GroundTruthPair records"
      pattern: "GroundTruthPair"
    - from: "scripts/generate_ground_truth.py"
      to: "src/event_dedup/ground_truth/auto_labeler.py"
      via: "Orchestrates ingestion, candidate generation, and auto-labeling"
      pattern: "auto_label_candidates"
---

<objective>
Auto-generate a ground truth dataset of 200+ labeled event pairs using conservative multi-signal heuristics, eliminating the need for manual labeling.

Purpose: Phase 1 SC-3 requires a labeled ground truth dataset for evaluation. The original plan used a manual CLI labeling tool, but manual labeling contradicts the project's automation goal. Auto-labeling with stricter-than-matching heuristics produces a reliable ground truth while keeping the process fully automated.

Output: A SQLite database containing 200+ labeled pairs (same/different) from the 765-event sample, with stats showing the labeling distribution. The evaluation harness can immediately use this for Phase 2 matching accuracy measurement.
</objective>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01/01-03-SUMMARY.md

Signal analysis from 765-event dataset (1014 candidate pairs):
- HIGH (0.95-1.0): 205 pairs, 203 same city — nearly all genuine duplicates
- GOOD (0.85-0.95): 33 pairs, 32 same city — most are duplicates with slight title variation
- MEDIUM (0.60-0.85): 82 pairs — mixed, includes both duplicates and false positives (e.g., different kindergartens at same city)
- LOW (0.35-0.60): 362 pairs — mostly different events, some edge cases
- VERY LOW (<0.35): 656 pairs — clearly different events
</context>

<tasks>

<task type="auto">
  <name>Task 1: Auto-labeler module and tests</name>
  <files>
    src/event_dedup/ground_truth/auto_labeler.py
    tests/test_auto_labeler.py
  </files>
  <action>
    Build auto-labeler with conservative multi-signal heuristics. The key principle:
    auto-labeling criteria must be STRICTER than the matching algorithm to ensure ground truth reliability.

    **Auto "same" rules (all require different source_code):**
    1. title_sim >= 0.90 AND same normalized city → "same" (high confidence)
    2. title_sim >= 0.70 AND same normalized city AND desc_sim >= 0.80 → "same" (multi-signal confirmed)

    **Auto "different" rules:**
    1. title_sim < 0.40 → "different" (clearly unrelated)
    2. different normalized city AND title_sim < 0.70 → "different"

    **Ambiguous (excluded from ground truth):**
    - Everything not matching above rules is skipped — better to have fewer but reliable labels
  </action>
</task>

<task type="auto">
  <name>Task 2: Generation script and execution</name>
  <files>
    scripts/generate_ground_truth.py
  </files>
  <action>
    Create script that ingests all 765 events, generates candidates, runs auto-labeler,
    persists results to SQLite, and prints distribution stats.
  </action>
</task>

</tasks>

<success_criteria>
- Auto-labeler produces 200+ labeled pairs without manual intervention
- All "same" labels are high-confidence (multi-signal agreement)
- All "different" labels are clearly unrelated events
- Ambiguous pairs are excluded rather than guessed
- Ground truth is loadable by the existing evaluation harness
- All tests pass
</success_criteria>
