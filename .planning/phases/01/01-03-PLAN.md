---
phase: 01-foundation
plan: 03
type: execute
wave: 3
depends_on: ["01-01", "01-02"]
files_modified:
  - src/event_dedup/models/ground_truth.py
  - src/event_dedup/models/__init__.py
  - src/event_dedup/ground_truth/__init__.py
  - src/event_dedup/ground_truth/candidate_generator.py
  - src/event_dedup/ground_truth/labeling_tool.py
  - src/event_dedup/evaluation/__init__.py
  - src/event_dedup/evaluation/metrics.py
  - src/event_dedup/evaluation/harness.py
  - config/alembic/versions/ground_truth_migration.py
  - tests/test_candidate_generator.py
  - tests/test_metrics.py
  - tests/test_harness.py
autonomous: true
requirements:
  - EVAL-01
  - EVAL-02
  - EVAL-03

must_haves:
  truths:
    - "Candidate pair generator produces cross-source event pairs grouped by date+city, scored by title similarity"
    - "CLI labeling tool presents event pairs side-by-side with title, short_description, location, dates and allows same/different labeling"
    - "Pairs with title similarity >= 0.85 are auto-suggested as 'same' for quick confirmation"
    - "Ground truth labels are stored in ground_truth_pairs table with event IDs, label, notes, and timestamp"
    - "Evaluation harness compares any set of predicted pairs against ground truth and reports precision, recall, F1"
    - "Evaluation harness accepts configurable threshold parameters so different configurations can be tested"
  artifacts:
    - path: "src/event_dedup/models/ground_truth.py"
      provides: "SQLAlchemy model for ground_truth_pairs table"
      exports: ["GroundTruthPair"]
    - path: "src/event_dedup/ground_truth/candidate_generator.py"
      provides: "Candidate pair generation for ground truth labeling"
      exports: ["generate_candidates", "CandidatePair"]
    - path: "src/event_dedup/ground_truth/labeling_tool.py"
      provides: "CLI tool for labeling event pairs"
      exports: ["run_labeling_session"]
    - path: "src/event_dedup/evaluation/metrics.py"
      provides: "Precision, recall, F1 calculation"
      exports: ["compute_metrics", "MetricsResult"]
    - path: "src/event_dedup/evaluation/harness.py"
      provides: "Evaluation harness orchestration"
      exports: ["run_evaluation", "EvaluationResult"]
  key_links:
    - from: "src/event_dedup/ground_truth/candidate_generator.py"
      to: "src/event_dedup/models/source_event.py"
      via: "Queries source_events with blocking_keys for grouping"
      pattern: "blocking_keys|SourceEvent"
    - from: "src/event_dedup/ground_truth/candidate_generator.py"
      to: "rapidfuzz"
      via: "Title similarity scoring for candidate ranking"
      pattern: "fuzz\\.ratio|fuzz\\.token_sort_ratio"
    - from: "src/event_dedup/evaluation/harness.py"
      to: "src/event_dedup/models/ground_truth.py"
      via: "Loads ground truth labels for comparison"
      pattern: "GroundTruthPair"
    - from: "src/event_dedup/evaluation/metrics.py"
      to: "sklearn"
      via: "Uses sklearn for precision/recall/F1 calculation"
      pattern: "precision_recall_fscore_support"
---

<objective>
Create the ground truth dataset infrastructure (candidate generator, labeling tool, database table) and the evaluation harness that measures deduplication accuracy against labeled pairs.

Purpose: Without ground truth, all matching work in Phase 2+ is guesswork. The evaluation harness is the objective yardstick for measuring whether changes improve or regress accuracy. This must exist before any matching algorithm is implemented.

Output: A ground_truth_pairs table, a CLI tool that generates candidate pairs and lets the user label them, and an evaluation harness that reports precision/recall/F1 for any matching configuration.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01/01-RESEARCH.md
@.planning/1-CONTEXT.md
@.planning/phases/01/01-01-SUMMARY.md
@.planning/phases/01/01-02-SUMMARY.md

<interfaces>
<!-- Interfaces from Plans 01-01 and 01-02 -->

From src/event_dedup/models/source_event.py:
```python
class SourceEvent(Base):
    __tablename__ = "source_events"
    id: Mapped[str]  # pdf-{hash}-{batch}-{index}
    title: Mapped[str]
    short_description: Mapped[str | None]
    description: Mapped[str | None]
    location_name: Mapped[str | None]
    location_city: Mapped[str | None]
    location_district: Mapped[str | None]
    title_normalized: Mapped[str | None]
    short_description_normalized: Mapped[str | None]
    location_name_normalized: Mapped[str | None]
    location_city_normalized: Mapped[str | None]
    blocking_keys: Mapped[list[str] | None]
    geo_latitude: Mapped[float | None]
    geo_longitude: Mapped[float | None]
    source_type: Mapped[str]
    source_code: Mapped[str]
    categories: Mapped[list | None]
    dates: Mapped[list["EventDate"]]
```

From src/event_dedup/models/event_date.py:
```python
class EventDate(Base):
    __tablename__ = "event_dates"
    event_id: Mapped[str]
    date: Mapped[date]
    start_time: Mapped[time | None]
```

From src/event_dedup/preprocessing/normalizer.py:
```python
def normalize_text(text: str) -> str
def normalize_city(city: str, aliases: dict[str, str] | None = None) -> str
```

From src/event_dedup/db/session.py:
```python
async def get_session() -> AsyncGenerator[AsyncSession, None]
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Ground truth model, candidate generator, and labeling tool</name>
  <files>
    src/event_dedup/models/ground_truth.py
    src/event_dedup/models/__init__.py
    src/event_dedup/ground_truth/__init__.py
    src/event_dedup/ground_truth/candidate_generator.py
    src/event_dedup/ground_truth/labeling_tool.py
    tests/test_candidate_generator.py
  </files>
  <action>
    **1. Create `src/event_dedup/models/ground_truth.py`:**

    - `class GroundTruthPair(Base)`:
      - Table: `ground_truth_pairs`
      - `id: Mapped[int]` autoincrement PK
      - `event_id_a: Mapped[str]` -- FK to source_events.id
      - `event_id_b: Mapped[str]` -- FK to source_events.id
      - `label: Mapped[str]` -- "same" or "different"
      - `notes: Mapped[str | None]` -- optional reviewer notes
      - `title_similarity: Mapped[float | None]` -- similarity score at time of labeling (for reference)
      - `labeled_at: Mapped[datetime]` -- server_default=func.now()
      - Unique constraint on `(event_id_a, event_id_b)` to prevent duplicate labels
      - CHECK constraint: `event_id_a < event_id_b` to enforce canonical ordering (alphabetically smaller ID always first)
      - CHECK constraint: `label IN ('same', 'different')`

    - Update `src/event_dedup/models/__init__.py` to export GroundTruthPair.

    - Generate Alembic migration: `alembic revision --autogenerate -m "add ground_truth_pairs table"`

    **2. Create `src/event_dedup/ground_truth/candidate_generator.py`:**

    - `@dataclass CandidatePair`: `event_id_a: str`, `event_id_b: str`, `title_sim: float`, `event_a_title: str`, `event_b_title: str`, `event_a_city: str | None`, `event_b_city: str | None`, `event_a_source: str`, `event_b_source: str`

    - `async def generate_candidates(session: AsyncSession, min_title_sim: float = 0.30) -> list[CandidatePair]`:
      1. Query all SourceEvents with their blocking_keys, title_normalized, location_city, source_code, and original titles
      2. Build a blocking index: `dict[str, list[SourceEvent]]` -- for each blocking key that starts with "dc|" (date+city), collect all events that have that key
      3. For each blocking group:
         - Generate all cross-source pairs (where source_code differs)
         - For each pair, compute title similarity using `rapidfuzz.fuzz.token_sort_ratio(title_normalized_a, title_normalized_b) / 100.0`
         - Use `token_sort_ratio` (not plain `ratio`) because word order varies between sources ("Nachtumzug Denzlinger Fasnet" vs "Nachtumzug Fasnet Denzlingen")
      4. Deduplicate pairs by canonical (event_id_a, event_id_b) ordering -- if event_id_a > event_id_b, swap them
      5. Filter: keep pairs where title_sim >= min_title_sim
      6. Also include a random ~20% sample of pairs with title_sim < min_title_sim as hard negatives (use random seed for reproducibility)
      7. Sort by title_sim descending
      8. Return list of CandidatePair

    - Also generate candidates using geo-grid blocking ("dg|" keys) for events where city-based blocking might miss duplicates (different city names). Merge with date+city candidates, deduplicate.

    **3. Create `src/event_dedup/ground_truth/labeling_tool.py`:**

    - `async def run_labeling_session(session: AsyncSession, candidates: list[CandidatePair], auto_threshold: float = 0.85) -> int`:
      - Interactive CLI tool for labeling pairs
      - For each candidate pair:
        1. Print separator line
        2. Print pair number and total remaining: `[42/326] Title similarity: 0.87`
        3. Print side-by-side comparison:
           ```
           === Event A (bwb) ===                    === Event B (emt) ===
           Title: Primel-Aktion Emmendingen          Title: Primel-Aktion Emmendingen
           Short: Gewerbetreibende verschenken...     Short: Primel-Aktion der AGL und GVE...
           City:  Emmendingen                         City:  Emmendingen
           Dates: 2026-02-13, 2026-02-14             Dates: 2026-02-13, 2026-02-14
           Location: Emmendingen Innenstadt           Location: Innenstadt Emmendingen
           Type:  artikel                             Type:  artikel
           ```
        4. If title_sim >= auto_threshold: pre-suggest "same" with `[S]ame / [D]ifferent / [s]kip / [q]uit (auto-suggest: Same):`
        5. If title_sim < auto_threshold: prompt `[S]ame / [D]ifferent / [s]kip / [q]uit:`
        6. Accept input: 's' or Enter (for auto-suggested) = same, 'd' = different, 'k' = skip (don't label), 'q' = quit session, 'n' = add note before confirming
        7. On label: create GroundTruthPair record with canonical ID ordering, label, title_similarity, notes
        8. Commit each label immediately (so progress is saved if user quits)
        9. Skip pairs that already have a label in ground_truth_pairs (resume support)
      - Return count of pairs labeled in this session

    - `async def get_labeling_stats(session: AsyncSession) -> dict`:
      Query ground_truth_pairs, return: total_labeled, same_count, different_count, remaining (if candidates list provided)

    **4. Create `tests/test_candidate_generator.py`:**

    Tests use synthetic data (not real DB) to test the logic:

    - `test_cross_source_pairs_only`: Create events from same source, verify no pairs generated
    - `test_title_similarity_filter`: Create events with known titles, verify pairs below min_title_sim are filtered
    - `test_pair_deduplication`: Create events that share multiple blocking keys (multi-date), verify each pair appears only once
    - `test_canonical_id_ordering`: Verify event_id_a < event_id_b in all generated pairs
    - `test_geo_blocking_candidates`: Create events with same geo grid but different cities, verify they are candidates
    - `test_hard_negative_sampling`: With min_title_sim=0.30, verify some pairs below 0.30 are included (hard negatives)

    For testing without a real database, either:
    a) Extract the core grouping/pairing logic into a pure function that takes a list of dicts (not SQLAlchemy objects), then the async DB function calls this pure function, OR
    b) Use the SQLite in-memory fixtures from conftest.py

    Prefer option (a) for faster, simpler tests.
  </action>
  <verify>
    <automated>cd /Users/svenkarl/workspaces/event-deduplication && uv run pytest tests/test_candidate_generator.py -x -v</automated>
  </verify>
  <done>
    - GroundTruthPair model exists with event_id_a, event_id_b, label, notes, title_similarity, labeled_at
    - Unique constraint prevents duplicate labels for the same pair
    - Candidate generator groups events by blocking keys, generates cross-source pairs, scores by title similarity
    - Candidate generator deduplicates pairs across multiple blocking key matches
    - Labeling tool presents pairs side-by-side with auto-suggest for high-similarity pairs
    - Labeling tool supports resume (skips already-labeled pairs)
    - All candidate generator tests pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Evaluation harness with precision, recall, F1 metrics</name>
  <files>
    src/event_dedup/evaluation/__init__.py
    src/event_dedup/evaluation/metrics.py
    src/event_dedup/evaluation/harness.py
    tests/test_metrics.py
    tests/test_harness.py
  </files>
  <action>
    **1. Create `src/event_dedup/evaluation/metrics.py`:**

    - `@dataclass MetricsResult`: `precision: float`, `recall: float`, `f1: float`, `true_positives: int`, `false_positives: int`, `false_negatives: int`, `true_negatives: int`, `total_ground_truth_same: int`, `total_ground_truth_different: int`, `total_predicted_same: int`

    - `def compute_metrics(predicted_same: set[tuple[str, str]], ground_truth_same: set[tuple[str, str]], ground_truth_different: set[tuple[str, str]]) -> MetricsResult`:
      1. Normalize all pairs to canonical ordering (event_id_a < event_id_b) -- sort each tuple
      2. True positives: pairs in both predicted_same AND ground_truth_same
      3. False positives: pairs in predicted_same but in ground_truth_different (predicted same, actually different)
      4. False negatives: pairs in ground_truth_same but NOT in predicted_same (missed duplicates)
      5. True negatives: pairs in ground_truth_different and NOT in predicted_same
      6. Calculate precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0
      7. Calculate recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0
      8. Calculate F1 = 2 * P * R / (P + R) if (P + R) > 0 else 0.0
      9. Return MetricsResult with all values

      NOTE: Do NOT use sklearn for this -- the pair-based evaluation is simple enough that a custom implementation is clearer and avoids the sklearn dependency just for this. The research suggested sklearn, but for pair-based binary classification with explicit TP/FP/FN counting, a simple implementation is more appropriate and testable.

    - `def format_metrics(result: MetricsResult) -> str`:
      Returns a formatted string:
      ```
      Precision: 0.92 (TP=46, FP=4)
      Recall:    0.85 (TP=46, FN=8)
      F1 Score:  0.88
      Ground truth: 54 same, 200 different
      Predicted same: 50
      ```

    **2. Create `src/event_dedup/evaluation/harness.py`:**

    - `@dataclass EvaluationConfig`: `title_sim_threshold: float = 0.80` -- the threshold above which pairs are predicted as "same". This is a simple config for Phase 1; Phase 2 will add multi-signal scoring.

    - `@dataclass EvaluationResult`: `config: EvaluationConfig`, `metrics: MetricsResult`, `false_positive_pairs: list[tuple[str, str]]`, `false_negative_pairs: list[tuple[str, str]]`

    - `async def load_ground_truth(session: AsyncSession) -> tuple[set[tuple[str, str]], set[tuple[str, str]]]`:
      Query all GroundTruthPair records. Return two sets: `ground_truth_same` (pairs labeled "same") and `ground_truth_different` (pairs labeled "different"). All pairs in canonical (a < b) order.

    - `async def generate_predictions(session: AsyncSession, config: EvaluationConfig) -> set[tuple[str, str]]`:
      1. Query all SourceEvents with blocking_keys and title_normalized
      2. Group by shared blocking keys (same logic as candidate generator but using the threshold from config)
      3. For each cross-source pair in the same blocking group, compute title similarity
      4. If similarity >= config.title_sim_threshold, add to predicted_same set
      5. Return predicted_same (canonical pair ordering)

      This is intentionally simple for Phase 1 -- a title-similarity-only matcher. Phase 2 will replace this with multi-signal scoring. The harness structure stays the same.

    - `async def run_evaluation(session: AsyncSession, config: EvaluationConfig) -> EvaluationResult`:
      1. Load ground truth
      2. Generate predictions using config
      3. Compute metrics
      4. Identify false positive and false negative pairs for debugging
      5. Print formatted metrics
      6. Return EvaluationResult

    - `async def run_threshold_sweep(session: AsyncSession, thresholds: list[float] | None = None) -> list[EvaluationResult]`:
      Run evaluation for multiple thresholds (default: 0.50, 0.60, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95).
      Print a comparison table:
      ```
      Threshold | Precision | Recall |   F1   | Predicted Same
      ----------|-----------|--------|--------|---------------
         0.50   |   0.65    |  0.95  |  0.77  |      120
         0.60   |   0.72    |  0.91  |  0.80  |       98
         ...
      ```
      Return list of EvaluationResult for each threshold.

    **3. Create `tests/test_metrics.py`:**

    - `test_perfect_score`: All ground truth same pairs predicted as same, none of different predicted as same -> precision=1.0, recall=1.0, f1=1.0
    - `test_all_false_positives`: Predict everything as same including different pairs -> precision < 1.0
    - `test_all_false_negatives`: Predict nothing as same -> recall=0.0, f1=0.0
    - `test_mixed_results`: Known TP=3, FP=1, FN=2 -> precision=0.75, recall=0.6, f1=0.666...
    - `test_canonical_ordering`: Pairs (b, a) and (a, b) are treated as the same pair
    - `test_empty_ground_truth`: No ground truth pairs -> metrics are 0.0 (no division by zero)
    - `test_empty_predictions`: No predictions -> precision=0.0 (by convention), recall=0.0

    **4. Create `tests/test_harness.py`:**

    Tests for the harness logic using synthetic data:

    - `test_load_ground_truth_separates_labels`: Insert 3 "same" and 5 "different" GroundTruthPair records, verify load_ground_truth returns correct sets
    - `test_generate_predictions_respects_threshold`: Create events with known title similarities, run with different thresholds, verify correct pairs are predicted
    - `test_threshold_sweep_returns_multiple_results`: Run sweep, verify results list has one entry per threshold
    - `test_evaluation_identifies_false_positives`: Set up a scenario with a known false positive, verify it appears in EvaluationResult.false_positive_pairs
    - `test_evaluation_identifies_false_negatives`: Set up a scenario with a known false negative, verify it appears in EvaluationResult.false_negative_pairs

    For harness tests, extract the core prediction logic (grouping by blocking keys, computing similarity, applying threshold) into a pure function that can be tested with in-memory data. The async DB wrapper calls this pure function.

    Add `scikit-learn` to dev dependencies ONLY IF used (based on implementation choice in metrics.py -- if using custom implementation, do NOT add sklearn).
  </action>
  <verify>
    <automated>cd /Users/svenkarl/workspaces/event-deduplication && uv run pytest tests/test_metrics.py tests/test_harness.py -x -v</automated>
  </verify>
  <done>
    - MetricsResult correctly computes precision, recall, F1 from pair sets
    - compute_metrics handles edge cases (empty sets, zero division)
    - Evaluation harness loads ground truth from DB, generates predictions, computes metrics
    - Threshold sweep runs evaluation for multiple thresholds and prints comparison table
    - False positive and false negative pairs are identified for debugging
    - All evaluation tests pass
  </done>
</task>

</tasks>

<verification>
After both tasks complete:

1. `uv run pytest tests/ -x -v` -- ALL tests pass (including tests from Plans 01-01 and 01-02)
2. Verify ground truth model imports: `uv run python -c "from event_dedup.models import GroundTruthPair; print('GroundTruthPair OK')"`
3. Verify evaluation imports: `uv run python -c "from event_dedup.evaluation.metrics import compute_metrics, MetricsResult; from event_dedup.evaluation.harness import run_evaluation, EvaluationConfig; print('Evaluation imports OK')"`
4. Verify candidate generator imports: `uv run python -c "from event_dedup.ground_truth.candidate_generator import generate_candidates, CandidatePair; print('Candidate generator OK')"`
5. Verify metrics computation: `uv run python -c "from event_dedup.evaluation.metrics import compute_metrics; r = compute_metrics({('a','b'),('c','d')}, {('a','b')}, {('c','d')}); print(f'P={r.precision}, R={r.recall}, F1={r.f1}'); assert r.precision == 0.5; assert r.recall == 1.0"` -- precision=0.5, recall=1.0
</verification>

<success_criteria>
- ground_truth_pairs table has correct schema with unique constraint and canonical ID ordering
- Candidate generator produces cross-source pairs from blocking groups with title similarity scoring
- Candidate pairs are deduplicated (multi-date events don't create duplicate pairs)
- CLI labeling tool presents pairs side-by-side, supports auto-suggest, resume, and notes
- Evaluation harness computes precision, recall, F1 from predicted pairs vs ground truth
- Threshold sweep enables testing multiple configurations without code changes (EVAL-03)
- All tests pass, including all tests from previous plans
</success_criteria>

<output>
After completion, create `.planning/phases/01/01-03-SUMMARY.md`
</output>
