---
phase: 01-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - src/event_dedup/__init__.py
  - src/event_dedup/models/__init__.py
  - src/event_dedup/models/base.py
  - src/event_dedup/models/source_event.py
  - src/event_dedup/models/event_date.py
  - src/event_dedup/models/file_ingestion.py
  - src/event_dedup/db/__init__.py
  - src/event_dedup/db/engine.py
  - src/event_dedup/db/session.py
  - src/event_dedup/config/__init__.py
  - src/event_dedup/config/settings.py
  - src/event_dedup/ingestion/__init__.py
  - src/event_dedup/ingestion/json_loader.py
  - src/event_dedup/ingestion/file_processor.py
  - config/alembic.ini
  - config/alembic/env.py
  - config/alembic/script.py.mako
  - config/alembic/versions/.gitkeep
  - tests/__init__.py
  - tests/conftest.py
  - tests/test_json_loader.py
  - tests/test_file_processor.py
autonomous: true
requirements:
  - PIPE-02
  - PIPE-03
  - PIPE-04

must_haves:
  truths:
    - "A JSON event file can be loaded into PostgreSQL and all events appear in source_events table"
    - "Re-processing the same file is skipped (no duplicate records created)"
    - "If ingestion fails mid-file, no partial data is written (transaction rollback)"
    - "Failed files are moved to a dead letter directory with error details logged"
    - "All event fields are preserved: title, short_description, description, highlights, dates, location (including geo, district, _sanitizeResult.city), categories, flags, confidence_score"
  artifacts:
    - path: "pyproject.toml"
      provides: "Project configuration with all Phase 1 dependencies"
      contains: "sqlalchemy"
    - path: "src/event_dedup/models/source_event.py"
      provides: "SQLAlchemy model for source_events table"
      exports: ["SourceEvent"]
    - path: "src/event_dedup/models/event_date.py"
      provides: "SQLAlchemy model for event_dates table"
      exports: ["EventDate"]
    - path: "src/event_dedup/models/file_ingestion.py"
      provides: "SQLAlchemy model for file_ingestions table"
      exports: ["FileIngestion"]
    - path: "src/event_dedup/ingestion/file_processor.py"
      provides: "File processing orchestrator with idempotency and transaction safety"
      exports: ["FileProcessor"]
    - path: "src/event_dedup/ingestion/json_loader.py"
      provides: "JSON file parser and validator"
      exports: ["load_event_file", "EventFileData"]
  key_links:
    - from: "src/event_dedup/ingestion/file_processor.py"
      to: "src/event_dedup/models/file_ingestion.py"
      via: "SHA-256 hash check before processing"
      pattern: "file_hash.*sha256"
    - from: "src/event_dedup/ingestion/file_processor.py"
      to: "src/event_dedup/models/source_event.py"
      via: "bulk insert within transaction"
      pattern: "session\\.add_all|session\\.add"
    - from: "src/event_dedup/ingestion/json_loader.py"
      to: "src/event_dedup/ingestion/file_processor.py"
      via: "parsed EventFileData passed to processor"
      pattern: "EventFileData"
---

<objective>
Set up the project from scratch, define the database schema, and implement JSON file ingestion with idempotency and transaction safety.

Purpose: This is the foundation everything else builds on. Without a working database and reliable file ingestion, nothing else in the project can function. The ingestion pipeline must be correct from day one -- idempotency bugs and partial writes are the hardest to debug later.

Output: A working Python project that can load any of the 20 JSON event files into PostgreSQL, skip already-processed files, and roll back cleanly on failures.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01/01-RESEARCH.md
@.planning/1-CONTEXT.md

Sample event data for reference:
@eventdata/bwb_11.02.2026_2026-02-11T20-46-41-776Z.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Project scaffolding, database models, and Alembic migration</name>
  <files>
    pyproject.toml
    src/event_dedup/__init__.py
    src/event_dedup/models/__init__.py
    src/event_dedup/models/base.py
    src/event_dedup/models/source_event.py
    src/event_dedup/models/event_date.py
    src/event_dedup/models/file_ingestion.py
    src/event_dedup/db/__init__.py
    src/event_dedup/db/engine.py
    src/event_dedup/db/session.py
    src/event_dedup/config/__init__.py
    src/event_dedup/config/settings.py
    config/alembic.ini
    config/alembic/env.py
    config/alembic/script.py.mako
    config/alembic/versions/.gitkeep
  </files>
  <action>
    **1. Create `pyproject.toml` with uv project configuration:**
    - name: `event-dedup`, requires-python `>=3.12`
    - Dependencies: `sqlalchemy>=2.0`, `asyncpg>=0.30`, `alembic>=1.14`, `pydantic>=2.9`, `rapidfuzz>=3.10`, `pyyaml>=6.0`, `psycopg2-binary>=2.9` (for Alembic sync operations)
    - Dev dependencies: `pytest>=8.3`, `pytest-asyncio>=0.24`, `ruff>=0.9`
    - Set `[tool.pytest.ini_options]` with `asyncio_mode = "auto"` and `testpaths = ["tests"]`
    - Set `[tool.ruff]` with `target-version = "py312"`, `line-length = 120`
    - Package source: `src` layout (set `[tool.setuptools.packages.find]` where `["src"]`)

    **2. Create `src/event_dedup/config/settings.py`:**
    - Use Pydantic `BaseSettings` with `model_config = SettingsConfigDict(env_prefix="EVENT_DEDUP_")`
    - Fields: `database_url: str` (default: `postgresql+asyncpg://postgres:postgres@localhost:5432/event_dedup`), `database_url_sync: str` (default: `postgresql+psycopg2://postgres:postgres@localhost:5432/event_dedup`), `dead_letter_dir: Path` (default: `./dead_letters`), `event_data_dir: Path` (default: `./eventdata`)
    - Export a `get_settings()` function using `@lru_cache`

    **3. Create `src/event_dedup/db/engine.py`:**
    - `create_async_engine` from SQLAlchemy with the async database URL
    - `get_engine()` function returning a cached engine instance
    - Do NOT use `echo=True` by default (add `echo: bool = False` parameter)

    **4. Create `src/event_dedup/db/session.py`:**
    - `async_sessionmaker` bound to the engine
    - `get_session()` async context manager yielding an `AsyncSession`
    - The session should NOT autocommit -- caller controls commit/rollback

    **5. Create `src/event_dedup/models/base.py`:**
    - `DeclarativeBase` subclass `Base` with `metadata` naming convention for constraints:
      ```python
      convention = {
          "ix": "ix_%(column_0_label)s",
          "uq": "uq_%(table_name)s_%(column_0_name)s",
          "ck": "ck_%(table_name)s_%(constraint_name)s",
          "fk": "fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s",
          "pk": "pk_%(table_name)s",
      }
      ```

    **6. Create `src/event_dedup/models/source_event.py` -- SourceEvent model:**
    - Table: `source_events`
    - Primary key: `id: Mapped[str]` (the `pdf-{hash}-{batch}-{index}` natural key, VARCHAR)
    - `file_ingestion_id: Mapped[int]` -- FK to `file_ingestions.id`
    - Original fields (for display): `title: Mapped[str]`, `short_description: Mapped[str | None]`, `description: Mapped[str | None]`, `highlights: Mapped[list | None]` (use `JSONB`), `location_name: Mapped[str | None]`, `location_city: Mapped[str | None]`, `location_district: Mapped[str | None]`, `location_street: Mapped[str | None]`, `location_street_no: Mapped[str | None]`, `location_zipcode: Mapped[str | None]`
    - Normalized fields (for matching -- populated in Plan 01-02): `title_normalized: Mapped[str | None]`, `short_description_normalized: Mapped[str | None]`, `location_name_normalized: Mapped[str | None]`, `location_city_normalized: Mapped[str | None]`
    - Blocking keys (populated in Plan 01-02): `blocking_keys: Mapped[list[str] | None]` (use `ARRAY(String)`)
    - Geo fields: `geo_latitude: Mapped[float | None]`, `geo_longitude: Mapped[float | None]`, `geo_confidence: Mapped[float | None]`, `geo_country: Mapped[str | None]`
    - Event metadata: `source_type: Mapped[str]` (artikel/terminliste/anzeige), `source_code: Mapped[str]` (derived from filename: bwb, emt, etc.), `categories: Mapped[list | None]` (JSONB), `is_family_event: Mapped[bool | None]`, `is_child_focused: Mapped[bool | None]`, `admission_free: Mapped[bool | None]`, `registration_required: Mapped[bool | None]`, `registration_contact: Mapped[str | None]`, `confidence_score: Mapped[float | None]`
    - Extraction metadata (kept per CONTEXT.md decision): `batch_index: Mapped[int | None]`, `extracted_at: Mapped[str | None]`
    - Timestamps: `created_at: Mapped[datetime]` (server_default=func.now())
    - Relationships: `dates: Mapped[list["EventDate"]]` via back_populates, `file_ingestion: Mapped["FileIngestion"]` via back_populates
    - IMPORTANT: Use `_sanitizeResult.city` as the authoritative city value, NOT the top-level `city`. Fall back to top-level `city` only if `_sanitizeResult` is absent.

    **7. Create `src/event_dedup/models/event_date.py` -- EventDate model:**
    - Table: `event_dates`
    - `id: Mapped[int]` autoincrement PK
    - `event_id: Mapped[str]` FK to `source_events.id`, with `ondelete="CASCADE"`
    - `date: Mapped[date]` (Python date, NOT datetime)
    - `start_time: Mapped[time | None]` (Python time)
    - `end_time: Mapped[time | None]`
    - `end_date: Mapped[date | None]` (for date ranges like `{"date": "2026-08-21", "end_date": "2026-08-22"}`)
    - Composite index on `(event_id, date)` for efficient date-based lookups
    - Relationship: `event: Mapped["SourceEvent"]` via back_populates

    **8. Create `src/event_dedup/models/file_ingestion.py` -- FileIngestion model:**
    - Table: `file_ingestions`
    - `id: Mapped[int]` autoincrement PK
    - `filename: Mapped[str]` (original filename, e.g. `bwb_11.02.2026_2026-02-11T20-46-41-776Z.json`)
    - `file_hash: Mapped[str]` (SHA-256 hex digest) -- UNIQUE constraint
    - `source_code: Mapped[str]` (extracted from filename: part before first underscore)
    - `event_count: Mapped[int]` (number of events in the file)
    - `status: Mapped[str]` (enum: 'completed', 'failed') with default 'completed'
    - `error_message: Mapped[str | None]` (error details if status='failed')
    - `ingested_at: Mapped[datetime]` (server_default=func.now())
    - `file_metadata: Mapped[dict | None]` (JSONB -- store the JSON file's `metadata` object)
    - Unique index on `file_hash`
    - Relationship: `events: Mapped[list["SourceEvent"]]` via back_populates

    **9. Create `src/event_dedup/models/__init__.py`:**
    - Import and re-export `Base`, `SourceEvent`, `EventDate`, `FileIngestion`

    **10. Set up Alembic:**
    - Create `config/alembic.ini` pointing to `config/alembic` as script_location
    - Set `sqlalchemy.url` to `postgresql+psycopg2://postgres:postgres@localhost:5432/event_dedup` (sync driver for Alembic)
    - Create `config/alembic/env.py` that imports `Base.metadata` from `src/event_dedup/models`
    - Create `config/alembic/script.py.mako` (standard template)
    - Generate the initial migration with `alembic revision --autogenerate -m "initial schema"`
    - Do NOT run `alembic upgrade head` yet -- leave that for the verify step

    **11. Create empty `__init__.py` files** for all packages: `src/event_dedup/`, `src/event_dedup/models/`, `src/event_dedup/db/`, `src/event_dedup/config/`, `src/event_dedup/ingestion/`, `src/event_dedup/preprocessing/`, `src/event_dedup/evaluation/`, `src/event_dedup/ground_truth/`, `tests/`

    Run `uv sync` after creating pyproject.toml to install all dependencies.
  </action>
  <verify>
    <automated>
      cd /Users/svenkarl/workspaces/event-deduplication && uv run python -c "from event_dedup.models import Base, SourceEvent, EventDate, FileIngestion; print(f'Tables: {list(Base.metadata.tables.keys())}'); assert 'source_events' in Base.metadata.tables; assert 'event_dates' in Base.metadata.tables; assert 'file_ingestions' in Base.metadata.tables; print('All models import OK')"
    </automated>
  </verify>
  <done>
    - pyproject.toml exists with all Phase 1 dependencies
    - `uv sync` succeeds, all packages installed
    - SQLAlchemy models for source_events, event_dates, file_ingestions import without error
    - source_events has dual text columns (original + normalized), blocking_keys array, all event fields
    - event_dates has date, start_time, end_time, end_date columns
    - file_ingestions has file_hash unique constraint, status, error_message
    - Alembic initial migration file exists in config/alembic/versions/
    - Database engine and async session factory are configured
  </done>
</task>

<task type="auto">
  <name>Task 2: JSON loader and file processor with idempotency</name>
  <files>
    src/event_dedup/ingestion/json_loader.py
    src/event_dedup/ingestion/file_processor.py
    src/event_dedup/ingestion/__init__.py
    tests/conftest.py
    tests/test_json_loader.py
    tests/test_file_processor.py
  </files>
  <action>
    **1. Create `src/event_dedup/ingestion/json_loader.py`:**

    - Define Pydantic models for JSON validation:
      ```python
      class GeoData(BaseModel):
          longitude: float | None = None
          latitude: float | None = None
          confidence: float | None = None
          country: str | None = None

      class SanitizeResult(BaseModel):
          city: str | None = None
          district: str | None = None
          confidence: float | None = None
          # Allow extra fields (matchType, requiresReview)
          model_config = ConfigDict(extra="allow")

      class LocationData(BaseModel):
          name: str | None = None
          city: str | None = None
          district: str | None = None
          street: str | None = None
          street_no: str | None = None
          zipcode: str | None = None
          sanitize_result: SanitizeResult | None = Field(None, alias="_sanitizeResult")
          geo: GeoData | None = None
          # Allow extra fields (_sanitized)
          model_config = ConfigDict(extra="allow", populate_by_name=True)

      class EventDateData(BaseModel):
          date: str
          start_time: str | None = None
          end_time: str | None = None
          end_date: str | None = None

      class EventData(BaseModel):
          id: str
          title: str
          short_description: str | None = None
          description: str | None = None
          highlights: list[str] | None = None
          event_dates: list[EventDateData] = []
          location: LocationData | None = None
          source_type: str
          categories: list[str] | None = None
          is_family_event: bool | None = None
          is_child_focused: bool | None = None
          admission_free: bool | None = None
          registration_required: bool | None = None
          registration_contact: str | None = None
          confidence_score: float | None = None
          batch_index: int | None = Field(None, alias="_batch_index")
          extracted_at: str | None = Field(None, alias="_extracted_at")
          # Allow extra fields we explicitly ignore (_event_index, _sanitized, etc.)
          model_config = ConfigDict(extra="allow", populate_by_name=True)

      class FileMetadata(BaseModel):
          processedAt: str | None = None
          sourceKey: str | None = None
          # Allow all other metadata fields
          model_config = ConfigDict(extra="allow")

      class EventFileData(BaseModel):
          events: list[EventData]
          rejected: list = []
          metadata: FileMetadata | None = None
      ```

    - `load_event_file(file_path: Path) -> EventFileData`: Read JSON file, validate with Pydantic, return structured data. Raise `ValueError` with descriptive message if validation fails.

    - `compute_file_hash(file_path: Path) -> str`: Read file in 64KB chunks, compute SHA-256 hex digest.

    - `extract_source_code(filename: str) -> str`: Extract source code from filename. Pattern: everything before the first underscore. Example: `bwb_11.02.2026_2026-02-11T20-46-41-776Z.json` -> `bwb`.

    **2. Create `src/event_dedup/ingestion/file_processor.py`:**

    - `class FileProcessor`:
      - Constructor takes `async_sessionmaker` and `dead_letter_dir: Path`
      - `async def process_file(self, file_path: Path) -> FileProcessResult`:
        1. Compute file hash via `compute_file_hash(file_path)`
        2. Check if hash exists in `file_ingestions` table -- if yes, return `FileProcessResult(status="skipped", reason="already processed")`
        3. Parse JSON via `load_event_file(file_path)`
        4. Extract source_code from filename
        5. Begin a single database transaction (use `async with self.session_factory() as session, session.begin():`)
        6. Create `FileIngestion` record (status='completed')
        7. For each event in the parsed data:
           - Create `SourceEvent` with all fields mapped from `EventData`
           - CRITICAL: For `location_city`, use `event.location.sanitize_result.city` if available, fall back to `event.location.city`. Per CONTEXT.md decision: `_sanitizeResult.city` is the authoritative city field.
           - For geo fields, only populate if `event.location.geo` exists
           - Create `EventDate` records for each date entry, parsing date strings to Python `date`, time strings (`"HH:MM"`) to Python `time`
           - Set `source_code` from the filename extraction
           - Leave `title_normalized`, `short_description_normalized`, `location_name_normalized`, `location_city_normalized`, and `blocking_keys` as NULL -- Plan 01-02 populates these
        8. `session.add_all()` all models
        9. If ANY exception occurs: catch it, move file to `dead_letter_dir/{filename}`, create a `FileIngestion` record with status='failed' and error_message (in a NEW separate session/transaction), return error result
        10. Return `FileProcessResult(status="completed", event_count=N, file_hash=hash)`

      - `FileProcessResult` dataclass: `status: str`, `event_count: int = 0`, `file_hash: str = ""`, `reason: str = ""`

    - Time parsing helper: `parse_time(time_str: str | None) -> time | None` -- handles `"HH:MM"` format (e.g., `"10:11"`, `"19:00"`), returns None for None/empty input. Some times have seconds like `"09:15"` -- handle both `HH:MM` and `HH:MM:SS`.

    - Date parsing helper: `parse_date(date_str: str) -> date` -- parses `"YYYY-MM-DD"` format.

    **3. Create `tests/conftest.py`:**
    - Fixture `sample_event_file` returning the path to `eventdata/bwb_11.02.2026_2026-02-11T20-46-41-776Z.json`
    - Fixture `tmp_dead_letter_dir` using pytest's `tmp_path`
    - Fixture `sample_event_json` returning a minimal valid event JSON dict for unit tests (not requiring a real file)
    - Use SQLite in-memory for model tests where DB is needed: create `test_engine` fixture with `create_async_engine("sqlite+aiosqlite:///:memory:")`
    - Add `aiosqlite` to dev dependencies in pyproject.toml

    **4. Create `tests/test_json_loader.py`:**
    - `test_load_valid_file`: Load the real bwb sample file, verify it returns EventFileData with 28 events
    - `test_load_event_ids`: Verify all event IDs follow `pdf-{hash}-{batch}-{index}` pattern
    - `test_load_event_fields`: Verify first event has title "Nordwiler Narrenfahrplan - Kita-Gizig-Umzug", location city "Kenzingen" (from _sanitizeResult), source_type "artikel"
    - `test_load_multi_date_event`: Find the "Primel-Aktion" event (id pdf-9d58bea1-3-1), verify it has 2 dates (2026-02-13, 2026-02-14)
    - `test_load_online_event`: Find the online event (id pdf-9d58bea1-4-0), verify city is None
    - `test_compute_file_hash`: Hash the sample file, verify it returns a 64-char hex string, verify same file gives same hash
    - `test_extract_source_code`: Test multiple filenames: `bwb_11.02.2026_...json` -> `bwb`, `emt_18.02.2026_...json` -> `emt`, `rkb_18.02.2026_...json` -> `rkb`
    - `test_invalid_json`: Create a temp file with invalid JSON, verify ValueError is raised

    **5. Create `tests/test_file_processor.py`:**
    - Use the SQLite in-memory database fixtures
    - `test_process_file_creates_records`: Process the bwb sample file, verify FileIngestion record exists with status='completed', event_count=28
    - `test_process_file_idempotency`: Process the same file twice, verify second call returns status='skipped', verify still only 28 SourceEvent records in DB
    - `test_process_file_creates_event_dates`: Process bwb file, verify EventDate records exist, verify Primel-Aktion has 2 date rows
    - `test_process_file_city_from_sanitize_result`: Verify that events use `_sanitizeResult.city` (e.g., first event should have location_city="Kenzingen")
    - `test_process_file_transaction_rollback`: Mock a failure during event creation, verify no FileIngestion or SourceEvent records exist in DB after failure
    - `test_dead_letter_on_failure`: Process an invalid file, verify it gets moved to dead_letter_dir

    All tests should be runnable with `uv run pytest tests/ -x -v` and not require a running PostgreSQL instance (use SQLite in-memory or mocks).
  </action>
  <verify>
    <automated>cd /Users/svenkarl/workspaces/event-deduplication && uv run pytest tests/test_json_loader.py tests/test_file_processor.py -x -v</automated>
  </verify>
  <done>
    - `load_event_file()` correctly parses all 20 JSON files without error
    - `compute_file_hash()` returns deterministic SHA-256 hashes
    - `extract_source_code()` correctly extracts source codes from all filename patterns
    - `FileProcessor.process_file()` creates FileIngestion + SourceEvent + EventDate records in a single transaction
    - Re-processing the same file returns status="skipped" without creating duplicates
    - Failed files are moved to dead_letter_dir
    - Transaction rollback leaves no partial data on failure
    - All tests pass with `uv run pytest tests/ -x -v`
  </done>
</task>

</tasks>

<verification>
After both tasks complete, run the full integration check:

1. `uv run python -c "from event_dedup.models import Base, SourceEvent, EventDate, FileIngestion"` -- all models import
2. `uv run pytest tests/ -x -v` -- all tests pass
3. Verify Alembic migration exists: `ls config/alembic/versions/*.py` shows at least one migration file
4. Verify project structure: all directories under `src/event_dedup/` exist (models, preprocessing, ingestion, evaluation, ground_truth, db, config)
</verification>

<success_criteria>
- Python project is fully scaffolded with uv, all dependencies installed
- SQLAlchemy models define source_events (with dual text columns), event_dates, file_ingestions tables
- Alembic is configured with an initial migration
- JSON loader validates and parses all 20 event files correctly
- File processor implements SHA-256 idempotency, single-transaction writes, and dead letter handling
- All tests pass without requiring a running PostgreSQL instance
</success_criteria>

<output>
After completion, create `.planning/phases/01/01-01-SUMMARY.md`
</output>
