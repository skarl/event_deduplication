---
phase: 03-pipeline-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - src/event_dedup/worker/__init__.py
  - src/event_dedup/worker/__main__.py
  - src/event_dedup/worker/watcher.py
  - src/event_dedup/worker/orchestrator.py
  - src/event_dedup/worker/persistence.py
  - src/event_dedup/logging_config.py
  - src/event_dedup/config/settings.py
  - tests/test_orchestrator.py
  - tests/test_persistence.py
  - tests/test_watcher.py
autonomous: true
requirements:
  - PIPE-01
  - PIPE-05

must_haves:
  truths:
    - "File watcher detects new .json files in the watched directory and triggers processing"
    - "Pipeline orchestrator connects ingestion -> load all events -> matching -> canonical persistence in sequence"
    - "Canonical events and source links are persisted to DB using clear-and-replace strategy"
    - "Match decisions are persisted to DB for Phase 6 review queue"
    - "Structured JSON logging reports per-file stats: events_ingested, total_events, matches_found, canonicals_created, flagged_for_review"
    - "Worker processes existing unprocessed files on startup for restart resilience"
    - "Graceful shutdown completes in-progress file before exiting"
  artifacts:
    - path: "src/event_dedup/worker/watcher.py"
      provides: "Async file watcher using watchfiles awatch with .json filter"
      exports: ["watch_and_process"]
    - path: "src/event_dedup/worker/orchestrator.py"
      provides: "Pipeline orchestrator bridging ingestion, matching, and persistence"
      exports: ["process_new_file", "process_existing_files"]
    - path: "src/event_dedup/worker/persistence.py"
      provides: "Canonical event and match decision DB persistence"
      exports: ["replace_canonical_events", "load_all_events_as_dicts"]
    - path: "src/event_dedup/logging_config.py"
      provides: "Unified structlog + stdlib JSON logging configuration"
      exports: ["configure_logging"]
    - path: "tests/test_orchestrator.py"
      provides: "Integration tests for the pipeline orchestrator"
      min_lines: 80
    - path: "tests/test_persistence.py"
      provides: "Unit tests for canonical event persistence"
      min_lines: 60
    - path: "tests/test_watcher.py"
      provides: "Unit tests for file watcher"
      min_lines: 40
  key_links:
    - from: "src/event_dedup/worker/orchestrator.py"
      to: "src/event_dedup/ingestion/file_processor.py"
      via: "FileProcessor.process_file for file ingestion"
      pattern: "FileProcessor|process_file"
    - from: "src/event_dedup/worker/orchestrator.py"
      to: "src/event_dedup/matching/pipeline.py"
      via: "run_full_pipeline for matching + clustering + synthesis"
      pattern: "run_full_pipeline|PipelineResult"
    - from: "src/event_dedup/worker/persistence.py"
      to: "src/event_dedup/models/canonical_event.py"
      via: "CanonicalEvent ORM model for DB writes"
      pattern: "CanonicalEvent"
    - from: "src/event_dedup/worker/watcher.py"
      to: "src/event_dedup/worker/orchestrator.py"
      via: "Watcher calls orchestrator for each detected file"
      pattern: "process_new_file"
---

<objective>
Build the pipeline worker service that watches a directory for new JSON files, processes them through the full deduplication pipeline (ingest → load events → match → persist canonicals), and reports structured processing logs. This is the runtime that turns the existing pure-function pipeline into an operational service.

Purpose: Without this plan, the matching pipeline exists only as a library — no code connects file arrival to canonical event creation. This plan creates the operational bridge.

Output: Worker service with file watcher, pipeline orchestrator, canonical persistence, structured logging, and comprehensive tests.
</objective>

<execution_context>
@.planning/phases/03/03-RESEARCH.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/phases/03/03-CONTEXT.md
@.planning/phases/03/03-RESEARCH.md
@.planning/phases/02/02-04-SUMMARY.md

<interfaces>
<!-- Existing interfaces this plan consumes -->

From src/event_dedup/ingestion/file_processor.py:
```python
class FileProcessor:
    def __init__(self, session_factory, dead_letter_dir, prefix_config_path=None, city_aliases_path=None): ...
    async def process_file(self, file_path: Path) -> FileProcessResult: ...

@dataclass
class FileProcessResult:
    status: str       # "completed", "skipped", "failed"
    event_count: int
    file_hash: str
    reason: str
```

From src/event_dedup/matching/pipeline.py:
```python
@dataclass
class PipelineResult:
    match_result: MatchResult
    cluster_result: ClusterResult       # .clusters: list[set[str]], .flagged_clusters: list[set[str]]
    canonical_events: list[dict]        # Parallel to clusters + flagged_clusters
    canonical_count: int
    flagged_count: int

def run_full_pipeline(events: list[dict], config: MatchingConfig) -> PipelineResult: ...

@dataclass
class MatchDecisionRecord:
    event_id_a: str; event_id_b: str
    signals: SignalScores  # .date, .geo, .title, .description
    combined_score_value: float
    decision: str          # "match", "no_match", "ambiguous"
    tier: str
```

From src/event_dedup/matching/config.py:
```python
def load_matching_config(path: Path) -> MatchingConfig: ...
```

From src/event_dedup/db/session.py:
```python
def get_session_factory() -> async_sessionmaker[AsyncSession]: ...
```

From src/event_dedup/config/settings.py:
```python
class Settings(BaseSettings):
    model_config = SettingsConfigDict(env_prefix="EVENT_DEDUP_")
    database_url: str = "postgresql+asyncpg://..."
    database_url_sync: str = "postgresql+psycopg2://..."
    dead_letter_dir: Path = Path("./dead_letters")
    event_data_dir: Path = Path("./eventdata")
```

From src/event_dedup/models/canonical_event.py:
```python
class CanonicalEvent(Base):
    __tablename__ = "canonical_events"
    id: Mapped[int]  # autoincrement PK
    title, short_description, description, highlights
    location_name, location_city, location_district, location_street, location_zipcode
    geo_latitude, geo_longitude, geo_confidence
    dates (JSON), categories (JSON)
    is_family_event, is_child_focused, admission_free
    field_provenance (JSON), source_count, match_confidence, needs_review, version
    created_at, updated_at
    sources: relationship to CanonicalEventSource (cascade delete-orphan)
```

From src/event_dedup/models/canonical_event_source.py:
```python
class CanonicalEventSource(Base):
    __tablename__ = "canonical_event_sources"
    id, canonical_event_id (FK), source_event_id (FK), added_at
    # Unique constraint on (canonical_event_id, source_event_id)
```

From src/event_dedup/models/match_decision.py:
```python
class MatchDecision(Base):
    __tablename__ = "match_decisions"
    id, source_event_id_a (FK), source_event_id_b (FK)
    combined_score, date_score, geo_score, title_score, description_score
    decision, tier, decided_at
    # Unique constraint on (source_event_id_a, source_event_id_b)
    # Check constraint: source_event_id_a < source_event_id_b
```

From src/event_dedup/models/source_event.py:
```python
class SourceEvent(Base):
    __tablename__ = "source_events"
    id: Mapped[str]  # "pdf-{hash}-{batch}-{index}"
    dates: relationship to EventDate (cascade)
    # All fields needed by pipeline: title, title_normalized, short_description_normalized,
    # description, highlights, location_*, geo_*, source_code, source_type, blocking_keys,
    # categories, is_family_event, is_child_focused, admission_free
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add Dependencies and Extend Settings</name>
  <files>
    pyproject.toml
    src/event_dedup/config/settings.py
  </files>
  <action>
1. **Update `pyproject.toml`** -- add runtime dependencies:
   - Add `"watchfiles>=1.0"` to `dependencies`
   - Add `"structlog>=24.0"` to `dependencies`

2. **Update `src/event_dedup/config/settings.py`** -- add worker configuration fields:
   - `matching_config_path: Path = Path("config/matching.yaml")` -- path to matching YAML config
   - `log_json: bool = True` -- JSON logging output (set False for dev readability)
   - `log_level: str = "INFO"` -- logging level

   These are all configurable via `EVENT_DEDUP_` env prefix (already set up).

3. **Run `uv sync`** to install new dependencies.
  </action>
  <verify>
    <automated>cd /Users/svenkarl/workspaces/event-deduplication && uv sync && uv run python -c "import watchfiles; import structlog; print('Dependencies OK')"</automated>
  </verify>
  <done>
    - watchfiles and structlog are installable and importable
    - Settings model extended with matching_config_path, log_json, log_level
  </done>
</task>

<task type="auto">
  <name>Task 2: Structured Logging Configuration</name>
  <files>
    src/event_dedup/logging_config.py
  </files>
  <action>
1. **Create `src/event_dedup/logging_config.py`**:
   - Function `configure_logging(json_output: bool = True, log_level: str = "INFO") -> None`:
     - Define `shared_processors` list:
       - `structlog.contextvars.merge_contextvars` -- support bound context
       - `structlog.stdlib.add_log_level` -- add "level" key
       - `structlog.stdlib.add_logger_name` -- add "logger" key
       - `structlog.processors.TimeStamper(fmt="iso")` -- ISO timestamp
       - `structlog.processors.StackInfoRenderer()` -- stack traces
       - `structlog.processors.format_exc_info` -- exception formatting
     - Choose renderer based on `json_output`:
       - True: `structlog.processors.JSONRenderer()`
       - False: `structlog.dev.ConsoleRenderer()` (colored dev output)
     - Configure structlog:
       ```python
       structlog.configure(
           processors=shared_processors + [
               structlog.stdlib.ProcessorFormatter.wrap_for_formatter,
           ],
           logger_factory=structlog.stdlib.LoggerFactory(),
           wrapper_class=structlog.stdlib.BoundLogger,
           cache_logger_on_first_use=True,
       )
       ```
     - Configure stdlib root logger to use structlog's ProcessorFormatter:
       ```python
       formatter = structlog.stdlib.ProcessorFormatter(
           foreign_pre_chain=shared_processors,
           processors=[
               structlog.stdlib.ProcessorFormatter.remove_processors_meta,
               renderer,
           ],
       )
       handler = logging.StreamHandler(sys.stdout)
       handler.setFormatter(formatter)
       root = logging.getLogger()
       root.handlers.clear()
       root.addHandler(handler)
       root.setLevel(getattr(logging, log_level.upper()))
       ```
   - This ensures BOTH `structlog.get_logger()` and existing `logging.getLogger(__name__)` calls (used in file_processor.py, alembic env.py) produce uniform output.
  </action>
  <verify>
    <automated>cd /Users/svenkarl/workspaces/event-deduplication && uv run python -c "
from event_dedup.logging_config import configure_logging
import logging, structlog
configure_logging(json_output=True)
# Test structlog logger
log = structlog.get_logger()
log.info('structlog_test', key='value')
# Test stdlib logger
stdlib_log = logging.getLogger('test_stdlib')
stdlib_log.info('stdlib test message')
print('Logging configuration OK')
"</automated>
  </verify>
  <done>
    - Both structlog and stdlib loggers produce JSON output to stdout
    - ConsoleRenderer mode available for development
  </done>
</task>

<task type="auto">
  <name>Task 3: Canonical Event Persistence Layer</name>
  <files>
    src/event_dedup/worker/__init__.py
    src/event_dedup/worker/persistence.py
    tests/test_persistence.py
  </files>
  <action>
1. **Create `src/event_dedup/worker/__init__.py`**: Empty file.

2. **Create `src/event_dedup/worker/persistence.py`**:
   - Function `async load_all_events_as_dicts(session: AsyncSession) -> list[dict]`:
     - Query all SourceEvents with eager-loaded dates: `select(SourceEvent).options(selectinload(SourceEvent.dates))`
     - Convert each to dict matching pipeline's expected format (see interfaces above for full field list)
     - Dates conversion: `[{"date": str(d.date), "start_time": str(d.start_time) if d.start_time else None, "end_time": str(d.end_time) if d.end_time else None, "end_date": str(d.end_date) if d.end_date else None} for d in evt.dates]`
     - Return list of dicts

   - Function `async replace_canonical_events(session: AsyncSession, pipeline_result: PipelineResult) -> int`:
     - Import PipelineResult, MatchDecisionRecord from matching.pipeline
     - In a single transaction (caller provides session within `session.begin()`):
       - Step 1: Delete all existing MatchDecision records: `await session.execute(delete(MatchDecision))`
       - Step 2: Delete all existing CanonicalEvent records (CASCADE deletes CanonicalEventSource): `await session.execute(delete(CanonicalEvent))`
       - Step 3: Build cluster list from pipeline_result:
         ```python
         all_clusters = list(pipeline_result.cluster_result.clusters) + list(pipeline_result.cluster_result.flagged_clusters)
         ```
         This is parallel to `pipeline_result.canonical_events` (same order).
       - Step 4: For each (canonical_dict, cluster) in zip(canonical_events, all_clusters):
         - Create CanonicalEvent ORM object from canonical_dict fields:
           - title, short_description, description, highlights
           - location_name, location_city, location_district, location_street, location_zipcode
           - geo_latitude, geo_longitude, geo_confidence
           - dates, categories
           - is_family_event, is_child_focused, admission_free
           - field_provenance, source_count, match_confidence, needs_review
         - `session.add(canonical)` then `await session.flush()` to get auto-generated ID
         - For each source_event_id in cluster (the set[str]):
           - Create CanonicalEventSource(canonical_event_id=canonical.id, source_event_id=source_event_id)
           - session.add(link)
       - Step 5: Persist match decisions:
         - For each decision in pipeline_result.match_result.decisions:
           - Create MatchDecision ORM object:
             - source_event_id_a=decision.event_id_a
             - source_event_id_b=decision.event_id_b
             - combined_score=decision.combined_score_value
             - date_score=decision.signals.date
             - geo_score=decision.signals.geo
             - title_score=decision.signals.title
             - description_score=decision.signals.description
             - decision=decision.decision
             - tier=decision.tier
           - session.add(match_decision)
       - Return len(pipeline_result.canonical_events)

3. **Create `tests/test_persistence.py`**:
   - Use the same test database pattern from existing tests (conftest.py fixtures)
   - **Test: load_all_events_as_dicts returns correct format**
     - Insert 2 SourceEvents with EventDates into test DB
     - Call load_all_events_as_dicts
     - Verify dicts have all required keys (id, title, title_normalized, blocking_keys, dates, etc.)
     - Verify dates are serialized as strings (not date/time objects)
   - **Test: replace_canonical_events creates canonical + sources + match decisions**
     - Insert source events into test DB
     - Create a mock PipelineResult with 2 canonical_events, 2 clusters, and some match decisions
     - Call replace_canonical_events
     - Query DB: verify canonical_events count, canonical_event_sources links, match_decisions count
   - **Test: replace_canonical_events clears previous data**
     - Insert some CanonicalEvents and MatchDecisions first
     - Call replace_canonical_events with new data
     - Verify old data is gone, new data is present
   - **Test: source links match cluster membership**
     - Create PipelineResult with a 3-event cluster
     - Call replace_canonical_events
     - Verify canonical has 3 source links (not just provenance entries)
  </action>
  <verify>
    <automated>cd /Users/svenkarl/workspaces/event-deduplication && uv run pytest tests/test_persistence.py -x -v</automated>
  </verify>
  <done>
    - load_all_events_as_dicts converts ORM models to pipeline-compatible dicts
    - replace_canonical_events persists canonicals, source links, and match decisions
    - Clear-and-replace strategy deletes old data before inserting new
    - Source links use cluster membership, not just field_provenance
  </done>
</task>

<task type="auto">
  <name>Task 4: Pipeline Orchestrator</name>
  <files>
    src/event_dedup/worker/orchestrator.py
    tests/test_orchestrator.py
  </files>
  <action>
1. **Create `src/event_dedup/worker/orchestrator.py`**:
   - Import structlog, use `logger = structlog.get_logger()`

   - Function `async process_new_file(file_path: Path, file_processor: FileProcessor, session_factory: async_sessionmaker, matching_config: MatchingConfig) -> dict`:
     - Bind file context: `log = logger.bind(file=file_path.name)`
     - Step 1: Ingest file
       - `result = await file_processor.process_file(file_path)`
       - If status != "completed": log "file_skipped" with status and reason, return dict with status info
       - Log "file_ingested" with event_count
     - Steps 2-4 wrapped in try/except Exception to catch pipeline errors:
     - Step 2: Load ALL source events from DB
       - `async with session_factory() as session:` then `events = await load_all_events_as_dicts(session)`
       - Log "events_loaded" with total_events=len(events)
     - Step 3: Run matching pipeline (pure function)
       - `pipeline_result = run_full_pipeline(events, matching_config)`
       - Log "matching_complete" with matches, canonical_count, flagged_count, ambiguous count, reduction_pct from pair_stats
     - Step 4: Persist canonical events
       - `async with session_factory() as session, session.begin():`
       - `count = await replace_canonical_events(session, pipeline_result)`
       - Log "pipeline_complete" with canonical_events_written=count
     - On exception: log "pipeline_failed" with error=str(e) and exc_info=True, return dict with status="error" and error message
     - Note on transaction design: File ingestion (Step 1) and canonical persistence (Step 4) are separate transactions. This is intentional — the clear-and-replace strategy rebuilds ALL canonicals on every run, so a failure between transactions leaves recoverable state. The next successful run rebuilds from all source events. SC-2 (single transaction per file) is satisfied by FileProcessor's existing transaction (Phase 1 PIPE-03).
     - Return stats dict:
       ```python
       {
           "status": "completed",
           "file": file_path.name,
           "events_ingested": result.event_count,
           "total_events": len(events),
           "matches_found": pipeline_result.match_result.match_count,
           "ambiguous": pipeline_result.match_result.ambiguous_count,
           "canonicals_created": pipeline_result.canonical_count,
           "flagged_for_review": pipeline_result.flagged_count,
       }
       ```

   - Function `async process_existing_files(data_dir: Path, file_processor: FileProcessor, session_factory: async_sessionmaker, matching_config: MatchingConfig) -> int`:
     - Scan data_dir for `*.json` files using `sorted(data_dir.glob("*.json"))`
     - For each file, call `process_new_file(...)` (the idempotency check in FileProcessor handles already-processed files)
     - Count and return number of files with status=="completed"
     - Log summary: "startup_scan_complete" with files_found, files_processed

   - Function `async process_file_batch(file_paths: list[Path], file_processor: FileProcessor, session_factory: async_sessionmaker, matching_config: MatchingConfig) -> list[dict]`:
     - Process a batch of files that arrived simultaneously from awatch
     - First, ingest ALL files sequentially (each creates source events)
     - Then run matching ONCE for all events (more efficient than per-file matching)
     - Implementation:
       - For each file: call file_processor.process_file(file_path) and collect results
       - If any completed: load all events, run pipeline, persist
       - Return list of stat dicts

2. **Create `tests/test_orchestrator.py`**:
   - Helper: Create test JSON files matching the expected format (use json_loader.py's EventData model)
   - **Test: process_new_file completes full pipeline**
     - Create a valid JSON file with 2 events from different sources that should match
     - Call process_new_file
     - Verify return dict has status="completed" and expected stat keys
     - Query DB: verify canonical events exist
   - **Test: process_new_file skips already-processed file**
     - Process a file once (completed)
     - Process same file again
     - Verify second call returns status="skipped"
   - **Test: process_new_file handles invalid JSON**
     - Create an invalid JSON file
     - Call process_new_file
     - Verify status="failed", file moved to dead letter dir
   - **Test: process_existing_files scans directory**
     - Place 3 JSON files in a temp dir
     - Call process_existing_files
     - Verify all 3 processed
   - **Test: process_file_batch runs matching once**
     - Create 2 JSON files
     - Call process_file_batch with both
     - Verify canonical events in DB represent matching across both files
  </action>
  <verify>
    <automated>cd /Users/svenkarl/workspaces/event-deduplication && uv run pytest tests/test_orchestrator.py -x -v</automated>
  </verify>
  <done>
    - process_new_file runs the full pipeline: ingest -> load -> match -> persist
    - process_existing_files scans for unprocessed files on startup
    - process_file_batch handles simultaneous file arrivals efficiently
    - Structured logs emitted at each pipeline stage
    - All tests pass
  </done>
</task>

<task type="auto">
  <name>Task 5: File Watcher and Worker Entry Point</name>
  <files>
    src/event_dedup/worker/watcher.py
    src/event_dedup/worker/__main__.py
    tests/test_watcher.py
  </files>
  <action>
1. **Create `src/event_dedup/worker/watcher.py`**:
   - Import `awatch, Change` from watchfiles
   - Import structlog, asyncio

   - Function `json_added_filter(change: Change, path: str) -> bool`:
     - Return `change == Change.added and path.endswith(".json")`

   - Function `async watch_and_process(data_dir: Path, file_processor: FileProcessor, session_factory: async_sessionmaker, matching_config: MatchingConfig, stop_event: asyncio.Event | None = None) -> None`:
     - `log = structlog.get_logger().bind(watch_dir=str(data_dir))`
     - `log.info("watcher_started")`
     - `async for changes in awatch(data_dir, watch_filter=json_added_filter, stop_event=stop_event):`
       - Collect all file paths from the change set: `file_paths = [Path(path) for _, path in changes]`
       - Log "files_detected" with count=len(file_paths)
       - If len(file_paths) == 1:
         - `await process_new_file(file_paths[0], file_processor, session_factory, matching_config)`
       - Else:
         - `await process_file_batch(file_paths, file_processor, session_factory, matching_config)`
     - `log.info("watcher_stopped")`

2. **Create `src/event_dedup/worker/__main__.py`**:
   ```python
   """Pipeline worker entry point: python -m event_dedup.worker"""
   import asyncio
   import signal
   import structlog

   from event_dedup.config.settings import get_settings
   from event_dedup.db.session import get_session_factory
   from event_dedup.ingestion.file_processor import FileProcessor
   from event_dedup.logging_config import configure_logging
   from event_dedup.matching.config import load_matching_config
   from event_dedup.worker.orchestrator import process_existing_files
   from event_dedup.worker.watcher import watch_and_process

   async def main() -> None:
       settings = get_settings()
       configure_logging(json_output=settings.log_json, log_level=settings.log_level)
       log = structlog.get_logger()

       session_factory = get_session_factory()
       matching_config = load_matching_config(settings.matching_config_path)
       file_processor = FileProcessor(
           session_factory=session_factory,
           dead_letter_dir=settings.dead_letter_dir,
       )

       data_dir = settings.event_data_dir
       data_dir.mkdir(parents=True, exist_ok=True)

       log.info("worker_starting", watch_dir=str(data_dir), database=settings.database_url.split("@")[-1])

       # Graceful shutdown via SIGTERM/SIGINT
       stop_event = asyncio.Event()
       loop = asyncio.get_running_loop()
       for sig in (signal.SIGTERM, signal.SIGINT):
           loop.add_signal_handler(sig, stop_event.set)

       # Process existing unprocessed files on startup
       processed = await process_existing_files(data_dir, file_processor, session_factory, matching_config)
       log.info("startup_complete", existing_files_processed=processed)

       # Watch for new files
       await watch_and_process(data_dir, file_processor, session_factory, matching_config, stop_event=stop_event)
       log.info("worker_shutdown")

   if __name__ == "__main__":
       asyncio.run(main())
   ```

3. **Create `tests/test_watcher.py`**:
   - **Test: json_added_filter accepts .json added files**
     - Verify `json_added_filter(Change.added, "/path/to/file.json")` returns True
   - **Test: json_added_filter rejects non-json files**
     - Verify `json_added_filter(Change.added, "/path/to/file.txt")` returns False
   - **Test: json_added_filter rejects modified files**
     - Verify `json_added_filter(Change.modified, "/path/to/file.json")` returns False
   - **Test: watch_and_process stops on stop_event**
     - Create a stop_event, set it after a short delay
     - Run watch_and_process with the stop_event
     - Verify it exits cleanly
  </action>
  <verify>
    <automated>cd /Users/svenkarl/workspaces/event-deduplication && uv run pytest tests/test_watcher.py -x -v</automated>
  </verify>
  <done>
    - File watcher detects new .json files using watchfiles awatch
    - Single files processed via process_new_file, batches via process_file_batch
    - Worker entry point configures logging, processes existing files, then watches
    - Graceful shutdown via SIGTERM/SIGINT using asyncio.Event
    - All tests pass
  </done>
</task>

</tasks>

<verification>
```bash
# All plan 03-01 tests pass
cd /Users/svenkarl/workspaces/event-deduplication && uv run pytest tests/test_persistence.py tests/test_orchestrator.py tests/test_watcher.py -x -v

# Import chain works
cd /Users/svenkarl/workspaces/event-deduplication && uv run python -c "
from event_dedup.worker.watcher import watch_and_process, json_added_filter
from event_dedup.worker.orchestrator import process_new_file, process_existing_files, process_file_batch
from event_dedup.worker.persistence import load_all_events_as_dicts, replace_canonical_events
from event_dedup.logging_config import configure_logging
from event_dedup.config.settings import Settings
print('All imports OK')
"

# Logging produces JSON output
cd /Users/svenkarl/workspaces/event-deduplication && uv run python -c "
from event_dedup.logging_config import configure_logging
import structlog, logging, json
configure_logging(json_output=True)
# Verify JSON output
structlog.get_logger().info('test', key='value')
logging.getLogger('stdlib_test').info('hello')
print('JSON logging OK')
"

# ALL prior tests still pass
cd /Users/svenkarl/workspaces/event-deduplication && uv run pytest tests/ -x -v --timeout=60
```
</verification>

<success_criteria>
1. File watcher detects new .json files and triggers processing via awatch
2. Pipeline orchestrator connects: ingest file -> load ALL events -> run_full_pipeline -> persist canonicals
3. Canonical events, source links, and match decisions are all persisted to DB
4. Clear-and-replace strategy ensures consistent state after every pipeline run
5. Structured JSON logs report per-file processing stats (events_ingested, total_events, matches_found, canonicals_created, flagged_for_review)
6. Worker processes existing unprocessed files on startup
7. Graceful shutdown via SIGTERM completes in-progress work before exiting
8. All new tests pass: test_persistence, test_orchestrator, test_watcher
9. ALL prior tests still pass (full test suite green)
</success_criteria>

<output>
After completion, create `.planning/phases/03/03-01-SUMMARY.md`
</output>
